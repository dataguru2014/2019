{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file D:\\anaconda\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import requests                              #保存为字典的\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "info1={'rank':[],'movie_title':[],'director':[],'year':[],'country':[],'type':[],'score':[],'comment_num':[],'comm_one':[]}\n",
    "# info2={'director':[],'lead_role':[],'language':[],'flength':[],}\n",
    "\n",
    "def get_info1(text,info1):          #获取文本中所需的数据并保存在列表info1中\n",
    "    soup=BeautifulSoup(text).find('ol', class_=\"grid_view\").findAll('li')      \n",
    "    for j in soup:  \n",
    "        info1['rank'].append(j.find('em', class_=\"\").string)\n",
    "        info1['movie_title'].append(j.find('span', class_=\"title\").string)\n",
    "        meg=j.find('p', class_=\"\").text.strip().split('\\n')\n",
    "        info1['director'].append(meg[0].strip().split('\\xa0\\xa0')[0].strip('导演: '))    #导演名字太长导致有些切割三个\\ax0时出现错误\n",
    "        info1['year'].append(meg[1].strip().split('\\xa0/\\xa0')[0])                       \n",
    "        info1['country'].append(meg[1].strip().split('\\xa0/\\xa0')[1])\n",
    "        info1['type'].append(meg[1].strip().split('\\xa0/\\xa0')[-1])\n",
    "        info1['score'].append(j.find('div', class_=\"star\").findAll('span')[1].string)\n",
    "        info1['comment_num'].append(j.find('div', class_=\"star\").findAll('span')[3].string.strip('人评价'))\n",
    "        if len(j.find('div', class_=\"bd\").findAll('p'))==2:                 #第7页某电影不存在一句影评模块,因此会导致读取不到信息\n",
    "            info1['comm_one'].append(j.find('span', class_=\"inq\").string)\n",
    "        else:\n",
    "            info1['comm_one'].append('无')\n",
    "    return DataFrame(info1)\n",
    "\n",
    "for i in range(0,230,25):\n",
    "    head={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0'}\n",
    "    url=r'https://movie.douban.com/top250?start={0}&filter='.format(str(i))\n",
    "    html=requests.get(url,headers=head)\n",
    "    html.encoding='utf-8'\n",
    "    Info1=get_info1(html.text,info1)\n",
    "        \n",
    "#         url2=j.find('div', class_=\"hd\").a['href']\n",
    "#         html2=requests.get(url2,headers=head)\n",
    "#         html2.encoding='utf-8'\n",
    "#         soup2=BeautifulSoup(html2.text).find('div', id=\"info\")\n",
    "#         info2['director'].append(soup2.find('span', class_=\"attrs\").a.string)\n",
    "#         info2['lead_role'].append(soup2.find('span', class_=\"actor\").a.string)\n",
    "#         info2['language'].append(soup2.findAll('span', class_=\"pl\")[5].string)\n",
    "#         info2['flength'].append(soup2.find('span', property=\"v:runtime\").string)\n",
    "\n",
    "Info1.to_excel(r'D:\\python操作文件\\豆瓣\\数据1.xlsx',encoding='utf-8')    #出现\\xf4编码在保存csv文件时出现问题\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file D:\\anaconda\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import requests                           #列表形式\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import csv\n",
    "ListFinal=[]\n",
    "\n",
    "def get_info1(text):          #获取文本中所需的数据并保存在列表info1中\n",
    "    soup=BeautifulSoup(text).find('ol', class_=\"grid_view\").findAll('li')  \n",
    "    M=[]\n",
    "    for j in soup:  \n",
    "        m=[]\n",
    "        rank=j.find('em', class_=\"\").string\n",
    "        movie_title=j.find('span', class_=\"title\").string\n",
    "        meg=j.find('p', class_=\"\").text.strip().split('\\n')\n",
    "        director=meg[0].strip().split('\\xa0\\xa0')[0].strip('导演: ')   #导演名字太长导致有些切割三个\\ax0时出现错误\n",
    "        year=meg[1].strip().split('\\xa0/\\xa0')[0]                     \n",
    "        country=meg[1].strip().split('\\xa0/\\xa0')[1]\n",
    "        types=meg[1].strip().split('\\xa0/\\xa0')[-1]\n",
    "        score=j.find('div', class_=\"star\").findAll('span')[1].string\n",
    "        comment_num=j.find('div', class_=\"star\").findAll('span')[3].string.strip('人评价')\n",
    "        if len(j.find('div', class_=\"bd\").findAll('p'))==2:                 #第7页某电影不存在一句影评模块,因此会导致读取不到信息\n",
    "            comm_one=j.find('span', class_=\"inq\").string\n",
    "        else:\n",
    "            comm_one='无'\n",
    "        m=[rank,movie_title,director,year,country,types,score,comment_num,comm_one]\n",
    "        M.append(m)\n",
    "    return M\n",
    "\n",
    "for i in range(0,230,25):\n",
    "    head={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0'}\n",
    "    url=r'https://movie.douban.com/top250?start={0}&filter='.format(str(i))\n",
    "    html=requests.get(url,headers=head)\n",
    "    html.encoding='utf-8'\n",
    "    ListFinal.extend(get_info1(html.text))\n",
    "with open(r'D:\\python操作文件\\豆瓣\\数据2.csv','w',newline='',encoding='gb18030') as file:\n",
    "    w=csv.writer(file)\n",
    "    w.writerow(['rank','movie_title','director','year','country','types','score','comment_num','comm_one'])\n",
    "    w.writerows(ListFinal)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file D:\\anaconda\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import requests                          #课堂使用try提起年份\\国家\\类型\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import csv\n",
    "\n",
    "listA=[]\n",
    "for i in range(0,230,25):    #230\n",
    "    head={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0'}\n",
    "    url=r'https://movie.douban.com/top250?start={0}&filter='.format(str(i))\n",
    "    html=requests.get(url,headers=head)\n",
    "    html.encoding='utf-8'\n",
    "    soup=BeautifulSoup(html.text).find('ol', class_=\"grid_view\").findAll('li')\n",
    "    for j in soup:\n",
    "        lista=[]\n",
    "        a=j.find('div', class_=\"bd\").text.strip().split('\\n')\n",
    "        try:\n",
    "            year=a[1].strip().split('\\xa0/\\xa0')[0]                     \n",
    "            country=a[1].strip().split('\\xa0/\\xa0')[1]\n",
    "            types=a[1].strip().split('\\xa0/\\xa0')[-1]\n",
    "        except:\n",
    "            year='无'\n",
    "            country='无'\n",
    "            types='无'\n",
    "        lista=[year,country,types]\n",
    "        listA.append(lista)\n",
    "\n",
    "with open(r'D:\\python操作文件\\豆瓣\\数据3.csv','w',newline='',encoding='gb18030') as file:\n",
    "    w=csv.writer(file)\n",
    "    w.writerow(['year','country','types'])\n",
    "    w.writerows(listA)\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
