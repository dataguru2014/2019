[MUSIC] Hi, everyone. In this section, we'll cover a very
powerful technique, mean encoding. It actually has a number of names. Some call it likelihood encoding,
some target encoding, but in this course,
we'll stick with plain mean encoding. The general idea of this
technique is to add new variables based on some
feature to get where we started,. In simplest case, we encode each
level of categorical variable with corresponding target mean. Let's take a look at
the following example. Here, we have some binary
classification task in which we have a categorical variable, some city. And of course,
we want to numerically encode it. The most obvious way and
what people usually use is label encoding. It's what we have in second column. Mean encoding is done differently, via encoding every city with
corresponding mean target. For example, for Moscow, we have
five rows with three 0s and two 1s. So we encode it with 2 divided by 5 or
0.4. Similarly, we deal with the rest
of cities, pretty straightforward. What I've described here
is a very high level idea. There are a huge number of pitfalls one
should overcome in actual competition. We went deep into details for
now, just keep it in mind. At first, let me explain. Why does it even work? Imagine, that our dataset is much bigger
and contains hundreds of different cities. Well, let's try to compare,
of course, very abstractly, mean encoding with label encoding. We plot future histograms for
class 0 and class 1. In case of label encoding,
we'll always get total and random picture because
there's no logical order, but when we use mean target to encode the
feature, classes look way more separable. The plot looks kind of sorted. It turns out that this sorting quality
of mean encoding is quite helpful. Remember, what is the most popular and effective way to solve
machine learning problem? Is grading using trees, [INAUDIBLE] OIGBM. One of the few downsides is
an inability to handle high cardinality categorical variables. Trees have limited depth,
with mean encoding, we can compensate it, we can reach better loss
with shorter trees. Cross validation loss
might even look like this. In general, the more complicated and
non linear feature target dependency, the more effective is mean encoding, okay. Further in this section, you will
learn how to construct mean encodings. There are actually a lot of ways. Also keep in mind that we use
classification tests only as an example. We can use mathematics
on other tests as well. The main idea remains the same. Despite the simplicity of the idea, you
need to be very careful with validation. It's got to be impeccable. It's probably the most important part. Understanding the correct linkless
validation is also a basis for staking. The last, but not least, are extensions. There are countless possibilities to
derive new features from target variable. Sometimes, they produce significant
improvement for your models. Let's start with some
characteristics of data sets, that indicate the usefulness
of main encoding. The presence of categorical
variables with a lot of levels is already a good indicator, but
we need to go a little deeper. Let's take a look at each of these
learning logs from Springleaf competition. I ran three models with different depths,
7, 9, and 11. Train logs are on the top plot. Validation logs are on the bottom one. As you can see, with increasing the depths
of trees, our training care becomes better and better, nearly perfect and
that's a normal part. But we don't actually over feed and
that's weird. Our validation score also increase,
it's a sign that trees need a huge number of splits to
extract information from some variables. And we can check it for mortal dump. It turns out that some features have
a tremendous amount of split points, like 1200 or 1600 and that's a lot. Our model tries to treat all
those categories differently and they are also very important for
predicting the target. We can help our model via mean encodings. There is a number of ways
to calculate encodings. The first one is the one
we've been discussing so far. Simply taking mean of target variable. Another popular option is to take
initial logarithm of this value, it's called weight of evidence. Or you can calculate all
of the numbers of ones. Or the difference between number
of ones and the number of zeros. All of these are variable options. Now, let's actually
construct the features. We will do it on sprinkled data set, suppose we've already separated
the data for train and validation, X_tr and X val data frames. These called snippet shows how
to construct mean encoding for an arbitrary column and map it into
a new data frame, train new and val new. We simply do group by on that column and
use target as a map. Resulting commands were able [INAUDIBLE]. It is then mapped to tree and
validation data sets by a map operator. After we've repeated this process for
every call, we can fit each of those
model on this new data. But something's definitely not right, after several efforts training AOC
is nearly 1, while on validation, the score set rates around 0.55,
which is practically noise. It's a clear sign of terrible overfitting. I'll explain what happened
in a few moments. Right now, I want to point out that
at least we validated correctly. We separated train and validation, and used all the train
data to estimate mean encodings. If, for instance, we would have
estimated mean encodings before train validation split, then we would
not notice such an overfitting. Now, let's figure out
the reason of overfitting. When they are categorized, it's pretty
common to get results like in an example, target 0 in train and
target 1 in validation. Mean encodings turns into a perfect
feature for such categories. That's why we immediately get
very good scores on train and fail hardly on validation. So far, we've grasped the concept of mean
encodings and walked through some trivial examples, that obviously can not use
mean encodings like this in practice. We need to deal with overfitting first,
we need some kind of regularization. And I will tell you about different methods in the next video. [MUSIC][MUSIC] In previous video, we realized that
mean encodings cannot be used as is and requires some kind of regularization
on training part of data. Now, we'll carry out four different
methods of regularization, namely, doing a cross-validation loop
to construct mean encodings. Then, smoothing based on
the size of category. Then, adding random noise. And finally, calculating expanding
mean on some parametrization of data. We will go through all of
these methods one by one. Let's start with CV loop regularization. It's a very intuitive and robust method. For a given data point, we don't want to
use target variable of that data point. So we separate the data into
K-node intersecting subsets, or in other words, folds. To get mean encoding value for
some subset, we don't use data points from that subset and estimate
the encoding only on the rest of subset. We iteratively walk through
all the data subsets. Usually, four or five folds
are enough to get decent results. You don't need to tune this number. It may seem that we have completely
avoided leakage from target variable. Unfortunately, it's not true. It will become apparent if we perform
leave one out scheme to separate the data. I'll return to it a little later, but first let's learn how to
apply this method in practice. Suppose that our training
data is in a DFTR data frame. We will add mean encoded features
into another train new data frame. In the outer loop,
we iterate through stratified K-fold iterator in order to separate
training data into chunks. X_tr is used to estimate the encoding. X_val is used to apply
estimating encoding. After that,
we iterate through all the columns and map estimated encodings
to X_val data frame. At the end of the outer loop we fill
train new data frame with the result. Finally, some rare categories may
be present only in a single fold. So we don't have the data to
estimate target mean for them. That's why we end up with some nans. We can fill them with global mean. As you can see,
the whole process is very simple. Now, let's return to
the question of whether we leak information about
target variable or not. Consider the following example. Here we want to encode Moscow
via leave-one-out scheme. For the first row, we get 0.5,
because there are two 1s and two 0s in the rest of rows. Similarly, for
the second row we get 0.25 and so on. But look closely, all the resulting and
the resulting features. It perfect splits the data,
rows with feature mean equal or greater than 0.5 have target 0 and
the rest of rows has target 1. We didn't explicitly use target variable,
but our encoding is biased. Furthermore, this effect remains valid
even for the KFold scheme, just milder. So is this type of regularization useless? Definitely not. In practice,
if you have enough data and use four or five folds, the encodings will work
fine with this regularization strategy. Just be careful and
use correct validation. Another regularization
method is smoothing. It's based on the following idea. If category is big,
has a lot of data points, then we can trust this to [INAUDIBLE] encoding, but
if category is rare it's the opposite. Formula on the slide uses this idea. It has hyper parameter alpha that
controls the amount of regularization. When alpha is zero,
we have no regularization, and when alpha approaches infinity
everything turns into globalmean. In some sense alpha is equal to
the category size we can trust. It's also possible to use some other
formula, basically anything that punishes encoding software categories
can be considered smoothing. Smoothing obviously won't
work on its own but we can combine it with for
example, CD loop regularization. Another way to regularize encodence is to
add some noise without regularization. Meaning codings have better quality for the [INAUDIBLE] data than for
the test data. And by adding noise, we simply degrade
the quality of encoding on training data. This method is pretty unstable,
it's hard to make it work. The main problem is the amount
of noise we need to add. Too much noise will turn
the feature into garbage, while too little noise
means worse regularization. This method is usually used together
with leave one out regularization. You need to diligently fine tune it. So, it's probably not the best option
if you don't have a lot of time. The last regularization method I'm going
to cover is based on expanding mean. The idea is very simple. We fix some sorting order of our data and use only rows from zero to n minus
one to calculate encoding for row n. You can check simple implementation
in the code snippet. Cumsum stores cumulative sum
of target variable up to the given row and
cumcnt stores cumulative count. This method introduces the least amount
of leakage from target variable and it requires no hyper parameter tuning. The only downside is that
feature quality is not uniform. But it's not a big deal. We can average models on encodings calculated from
different data permutations. It's also worth noting that
it is expanding mean method that is used in CatBoost grading,
boosting to it's library, which proves to perform magnificently
on data sets with categorical features. Okay, let's summarize what
we've discussed in this video. We covered four different
types of regularization. Each of them has its own advantages and
disadvantages. Sometimes unintuitively we
introduce target variable leakage. But in practice, we can bear with it. Personally, I recommend CV loop or
expanding mean methods for practical tasks. They are the most robust and easy to tune. This is was regularization. In the next video, I will tell
you about various extensions and practical applications of mean encodings. Thank you. [MUSIC][SOUND] In the final video,
we will cover various generalizations and extensions of mean encodings. Namely how to do meaning coding in
regression and multiclass tasks. How can we apply encoding to domains
with many-to-many relations. What features can we build based on
target we're able in time series. And finally, how to encode numerical
features and interactions of features. Let's start with regression tasks. They are actually more flexible for
feature encoding. Unlike binary classification where
a mean is frankly the only meaningful statistic we can extract
from target variable. In regression tasks, we can try
a variety of statistics, like medium, percentile, standard
deviation of target variable. We can even calculate
some distribution bins. For example, if target variable
is distributed between 1 and 100, we can create 10 bin features. In the first feature, we'll count how many
data points have targeted between 1 and 10, in the second between 10 and
20 and so on. Of course,
we need to realize all of these features. In a nutshell,
regression tasks are like classification. Just more flexible in terms
of feature engineering. Men encoding for multi-class tasks
is also pretty straightforward. For every feature we want to encode, we will have n different encodings
where n is the number of classes. It actually has non obvious advantage. Three models for example, usually solve multi-class
task in one versus old fashion. So every class had a different model, and when we feed that model, it doesn't
have any information about structure of other classes because they
are merge into one entity. Therefore, together with mean encodings, we introduce some additional information
about the structure of other classes. The domains with many-to-many
relations are usually very complex and require special approaches
to create mean encodings. I will give you only a very high
level idea, consider an example. Binary classification task for users based
on apps installed on their smartphones. Each user may have multiple apps and
each app is used by multiple users. Hence, many-to-many relation. We want to mean encode apps. The hard part we need to deal with is
that the user may have a lot of apps. So let's take a cross product of user and
app entities. It will result in a so
called long representation of data. We will have a role for
each user app pair. Using this table, we can naturally
calculate mean encoding for apps. So now every app is encoded with target
mean, but how to map it back to users. Every user has a number of apps, so instead of app1, app2, app3, we will now have a vector like 0.1,
0.2, 0.1. That was pretty simple. We can collect various statistics
from those vectors, like mean, minimal, maximum,
standard deviation, and so on. So far we assume that our data
has no inner structure, but with time series we can obviously
use future information. On one hand, it's a limitation, on the other hand, it actually allows
us to make some complicated features. In data sets without time component
when encoding the category, we are forced to use all the rules
to calculate the statistic. It makes no sense to choose
some subset of rules. Presence of time changes it. For a given category, we can't. For example, calculate the mean from
previous day, previous two days, previous week, etc. Consider an example. We need to predict which
categories users spends money. In these two example we have
a period of two days, two users, and three spending categories. Some good features would be
the total amount of money users spent in previous day. An average amount of money spent
by all users in given category. So, in day 1, user 101 spends $6, user 102, $3. Therefore, we feel those numbers
as future values for day 2. Similarly, with the average
amount by category. The more data we have, the more
complicated features we can create. In practice, it is often been official
to mean encode numeric features and some combination of features. To encode a numeric feature, we only need
to bin it and then treat as categorical. Now, we need to answer two questions. First, how to bin numeric feature, and second how to select useful
combination of features. Well, we can find it out from a model
structure by analyzing the trees. So at first, we take for
example, [INAUDIBLE] model and raw features without any encodings. Let's start with numeric features. If numeric feature has a lot of
[INAUDIBLE] points, it means that it has some complicated dependency with target
and its was trying to mean encode it. Furthermore, these exact split points
may be used to bin the feature. So by analyzing model structure, we both identify suspicious numeric
feature and found a good way to bin it. It's going to be a little harder
with selecting interactions, but nothing extraordinary. First, let's define how to extract to
way interaction from decision tree. The process will be similar for three way,
four way arbitrary way interactions. So two features interact in a tree if
they are in two neighbouring notes. With that in mind, we can iterate
through all the trees in the model and calculate how many times each
feature interaction appeared. The most frequent interactions
are probably worthy of mean encoding. For example, if we found that feature one
and feature two pair is most frequent, then we can concatenate that
those feature values in our data. And mean encode resulting interaction. Now let me illustrate how important
interaction encoding may be. Amazon Employee Access Challenge
Competition has a very specific data set. There are only nine categorical features. If we blindly fit say like GBM
model on the raw features, then no matter how we
return the parameters, we'll score in a 0.87 AUC range. Which will place roughly on 700
position on the leaderboard. Furthermore, even if we mean encode all
the labels, we won't have any progress. But if we fit cat boost model,
which internally mean encodes some feature interactions,
we will immediately score in 0.91 range, which will place us
onto win this position. The difference in both
absolute AUC values and relative leaderboard
positions is tremendous. Also note that cat boost
is no silver bullet. In order to get even higher
on the leader board, would still need to manually add
more mean encoded interactions. In general, if you participate in
a competition with a lot of categorical variables, it's always worth trying to
work with interactions and mean encodings. I also want to remind you about
correct validation process. During all local experiments, you should at first split data in X_tr and
X_val parts. Estimate encodings on X_tr,
map them to X_tr and X_val, and
then regularize them on X_tr and only after that validate your
model on X_tr / X_val split. Don't even think about estimating
encodings before splitting the data. And at submission stage, you can
estimate encodings on whole train data. Map it to train and test, then apply regularization on training
data and finally fit a model. And note that you should have already
decided on regularization method and its strength in local experiments. At the end of this section,
let's summarize main advantages and disadvantages of mean encodings. First of all, mean encoding allows us to make a compact
transformation of categorical variables. It is also a powerful basis for
feature engineering. Then the main disadvantage
is target rebel leakage. We need to be very careful with
validation and irregularization. It also works only on specific data sets. It definitely won't help
in every competition. But keep in mind, when this method works,
it may produce significant improvements. Thank you for your attention. [MUSIC]