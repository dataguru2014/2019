%!PS-Adobe-3.0
%%Title: Week4.txt
%%For: wb
%%Creator: VIM - Vi IMproved 8.0 (2016 Sep 12)
%%CreationDate: Wed Dec 18 17:16:50 2019
%%DocumentData: Clean8Bit
%%Orientation: Portrait
%%Pages: (atend)
%%PageOrder: Ascend
%%BoundingBox: 59 45 559 800
%%DocumentMedia: A4 595 842 0 () ()
%%DocumentNeededResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%DocumentSuppliedResources: procset VIM-Prolog 1.4 1
%%+ encoding VIM-latin1 1.0 0
%%Requirements: duplex collate
%%EndComments
%%BeginDefaults
%%PageResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%PageMedia: A4
%%EndDefaults
%%BeginProlog
%%BeginResource: procset VIM-Prolog
%%BeginDocument: /usr/share/vim/vim80/print/prolog.ps
%!PS-Adobe-3.0 Resource-ProcSet
%%Title: VIM-Prolog
%%Version: 1.4 1
%%EndComments
% Editing of this file is NOT RECOMMENDED.  You run a very good risk of causing
% all PostScript printing from VIM failing if you do.  PostScript is not called
% a write-only language for nothing!
/packedarray where not{userdict begin/setpacking/pop load def/currentpacking
false def end}{pop}ifelse/CP currentpacking def true setpacking
/bd{bind def}bind def/ld{load def}bd/ed{exch def}bd/d/def ld
/db{dict begin}bd/cde{currentdict end}bd
/T true d/F false d
/SO null d/sv{/SO save d}bd/re{SO restore}bd
/L2 systemdict/languagelevel 2 copy known{get exec}{pop pop 1}ifelse 2 ge d
/m/moveto ld/s/show ld /ms{m s}bd /g/setgray ld/r/setrgbcolor ld/sp{showpage}bd
/gs/gsave ld/gr/grestore ld/cp/currentpoint ld
/ul{gs UW setlinewidth cp UO add 2 copy newpath m 3 1 roll add exch lineto
stroke gr}bd
/bg{gs r cp BO add 4 -2 roll rectfill gr}bd
/sl{90 rotate 0 exch translate}bd
L2{
/sspd{mark exch{setpagedevice}stopped cleartomark}bd
/nc{1 db/NumCopies ed cde sspd}bd
/sps{3 db/Orientation ed[3 1 roll]/PageSize ed/ImagingBBox null d cde sspd}bd
/dt{2 db/Tumble ed/Duplex ed cde sspd}bd
/c{1 db/Collate ed cde sspd}bd
}{
/nc{/#copies ed}bd
/sps{statusdict/setpage get exec}bd
/dt{statusdict/settumble 2 copy known{get exec}{pop pop pop}ifelse
statusdict/setduplexmode 2 copy known{get exec}{pop pop pop}ifelse}bd
/c{pop}bd
}ifelse
/ffs{findfont exch scalefont d}bd/sf{setfont}bd
/ref{1 db findfont dup maxlength dict/NFD ed{exch dup/FID ne{exch NFD 3 1 roll
put}{pop pop}ifelse}forall/Encoding findresource dup length 256 eq{NFD/Encoding
3 -1 roll put}{pop}ifelse NFD dup/FontType get 3 ne{/CharStrings}{/CharProcs}
ifelse 2 copy known{2 copy get dup maxlength dict copy[/questiondown/space]{2
copy known{2 copy get 2 index/.notdef 3 -1 roll put pop exit}if pop}forall put
}{pop pop}ifelse dup NFD/FontName 3 -1 roll put NFD definefont pop end}bd
CP setpacking
(\004)cvn{}bd
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%BeginResource: encoding VIM-latin1
%%BeginDocument: /usr/share/vim/vim80/print/latin1.ps
%!PS-Adobe-3.0 Resource-Encoding
%%Title: VIM-latin1
%%Version: 1.0 0
%%EndComments
/VIM-latin1[
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclam /quotedbl /numbersign /dollar /percent /ampersand /quotesingle
/parenleft /parenright /asterisk /plus /comma /minus /period /slash
/zero /one /two /three /four /five /six /seven
/eight /nine /colon /semicolon /less /equal /greater /question
/at /A /B /C /D /E /F /G
/H /I /J /K /L /M /N /O
/P /Q /R /S /T /U /V /W
/X /Y /Z /bracketleft /backslash /bracketright /asciicircum /underscore
/grave /a /b /c /d /e /f /g
/h /i /j /k /l /m /n /o
/p /q /r /s /t /u /v /w
/x /y /z /braceleft /bar /braceright /asciitilde /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclamdown /cent /sterling /currency /yen /brokenbar /section
/dieresis /copyright /ordfeminine /guillemotleft /logicalnot /hyphen /registered /macron
/degree /plusminus /twosuperior /threesuperior /acute /mu /paragraph /periodcentered
/cedilla /onesuperior /ordmasculine /guillemotright /onequarter /onehalf /threequarters /questiondown
/Agrave /Aacute /Acircumflex /Atilde /Adieresis /Aring /AE /Ccedilla
/Egrave /Eacute /Ecircumflex /Edieresis /Igrave /Iacute /Icircumflex /Idieresis
/Eth /Ntilde /Ograve /Oacute /Ocircumflex /Otilde /Odieresis /multiply
/Oslash /Ugrave /Uacute /Ucircumflex /Udieresis /Yacute /Thorn /germandbls
/agrave /aacute /acircumflex /atilde /adieresis /aring /ae /ccedilla
/egrave /eacute /ecircumflex /edieresis /igrave /iacute /icircumflex /idieresis
/eth /ntilde /ograve /oacute /ocircumflex /otilde /odieresis /divide
/oslash /ugrave /uacute /ucircumflex /udieresis /yacute /thorn /ydieresis]
/Encoding defineresource pop
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%EndProlog
%%BeginSetup
595 842 0 sps
1 nc
T F dt
T c
%%IncludeResource: font Courier
/_F0 /VIM-latin1 /Courier ref
/F0 13 /_F0 ffs
%%IncludeResource: font Courier-Bold
/_F1 /VIM-latin1 /Courier-Bold ref
/F1 13 /_F1 ffs
%%IncludeResource: font Courier-Oblique
/_F2 /VIM-latin1 /Courier-Oblique ref
/F2 13 /_F2 ffs
%%IncludeResource: font Courier-BoldOblique
/_F3 /VIM-latin1 /Courier-BoldOblique ref
/F3 13 /_F3 ffs
/UO -1.3 d
/UW 0.65 d
/BO -3.25 d
%%EndSetup
%%Page: 1 1
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                 Page 1)59.5 790.15 ms
F0 sf
([MUSIC] Hi, in this lecture, we will study)59.5 764.15 ms
(hyperparameter optimization process and talk about hyperparamete)59.5 751.15 ms
(rs in)59.5 738.15 ms
(specific libraries and models. We will first discuss)59.5 725.15 ms
(hyperparameter tuning in general. General pipeline, ways to tuni)59.5 712.15 ms
(ng)59.5 699.15 ms
(hyperparameters, and what it actually means to understand how a )59.5 686.15 ms
(particular)59.5 673.15 ms
(hyperparameter influences the model. It is actually what we will)59.5 660.15 ms
(discuss in this video, and then we will talk about libraries and)59.5 647.15 ms
(frameworks, and see how to tune hyperparameters)59.5 634.15 ms
(of several types of models. Namely, we will first)59.5 621.15 ms
(study tree-based models, gradient boosting decision trees and)59.5 608.15 ms
(RandomForest. Then I'll review important)59.5 595.15 ms
(hyperparameters in neural nets. And finally, we will talk about)59.5 582.15 ms
(linear models, where to find them and how to tune them. Another )59.5 569.15 ms
(class of interesting)59.5 556.15 ms
(models is factorization machines. We will not discuss factorizat)59.5 543.15 ms
(ion)59.5 530.15 ms
(machines in this lecture, but I suggest you to read)59.5 517.15 ms
(about them on the internet. So, let's start with a general)59.5 504.15 ms
(discussion of a model tuning process. What are the most importan)59.5 491.15 ms
(t things to)59.5 478.15 ms
(understand when tuning hyperparameters? First, there are tons of)59.5 465.15 ms
( potential)59.5 452.15 ms
(parameters to tune in every model. And so we need to realize whi)59.5 439.15 ms
(ch)59.5 426.15 ms
(parameters are affect the model most. Of course,)59.5 413.15 ms
(all the parameters are reliable, but we kind of need to select)59.5 400.15 ms
(the most important ones. Anyway we never have time to tune)59.5 387.15 ms
(all the params, that's right. So we need to come up with a nice)59.5 374.15 ms
(subset of parameters to tune. Suppose we're new to xgboost and w)59.5 361.15 ms
(e're trying to find out what)59.5 348.15 ms
(parameters will better to tune, and say we don't even understand)59.5 335.15 ms
( how)59.5 322.15 ms
(gradient boosting decision tree works. We always can search what)59.5 309.15 ms
( parameters)59.5 296.15 ms
(people usually set when using xgboost. It's quite easy to look u)59.5 283.15 ms
(p, right? For example, at GitHub or Kaggle Kernels. Finally, the)59.5 270.15 ms
( documentation sometimes)59.5 257.15 ms
(explicitly states which parameter to tune first. From the select)59.5 244.15 ms
(ed set of parameters)59.5 231.15 ms
(we should then understand what would happen if we)59.5 218.15 ms
(change one of the parameters? How the training process and the t)59.5 205.15 ms
(raining)59.5 192.15 ms
(invalidation course will change if we, for example,)59.5 179.15 ms
(increased a certain parameter? And finally, actually tune)59.5 166.15 ms
(the selected parameters, right? Most people do it manually. Just)59.5 153.15 ms
( run, examine the logs,)59.5 140.15 ms
(change parameters, run again and)59.5 127.15 ms
(iterate till good parameters found. It is also possible to use)59.5 114.15 ms
(hyperparameter optimization tools like hyperopt, but it's usuall)59.5 101.15 ms
(y)59.5 88.15 ms
(faster to do it manually to be true. So later in this video, act)59.5 75.15 ms
(ually discuss)59.5 62.15 ms
(the most important parameters for some models along with some in)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 2 2
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                 Page 2)59.5 790.15 ms
F0 sf
(tuition how)59.5 764.15 ms
(to tune those parameters of those models. But before we start, I)59.5 751.15 ms
( actually want)59.5 738.15 ms
(to give you a list of libraries that you can use for)59.5 725.15 ms
(automatic hyperparameter tuning. There are lots of them actually)59.5 712.15 ms
(, and)59.5 699.15 ms
(I didn't try everything from this list myself, but from what I a)59.5 686.15 ms
(ctually tried,)59.5 673.15 ms
(I did not notice much difference in optimization speed on)59.5 660.15 ms
(real tasks between the libraries. But if you have time,)59.5 647.15 ms
(you can try every library and compare. From a user side these)59.5 634.15 ms
(libraries are very easy to use. We need first to define the func)59.5 621.15 ms
(tion)59.5 608.15 ms
(that will run our module, in this case, it is XGBoost. That will)59.5 595.15 ms
( run our module with)59.5 582.15 ms
(the given set of parameters and return a resulting validation sc)59.5 569.15 ms
(ore. And second,)59.5 556.15 ms
(we need to specify a source space. The range for the hyperparame)59.5 543.15 ms
(ters where)59.5 530.15 ms
(we want to look for the solution. For example, here we see that )59.5 517.15 ms
(a parameter,)59.5 504.15 ms
(it is fix 0.1. And we think that optimal max depth)59.5 491.15 ms
(is somewhere between 10 and 30. And actually that is it,)59.5 478.15 ms
(we are ready to run hyperopt. It can take much time, so)59.5 465.15 ms
(the best strategy is to run it overnight. And also please note t)59.5 452.15 ms
(hat everything)59.5 439.15 ms
(we need to know about hyperparameter's, in this case,)59.5 426.15 ms
(is an adequate range for the search. That's pretty convenient,)59.5 413.15 ms
(if you don't know the new model and you just try to run. But sti)59.5 400.15 ms
(ll,)59.5 387.15 ms
(most people tuned the models manually. So, what exactly does it)59.5 374.15 ms
(mean to understand how parameter influences the model? Broadly s)59.5 361.15 ms
(peaking, different values of parameters result)59.5 348.15 ms
(in three different fitting behavior. First, a model can underfit)59.5 335.15 ms
(. That is, it is so constrained that)59.5 322.15 ms
(it cannot even learn the train set. Another possibility is that)59.5 309.15 ms
(the model is so powerful that it just overfits to the train set )59.5 296.15 ms
(and)59.5 283.15 ms
(is not able to generalize it all. And finally, the third behavio)59.5 270.15 ms
(r is something)59.5 257.15 ms
(that we are actually looking for. It's somewhere between underfi)59.5 244.15 ms
(tting and)59.5 231.15 ms
(overfitting. So basically, what we should examine)59.5 218.15 ms
(while turning parameters is that we should try to understand if )59.5 205.15 ms
(the model)59.5 192.15 ms
(is currently underfitting or overfitting. And then, we should so)59.5 179.15 ms
(mehow)59.5 166.15 ms
(adjust the parameters to get closer to desired behavior. We need)59.5 153.15 ms
( to kind of split all the)59.5 140.15 ms
(parameters that we would like to tune into two groups. In the fi)59.5 127.15 ms
(rst group, we'll have)59.5 114.15 ms
(the parameters that constrain the model. So if we increase)59.5 101.15 ms
(the parameter from that group, the model would change its behavi)59.5 88.15 ms
(our)59.5 75.15 ms
(from overfitting to underfitting. The larger the value of the pa)59.5 62.15 ms
(rameter,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 3 3
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                 Page 3)59.5 790.15 ms
F0 sf
(the heavier the constraint. In the following videos, we'll color)59.5 764.15 ms
( such)59.5 751.15 ms
(parameters in red, and the parameters in the second group are do)59.5 738.15 ms
(ing an opposite)59.5 725.15 ms
(thing to our training process. The higher the value,)59.5 712.15 ms
(more powerful the main module. And so by increasing such paramet)59.5 699.15 ms
(ers, we can change fitting behavior)59.5 686.15 ms
(from underfitting to overfitting. We will use green color for)59.5 673.15 ms
(such parameters. So, in this video we'll be discussing some gene)59.5 660.15 ms
(ral aspects of)59.5 647.15 ms
(hyperparameter organization. Most importantly,)59.5 634.15 ms
(we've defined the color coding. If you did not understand)59.5 621.15 ms
(what color stands for what, please watch a part of)59.5 608.15 ms
(the video about it again. We'll use this color coding)59.5 595.15 ms
(throughout the following videos. [MUSIC][MUSIC] In this video, w)59.5 582.15 ms
(e will talk about)59.5 569.15 ms
(hyperparameter optimization for some tree based models. Nowadays)59.5 556.15 ms
(, XGBoost and)59.5 543.15 ms
(LightGBM became really gold standard. They are just awesome impl)59.5 530.15 ms
(ementation)59.5 517.15 ms
(of a very versatile gradient boosted decision trees model. There)59.5 504.15 ms
( is also a CatBoost library it)59.5 491.15 ms
(appeared exactly at the time when we were preparing this course,)59.5 478.15 ms
( so CatBoost)59.5 465.15 ms
(didn't have time to win people's hearts. But it looks very inter)59.5 452.15 ms
(esting and)59.5 439.15 ms
(promising, so check it out. There is a very nice)59.5 426.15 ms
(implementation of RandomForest and ExtraTrees models sklearn. Th)59.5 413.15 ms
(ese models are powerful, and)59.5 400.15 ms
(can be used along with gradient boosting. And finally, there is )59.5 387.15 ms
(a model)59.5 374.15 ms
(called regularized Greedy Forest. It showed very nice results fr)59.5 361.15 ms
(om several)59.5 348.15 ms
(competitions, but its implementation is very slow and hard to us)59.5 335.15 ms
(e, but)59.5 322.15 ms
(you can try it on small data sets. Okay, what important paramete)59.5 309.15 ms
(rs do)59.5 296.15 ms
(we have in XGBoost and LightGBM? The two libraries have similar )59.5 283.15 ms
(parameters)59.5 270.15 ms
(and we'll use names from XGBoost. And on the right half of the s)59.5 257.15 ms
(lide)59.5 244.15 ms
(you will see somehow loosely corresponding parameter)59.5 231.15 ms
(names from LightGBM. To understand the parameters,)59.5 218.15 ms
(we better understand how XGBoost and LightGBM work at least a ve)59.5 205.15 ms
(ry high level. What these models do, these models)59.5 192.15 ms
(build decision trees one after another gradually optimizing a gi)59.5 179.15 ms
(ven objective. And first there are many parameters)59.5 166.15 ms
(that control the tree building process. Max_depth is the maximum)59.5 153.15 ms
( depth of a tree. And of course, the deeper a tree can be)59.5 140.15 ms
(grown the better it can fit a dataset. So increasing this parame)59.5 127.15 ms
(ter will lead)59.5 114.15 ms
(to faster fitting to the train set. Depending on the task,)59.5 101.15 ms
(the optimal depth can vary a lot, sometimes it is 2, sometimes i)59.5 88.15 ms
(t is 27. If you increase the depth and can not get)59.5 75.15 ms
(the model to overfit, that is, the model is becoming better and )59.5 62.15 ms
(better on the)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 4 4
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                 Page 4)59.5 790.15 ms
F0 sf
(validation set as you increase the depth. It can be a sign that )59.5 764.15 ms
(there are a lot)59.5 751.15 ms
(of important interactions to extract from the data. So it's bett)59.5 738.15 ms
(er to stop tuning and)59.5 725.15 ms
(try to generate some features. I would recommend to start with)59.5 712.15 ms
(a max_depth of about seven. Also remember that as)59.5 699.15 ms
(you increase the depth, the learning will take a longer time. So)59.5 686.15 ms
( do not set depth to)59.5 673.15 ms
(a very higher values unless you are 100% sure you need it. In Li)59.5 660.15 ms
(ghtGBM,)59.5 647.15 ms
(it is possible to control the number of leaves in the tree rathe)59.5 634.15 ms
(r)59.5 621.15 ms
(than the maximum depth. It is nice since a resulting)59.5 608.15 ms
(tree can be very deep, but have small number of leaves and)59.5 595.15 ms
(not over fit. Some simple parameter controls a fraction)59.5 582.15 ms
(of objects to use when feeding a tree. It's a value between 0 an)59.5 569.15 ms
(d 1. One might think that it's better)59.5 556.15 ms
(always use all the objects, right? But in practice,)59.5 543.15 ms
(it turns out that it's not. Actually, if only a fraction of)59.5 530.15 ms
(objects is used at every duration, then the model is less)59.5 517.15 ms
(prone to overfitting. So using a fraction of objects, the model)59.5 504.15 ms
(will fit slower on the train set, but at the same time it will p)59.5 491.15 ms
(robably generalize)59.5 478.15 ms
(better than this over-fitted model. So, it works kind of as a re)59.5 465.15 ms
(gularization. Similarly, if we can consider only)59.5 452.15 ms
(a fraction of features [INAUDIBLE] split, this is controlled by )59.5 439.15 ms
(parameters)59.5 426.15 ms
(colsample_bytree and colsample_bylevel. Once again, if the model)59.5 413.15 ms
( is over fitting, you can try to lower)59.5 400.15 ms
(down these parameters. There are also various regularization)59.5 387.15 ms
(parameters, min_child_weight, lambda, alpha and others. The most)59.5 374.15 ms
( important one)59.5 361.15 ms
(is min_child_weight. If we increase it,)59.5 348.15 ms
(the model will become more conservative. If we set it to 0,)59.5 335.15 ms
(which is the minimum value for this parameter,)59.5 322.15 ms
(the model will be less constrained. In my experience, it's one o)59.5 309.15 ms
(f the most important parameters)59.5 296.15 ms
(to tune in XGBoost and LightGBM. Depending on the task,)59.5 283.15 ms
(I find optimal values to be 0, 5, 15, 300, so do not hesitate to)59.5 270.15 ms
( try a wide)59.5 257.15 ms
(range of values, it depends on the data. To this end we were dis)59.5 244.15 ms
(cussing)59.5 231.15 ms
(hyperparameters that are used to build a tree. And next, there a)59.5 218.15 ms
(re two very important)59.5 205.15 ms
(parameters that are tightly connected, eta and num_rounds. Eta i)59.5 192.15 ms
(s essentially a learning weight,)59.5 179.15 ms
(like in gradient descent. And the num_round is the how many)59.5 166.15 ms
(learning steps we want to perform or in other words how many)59.5 153.15 ms
(tree's we want to build. With each iteration)59.5 140.15 ms
(a new tree is built and added to the model with)59.5 127.15 ms
(a learning rate eta. So in general,)59.5 114.15 ms
(the higher the learning rate, the faster the model fits to the t)59.5 101.15 ms
(rain set)59.5 88.15 ms
(and probably it can lead to over fitting. And more steps model d)59.5 75.15 ms
(oes,)59.5 62.15 ms
(the better the model fits. But there are several caveats here. I)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 5 5
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                 Page 5)59.5 790.15 ms
F0 sf
(t happens that with a too high learning)59.5 764.15 ms
(rate the model will not fit at all, it will just not converge. S)59.5 751.15 ms
(o first, we need to find out if we)59.5 738.15 ms
(are using small enough learning rate. On the other hand,)59.5 725.15 ms
(if the learning rate is too small, the model will learn nothing)59.5 712.15 ms
(after a large number of rounds. But at the same time, small lear)59.5 699.15 ms
(ning rate)59.5 686.15 ms
(often leads to a better generalization. So it means that learnin)59.5 673.15 ms
(g)59.5 660.15 ms
(rate should be just right, so that the model generalize and)59.5 647.15 ms
(doesn't take forever to train. The nice thing is that we can fre)59.5 634.15 ms
(eze)59.5 621.15 ms
(eta to be reasonably small, say, 0.1 or 0.01, and then find how )59.5 608.15 ms
(many rounds we)59.5 595.15 ms
(should train the model til it over fits. We usually use early st)59.5 582.15 ms
(opping for it. We monitor the validation loss and exit)59.5 569.15 ms
(the training when loss starts to go up. Now when we found)59.5 556.15 ms
(the right number of rounds, we can do a trick that)59.5 543.15 ms
(usually improves the score. We multiply the number of)59.5 530.15 ms
(steps by a factor of alpha and at the same time,)59.5 517.15 ms
(we divide eta by the factor of alpha. For example, we double the)59.5 504.15 ms
( number)59.5 491.15 ms
(of steps and divide eta by 2. In this case, the learning will)59.5 478.15 ms
(take twice longer in time, but the resulting model)59.5 465.15 ms
(usually becomes better. It may happen that the valid parameters)59.5 452.15 ms
(will need to be adjusted too, but usually it's okay to leave the)59.5 439.15 ms
(m as is. Finally, you may want to)59.5 426.15 ms
(use random seed argument, many people recommend to)59.5 413.15 ms
(fix seed before hand. I think it doesn't make too much)59.5 400.15 ms
(sense to fix seed in XGBoost, as anyway every changed parameter )59.5 387.15 ms
(will)59.5 374.15 ms
(lead to completely different model. But I would use this)59.5 361.15 ms
(parameter to verify that different random seeds do not)59.5 348.15 ms
(change training results much. Say [INAUDIBLE] competition,)59.5 335.15 ms
(one could jump 1,000 places up or down on the leaderboard just b)59.5 322.15 ms
(y training)59.5 309.15 ms
(a model with different random seeds. If random seed doesn't)59.5 296.15 ms
(affect model too much, good. In other case, I suggest you to thi)59.5 283.15 ms
(nk)59.5 270.15 ms
(one more time if it's a good idea to participate in that competi)59.5 257.15 ms
(tion as)59.5 244.15 ms
(the results can be quite random. Or at least I suggest you to ad)59.5 231.15 ms
(just)59.5 218.15 ms
(validation scheme and account for the randomness. All right,)59.5 205.15 ms
(we're finished with gradient boosting. Now let's get to RandomFo)59.5 192.15 ms
(rest and)59.5 179.15 ms
(ExtraTrees. In fact, ExtraTrees is just a more)59.5 166.15 ms
(randomized version of RandomForest and has the same parameters. )59.5 153.15 ms
(So I will say RandomForest)59.5 140.15 ms
(meaning both of the models. RandomForest and ExtraBoost build tr)59.5 127.15 ms
(ees,)59.5 114.15 ms
(one tree after another. But, RandomForest builds each)59.5 101.15 ms
(tree to be independent of others. It means that having a lot of )59.5 88.15 ms
(trees)59.5 75.15 ms
(doesn't lead to overfeeding for RandomForest as opposed)59.5 62.15 ms
(to gradient boosting. In sklearn, the number of trees for random)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 6 6
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                 Page 6)59.5 790.15 ms
F0 sf
( forest is controlled)59.5 764.15 ms
(by N_estimators parameter. At the start, we may want to determin)59.5 751.15 ms
(e what number)59.5 738.15 ms
(of trees is sufficient to have. That is, if we use more than tha)59.5 725.15 ms
(t,)59.5 712.15 ms
(the result will not change much, but the models will fit longer.)59.5 699.15 ms
( I usually first set N_estimators)59.5 686.15 ms
(to very small number, say 10, and see how long does it take)59.5 673.15 ms
(to fit 10 trees on that data. If it is not too long then I set)59.5 660.15 ms
(N_estimators to a huge value, say 300, but it actually depends. )59.5 647.15 ms
(And feed the model. And then I plot how the validation)59.5 634.15 ms
(error changed depending on a number of used trees. This plot usu)59.5 621.15 ms
(ally looks like that. We have number of trees on the x-axis and)59.5 608.15 ms
(the accuracy score on y-axis. We see here that about 50 trees)59.5 595.15 ms
(already give reasonable score and we don't need to use more)59.5 582.15 ms
(while tuning parameter. It's pretty reliable to use 50 trees. Be)59.5 569.15 ms
(fore submitting to leaderboard, we can set N_estimators to)59.5 556.15 ms
(a higher value just to be sure. You can find code for this plot,)59.5 543.15 ms
(actually, in the reading materials. Similarly to XGBoost, there )59.5 530.15 ms
(is a parameter max_depth)59.5 517.15 ms
(that controls depth of the trees. But differently to XGBoost, it)59.5 504.15 ms
( can be set to none,)59.5 491.15 ms
(which corresponds to unlimited depth. It can be very useful actu)59.5 478.15 ms
(ally when)59.5 465.15 ms
(the features in the data set have repeated values and important )59.5 452.15 ms
(interactions. In other cases, the model with unconstrained)59.5 439.15 ms
(depth will over fit immediately. I recommend you to start with a)59.5 426.15 ms
( depth)59.5 413.15 ms
(of about 7 for random forest. Usually an optimal depth for)59.5 400.15 ms
(random forests is higher than for gradient boosting, so do not h)59.5 387.15 ms
(esitate)59.5 374.15 ms
(to try a depth 10, 20, and higher. Max_features is similar to ca)59.5 361.15 ms
(ll)59.5 348.15 ms
(sample parameter from XGBoost. The more features I use to deciph)59.5 335.15 ms
(er)59.5 322.15 ms
(a split, the faster the training. But on the other hand,)59.5 309.15 ms
(you don't want to use too few features. And min_samples_leaf is)59.5 296.15 ms
(a regularization parameter similar to min_child_weight from XGBo)59.5 283.15 ms
(ost and)59.5 270.15 ms
(the same as min_data_leaf from LightGPM. For Random Forest class)59.5 257.15 ms
(ifier,)59.5 244.15 ms
(we can select a criterion to eleviate a split in the tree with)59.5 231.15 ms
(a criterion parameter. It can be either Gini or Entropy. To choo)59.5 218.15 ms
(se one, we should just try both and)59.5 205.15 ms
(pick the best performing one. In my experience Gini is better mo)59.5 192.15 ms
(re)59.5 179.15 ms
(often, but sometimes Entropy wins. We can also fix random seed u)59.5 166.15 ms
(sing)59.5 153.15 ms
(random_state parameter, if we want. And finally, do not forget t)59.5 140.15 ms
(o set n_jobs)59.5 127.15 ms
(parameter to a number of cores you have. As by default, RandomFo)59.5 114.15 ms
(rest from sklearn)59.5 101.15 ms
(uses only one core for some reason. So in this video, we were ta)59.5 88.15 ms
(lking)59.5 75.15 ms
(about various hyperparameters of gradient boost and)59.5 62.15 ms
(decision trees, and random forest. In the following video, we'll)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 7 7
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                 Page 7)59.5 790.15 ms
F0 sf
( discuss neural networks and linear models. [MUSIC][MUSIC] In th)59.5 764.15 ms
(is video we'll briefly discuss)59.5 751.15 ms
(neural network libraries and then we'll see how to tune hyperpar)59.5 738.15 ms
(ameters)59.5 725.15 ms
(for neural networks and linear models. There are so many framewo)59.5 712.15 ms
(rks,)59.5 699.15 ms
(Keras, TensorFlow, MxNet, PyTorch. The choice is really personal)59.5 686.15 ms
(, all frameworks implement more than enough)59.5 673.15 ms
(functionality for competition tasks. Keras is for sure the most )59.5 660.15 ms
(popular in)59.5 647.15 ms
(Kaggle and has very simple interface. It takes only several doze)59.5 634.15 ms
(n lines)59.5 621.15 ms
(to train a network using Keras. TensorFlow is extensively used)59.5 608.15 ms
(by companies for production. And PyTorch is very popular in)59.5 595.15 ms
(deep learning research community. I personally recommend)59.5 582.15 ms
(you to try PyTorch and Keras as they are most transparent and)59.5 569.15 ms
(easy to use frameworks. Now, how do you tune)59.5 556.15 ms
(hyperparameters in a network? We'll now talk about only)59.5 543.15 ms
(dense neural networks, that is the networks that consist)59.5 530.15 ms
(only of fully connected layers. Say we start with a three)59.5 517.15 ms
(layer neural network, what do we expect to happen if we)59.5 504.15 ms
(increase the number of neurons per layer? The network now can le)59.5 491.15 ms
(arn more)59.5 478.15 ms
(complex decision boundaries and so it will over fit faster. The )59.5 465.15 ms
(same should happen when the number)59.5 452.15 ms
(of layers are increased, but due to optimization problems,)59.5 439.15 ms
(the learning can even stop to converge. But anyway, if you think)59.5 426.15 ms
( your)59.5 413.15 ms
(network is not powerful enough, you can try to add another layer)59.5 400.15 ms
( and)59.5 387.15 ms
(see what happens. My recommendation here is to)59.5 374.15 ms
(start with something very simple, say 1 or 2 layer and 64 units )59.5 361.15 ms
(per layer. Debug the code, make sure the training and)59.5 348.15 ms
([INAUDIBLE] losses go down. And then try to find a configuration)59.5 335.15 ms
( that)59.5 322.15 ms
(is able to overfit the training set, just as another sanity chec)59.5 309.15 ms
(k. After it, it is time to tune)59.5 296.15 ms
(something in the network. One of the crucial parts of neural)59.5 283.15 ms
(network is selected optimization method. Broadly speaking, we ca)59.5 270.15 ms
(n pick either)59.5 257.15 ms
(vanilla stochastic gradient descent with momentum or)59.5 244.15 ms
(one of modern adaptive methods like Adam, Adadelta, Adagrad and )59.5 231.15 ms
(so on. On this slide,)59.5 218.15 ms
(the adaptive methods are colored in green, as compared to SGD in)59.5 205.15 ms
( red. I want to show here that adaptive)59.5 192.15 ms
(methods do really allow you to fit the training set faster. But )59.5 179.15 ms
(in my experience,)59.5 166.15 ms
(they also lead to overfitting. Plain old stochastic gradient)59.5 153.15 ms
(descent converges slower, but the trained network usually)59.5 140.15 ms
(generalizes better. Adaptive methods are useful, but in the sett)59.5 127.15 ms
(ings others in)59.5 114.15 ms
(classification and regression. Now here is a question for you. J)59.5 101.15 ms
(ust keep the size. What should we expect when)59.5 88.15 ms
(increasing a batch size with other hyperparameters fixed? In fac)59.5 75.15 ms
(t, it turns out that huge batch)59.5 62.15 ms
(size leads to more overfitting. Say a batch of 500 objects)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 8 8
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                 Page 8)59.5 790.15 ms
F0 sf
(is large in experience. I recommend to pick a value around 32 or)59.5 764.15 ms
(64. Then if you see the network is)59.5 751.15 ms
(still overfitting try to decrease the batch size. If it is under)59.5 738.15 ms
(fitting, try to increase it. Know that is a the number)59.5 725.15 ms
(of outbox is fixed, then a network with a batch)59.5 712.15 ms
(size reduced by a factor of 2 gets updated twice more times)59.5 699.15 ms
(compared to original network. So take this into consideration. M)59.5 686.15 ms
(aybe you need to reduce the number of)59.5 673.15 ms
(networks or at least somehow adjust it. The batch size also shou)59.5 660.15 ms
(ld not be too)59.5 647.15 ms
(small, the gradient will be too noisy. Same as in gradient boost)59.5 634.15 ms
(ing,)59.5 621.15 ms
(we need to set the proper learning rate. When the learning rate )59.5 608.15 ms
(is too high,)59.5 595.15 ms
(network will not converge and with too small a learning rate,)59.5 582.15 ms
(the network will learn forever. The learning rate should be)59.5 569.15 ms
(not too high and not too low. So the optimal learning rate)59.5 556.15 ms
(depends on the other parameters. I usually start with a huge lea)59.5 543.15 ms
(rning rate,)59.5 530.15 ms
(say 0.1, and try to lower it down till I find one with which net)59.5 517.15 ms
(work converges)59.5 504.15 ms
(and then I try to revise further. Interestingly, there is a conn)59.5 491.15 ms
(ection)59.5 478.15 ms
(between the batch size and the learning rate. It is theoreticall)59.5 465.15 ms
(y grounded for)59.5 452.15 ms
(a specific type of models, but people usually use it,)59.5 439.15 ms
(well actually some people use it as a rule of thumb with neural )59.5 426.15 ms
(networks. The connection is the following. If you increase the b)59.5 413.15 ms
(atch)59.5 400.15 ms
(size by a factor of alpha, you can also increase the learning)59.5 387.15 ms
(rate by the same factor. But remember that the larger batch size)59.5 374.15 ms
(, the more your network is)59.5 361.15 ms
(prone to overfitting. So you need a good regularization here. So)59.5 348.15 ms
(metime ago, people mostly use L2 and)59.5 335.15 ms
(L1 regularization for weights. Nowadays, most people use)59.5 322.15 ms
(dropout regularization. So whenever you see a network overfittin)59.5 309.15 ms
(g,)59.5 296.15 ms
(try first to a dropout layer. You can override dropout probabili)59.5 283.15 ms
(ty and a)59.5 270.15 ms
(place where you insert the dropout layer. Usually people add the)59.5 257.15 ms
( dropout layer)59.5 244.15 ms
(closer to the end of the network, but it's okay to add some drop)59.5 231.15 ms
(out)59.5 218.15 ms
(to every layer, it also works. Dropout helps network to find fea)59.5 205.15 ms
(tures)59.5 192.15 ms
(that really matters, and what never worked for me is to have dro)59.5 179.15 ms
(pout as the very)59.5 166.15 ms
(first layer, immediately after data layer. This way some informa)59.5 153.15 ms
(tion is lost)59.5 140.15 ms
(completely at the very beginning of the network and)59.5 127.15 ms
(we observe performance degradation. An interesting regularizatio)59.5 114.15 ms
(n)59.5 101.15 ms
(technique that we used in the [UNKOWN] competition is)59.5 88.15 ms
(static dropconnect, as we call it. So recall that, usually we ha)59.5 75.15 ms
(ve an input)59.5 62.15 ms
(layer densely connected to, say 128 units. We will instead use a)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 9 9
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                 Page 9)59.5 790.15 ms
F0 sf
( first)59.5 764.15 ms
(hidden layer with a very huge number of units, say 4,096 units. )59.5 751.15 ms
(This is a huge network for a usual)59.5 738.15 ms
(competition and it will overfeed badly. But now to regularlize i)59.5 725.15 ms
(t,)59.5 712.15 ms
(we'll at random drop 99% of connections between the input layer )59.5 699.15 ms
(and)59.5 686.15 ms
(the first hidden layer. We call it static dropconnect)59.5 673.15 ms
(because originally in dropconnect, we need to drop random connec)59.5 660.15 ms
(tions at)59.5 647.15 ms
(every learning iterations while we fix connectivity pattern for )59.5 634.15 ms
(the network)59.5 621.15 ms
(for the whole learning process. So you see the point, we increas)59.5 608.15 ms
(e)59.5 595.15 ms
(the number of hidden units, but the number of parameters in the )59.5 582.15 ms
(first)59.5 569.15 ms
(hidden layer remains small. Notice that anyway the weight matrix)59.5 556.15 ms
(of the second layer becomes huge, but it turns out to be)59.5 543.15 ms
(okay in the practice. This is very powerful regularizations. And)59.5 530.15 ms
( more of the networks with)59.5 517.15 ms
(different connectivity patterns makes much nicer than networks)59.5 504.15 ms
(without static dropconnect. All right, last class of models)59.5 491.15 ms
(to discuss are my neuro models. Yet, a carefully tuned live GPM )59.5 478.15 ms
(would)59.5 465.15 ms
(probably beat support vector machines, even on a large, sparse d)59.5 452.15 ms
(ata set. SVM's do not require almost any tuning,)59.5 439.15 ms
(which is truly beneficial. SVM's for classification and regressi)59.5 426.15 ms
(on)59.5 413.15 ms
(are implemented in SK learners or wrappers to algorithms from li)59.5 400.15 ms
(braries)59.5 387.15 ms
(called libLinear and libSVM. The latest version of libLinear and)59.5 374.15 ms
(libSVM support multicore competitions, but unfortunately it is n)59.5 361.15 ms
(ot possible)59.5 348.15 ms
(to use multicore version in Sklearn, so we need to compile these)59.5 335.15 ms
( libraries)59.5 322.15 ms
(manually to use this option. And I've never had anyone)59.5 309.15 ms
(use kernel SVC lately, so in this video we will)59.5 296.15 ms
(talk only about linear SVM. In Sklearn we can also find logistic)59.5 283.15 ms
( and)59.5 270.15 ms
(linear regression with various regularization options and also,)59.5 257.15 ms
(as your declassifier and regressor. We've already mentioned them)59.5 244.15 ms
(while discussing metrics. For the data sets that do not fit in)59.5 231.15 ms
(the memory, we can use Vowpal Wabbit. It implements learning of )59.5 218.15 ms
(linear)59.5 205.15 ms
(models in online fashion. It only reads data row by row)59.5 192.15 ms
(directly from the hard drive and never loads the whole)59.5 179.15 ms
(data set in the memory. Thus, allowing to learn)59.5 166.15 ms
(on a very huge data sets. A method of online learning for linear)59.5 153.15 ms
(models called flow the regularized leader or FTRL in short was p)59.5 140.15 ms
(articularly)59.5 127.15 ms
(popular some time ago. It is implemented in Vowpal Wabbit but th)59.5 114.15 ms
(ere are also lots of)59.5 101.15 ms
(implementations in pure Python. The hyperparameters we usually n)59.5 88.15 ms
(eed)59.5 75.15 ms
(to tune linear models are L2 and L1 regularization of weights. O)59.5 62.15 ms
(nce again, regularization is denoted)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 10 10
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 10)59.5 790.15 ms
F0 sf
(with red color because of the higher the regularization weight i)59.5 764.15 ms
(s the more)59.5 751.15 ms
(model struggle to learn something. But know that, the parameter )59.5 738.15 ms
(see in)59.5 725.15 ms
(SVM is inversely proportional to regularization weight, so)59.5 712.15 ms
(the dynamics is opposite. In fact, we do not need to think about)59.5 699.15 ms
( the)59.5 686.15 ms
(meaning of the parameters in the case of one parameter, right? W)59.5 673.15 ms
(e just try several values and)59.5 660.15 ms
(find one that works best. For SVM, I usually start a very small)59.5 647.15 ms
(seed, say 10 to the power of minus 6 and then I try to increase )59.5 634.15 ms
(it,)59.5 621.15 ms
(multiply each time by a factor of 10. I start from small values )59.5 608.15 ms
(because)59.5 595.15 ms
(the larger the parameter C is, the longer the training takes. Wh)59.5 582.15 ms
(at type of regularization,)59.5 569.15 ms
(L1 or L2 do you choose? Actually, my answer is try both. To my m)59.5 556.15 ms
(ind actually they are quite similar)59.5 543.15 ms
(and one benefit that L1 can give us is weight sparsity, so the s)59.5 530.15 ms
(parsity)59.5 517.15 ms
(pattern can be used for feature selection. A general advise I wa)59.5 504.15 ms
(nt to give)59.5 491.15 ms
(here do not spend too much time on tuning hyperparameters, espec)59.5 478.15 ms
(ially)59.5 465.15 ms
(when the competition has only begun. You cannot win a competitio)59.5 452.15 ms
(n)59.5 439.15 ms
(by tuning parameters. Appropriate features, hacks,)59.5 426.15 ms
(leaks, and insights will give you much more than carefully)59.5 413.15 ms
(tuned model built on default features. I also advice you to be p)59.5 400.15 ms
(atient. It was my personal mistake several times. I hated to spe)59.5 387.15 ms
(nd more then ten minutes)59.5 374.15 ms
(on training models and was amazed how much the models could impr)59.5 361.15 ms
(ove if I)59.5 348.15 ms
(would let it train for a longer time. And finally, average every)59.5 335.15 ms
(thing. When submitting, learn five models)59.5 322.15 ms
(starting from different random initializations and)59.5 309.15 ms
(average their predictions. It helps a lot actually and)59.5 296.15 ms
(some people average not only random seed, but also other paramet)59.5 283.15 ms
(ers)59.5 270.15 ms
(around an optimal value. For example, if optimal depth for)59.5 257.15 ms
(extra boost is 5, we can average 3 digiboosts with depth 3,)59.5 244.15 ms
(4, and 5. Wow, it would be better if we could averaged 3 digiboo)59.5 231.15 ms
(sts with depth 4,)59.5 218.15 ms
(5, and 6. Finally, in this lecture, we discussed what is a gener)59.5 205.15 ms
(al pipeline)59.5 192.15 ms
(for a hyperparameter optimization. And we saw, in particular,)59.5 179.15 ms
(what important hyperparameters derive for several models,)59.5 166.15 ms
(gradient boosting decision trees, random forests and extra trees)59.5 153.15 ms
(,)59.5 140.15 ms
(neural networks, and linear models. I hope you found something i)59.5 127.15 ms
(nteresting)59.5 114.15 ms
(in this lecture and see you later. [MUSIC][SOUND] Hi, to this mo)59.5 101.15 ms
(ment,)59.5 88.15 ms
(we have already discussed all basics new things which build up t)59.5 75.15 ms
(o)59.5 62.15 ms
(a big solution like featured generation, validation, minimalist )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 11 11
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 11)59.5 790.15 ms
F0 sf
(codings and so on. We went through several)59.5 764.15 ms
(competitions together and tried our best to unite everything)59.5 751.15 ms
(we learn into one huge framework. But as with any other set of t)59.5 738.15 ms
(ools,)59.5 725.15 ms
(there are a lot of heuristics which people often find only with )59.5 712.15 ms
(a trial and)59.5 699.15 ms
(error approach, spending significant time on learning)59.5 686.15 ms
(how to use these tools efficiently. So to help you out here, in )59.5 673.15 ms
(this video we'll share things we)59.5 660.15 ms
(learned the hard way, by experience. These things may vary from)59.5 647.15 ms
(one person to another. So we decided that everyone on class will)59.5 634.15 ms
(present his own guidelines personally, to stress the possible)59.5 621.15 ms
(diversity in a broad issues and to make an accent on different m)59.5 608.15 ms
(oments. Some notes might seem obvious to you,)59.5 595.15 ms
(some may not. But be sure for even some of them or)59.5 582.15 ms
(at least no one involve them. Can save you a lot of time. So, le)59.5 569.15 ms
(t's start. When we want to enter a competition,)59.5 556.15 ms
(define your goals and try to estimate what you can)59.5 543.15 ms
(get out of your participation. You may want to learn more)59.5 530.15 ms
(about an interesting problem. You may want to get acquainted)59.5 517.15 ms
(with new software tools and packages, or)59.5 504.15 ms
(you may want to try to hunt for a medal. Each of these goals wil)59.5 491.15 ms
(l influence what)59.5 478.15 ms
(competition you choose to participate in. If you want to learn m)59.5 465.15 ms
(ore)59.5 452.15 ms
(about an interesting problem, you may want the competition to ha)59.5 439.15 ms
(ve)59.5 426.15 ms
(a wide discussion on the forums. For example, if you are interes)59.5 413.15 ms
(ted in)59.5 400.15 ms
(data science, in application to medicine, you can try to predict)59.5 387.15 ms
( lung cancer)59.5 374.15 ms
(in the Data Science Bowl 2017. Or to predict seizures in long)59.5 361.15 ms
(term human EEG recordings. In the Melbourne University)59.5 348.15 ms
(Seizure Prediction Competition. If you want to get acquainted)59.5 335.15 ms
(with new software tools, you may want the competition)59.5 322.15 ms
(to have required tutorials. For example, if you want to)59.5 309.15 ms
(learn a neural networks library. You may choose any of competiti)59.5 296.15 ms
(ons with)59.5 283.15 ms
(images like the nature conservancy features, monitoring competit)59.5 270.15 ms
(ion. Or the planet, understanding)59.5 257.15 ms
(the Amazon from space competition. And if you want to try to hun)59.5 244.15 ms
(t for a medal, you may want to check how)59.5 231.15 ms
(many submissions do participants have. And if the points that pe)59.5 218.15 ms
(ople have)59.5 205.15 ms
(over one hundred submissions, it can be a clear sign of legible)59.5 192.15 ms
(problem or difficulties in validation includes an inconsistency )59.5 179.15 ms
(of)59.5 166.15 ms
(validation and leaderboard scores. On the other hand, if there a)59.5 153.15 ms
(re people)59.5 140.15 ms
(with few submissions in the top, that usually means there should)59.5 127.15 ms
( be)59.5 114.15 ms
(a non-trivial approach to this competition or it's discovered on)59.5 101.15 ms
(ly by few people. Beside that, you may want to pay)59.5 88.15 ms
(attention to the size of the top teams. If leaderboard mostly co)59.5 75.15 ms
(nsists of)59.5 62.15 ms
(teams with only one participant, you'll probably have enough)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 12 12
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 12)59.5 790.15 ms
F0 sf
(chances if you gather a good team. Now, let's move to the next s)59.5 764.15 ms
(tep)59.5 751.15 ms
(after you chose a competition. As soon as you get familiar with )59.5 738.15 ms
(the data, start to write down your ideas about)59.5 725.15 ms
(what you may want to try later. What things could work here? Wha)59.5 712.15 ms
(t approaches you may want to take. After you're done, read forum)59.5 699.15 ms
(s and)59.5 686.15 ms
(highlight interesting posts and topics. Remember, you can get a )59.5 673.15 ms
(lot of information)59.5 660.15 ms
(and meet new people on forums. So I strongly encourage you to)59.5 647.15 ms
(participate in these discussions. After the initial pipeline is )59.5 634.15 ms
(ready and you roll down few ideas, you may want)59.5 621.15 ms
(to start improving your solution. Personally, I like to organize)59.5 608.15 ms
(these ideas into some structure. So you may want to sort)59.5 595.15 ms
(ideas into priority order. Most important and)59.5 582.15 ms
(promising needs to be implemented first. Or you may want to orga)59.5 569.15 ms
(nize)59.5 556.15 ms
(these ideas into topics. Ideas about feature generation,)59.5 543.15 ms
(validation, metric optimization. And so on. Now pick up an idea )59.5 530.15 ms
(and implement it. Try to derive some insights on the way. Especi)59.5 517.15 ms
(ally, try to understand why)59.5 504.15 ms
(something does or doesn't work. For example, you have an idea ab)59.5 491.15 ms
(out trying a deep)59.5 478.15 ms
(gradient boosting decision tree model. To your joy, it works. No)59.5 465.15 ms
(w, ask yourself why? Is there some hidden data)59.5 452.15 ms
(structure we didn't notice before? Maybe you have categorical fe)59.5 439.15 ms
(atures)59.5 426.15 ms
(with a lot of unique values. If this is the case, you as well ca)59.5 413.15 ms
(n make a conclusion that)59.5 400.15 ms
(mean encodings may work great here. So in some sense,)59.5 387.15 ms
(the ability to analyze the work and derive conclusions while)59.5 374.15 ms
(you're trying out your ideas will get you on the right track to)59.5 361.15 ms
(reveal hidden data patterns and leaks. After we checked out most)59.5 348.15 ms
( important ideas, you may want to switch)59.5 335.15 ms
(to parameter training. I personally like the view,)59.5 322.15 ms
(everything is a parameter. From the number of features, through)59.5 309.15 ms
(gradient boosting decision through depth. From the number of lay)59.5 296.15 ms
(ers in)59.5 283.15 ms
(convolutional neural network, to the coefficient you finally)59.5 270.15 ms
(submit is multiplied by. To understand what I should tune and ch)59.5 257.15 ms
(ange first, I like to sort all)59.5 244.15 ms
(parameters by these principles. First, importance. Arrange param)59.5 231.15 ms
(eters from)59.5 218.15 ms
(important to not useful at all. Tune in this order. These may de)59.5 205.15 ms
(pend on data structure,)59.5 192.15 ms
(on target, on metric, and so on. Second, feasibility. Rate param)59.5 179.15 ms
(eters from, it is easy to tune,)59.5 166.15 ms
(to, tuning this can take forever. Third, understanding. Rate par)59.5 153.15 ms
(ameters from, I know what)59.5 140.15 ms
(it's doing, to, I have no idea. Here it is important to understa)59.5 127.15 ms
(nd)59.5 114.15 ms
(what each parameter will change in the whole pipeline. For examp)59.5 101.15 ms
(le, if you increase)59.5 88.15 ms
(the number of features significantly, you may want to change rat)59.5 75.15 ms
(io of columns)59.5 62.15 ms
(which is used to find the best split in gradient boosting decisi)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 13 13
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 13)59.5 790.15 ms
F0 sf
(on tree. Or, if you change number of layers)59.5 764.15 ms
(in convolution neural network, you will need more reports to tra)59.5 751.15 ms
(in it,)59.5 738.15 ms
(and so on. So let's see, these were some)59.5 725.15 ms
(of my practical guidelines, I hope they will prove useful for)59.5 712.15 ms
(you as well. Every problem starts with data loading and)59.5 699.15 ms
(preprocessing. I usually don't pay much attention to)59.5 686.15 ms
(some sub optimal usage of computational resources but this parti)59.5 673.15 ms
(cular)59.5 660.15 ms
(case is of crucial importance. Doing things right at the very be)59.5 647.15 ms
(ginning)59.5 634.15 ms
(will make your life much simpler and will allow you to save a lo)59.5 621.15 ms
(t of time and)59.5 608.15 ms
(computational resources. I usually start with basic data)59.5 595.15 ms
(preprocessing like labeling, coding, category recovery,)59.5 582.15 ms
(both enjoying additional data. Then, I dump resulting data into )59.5 569.15 ms
(HDF5 or)59.5 556.15 ms
(MPI format. HDF5 for Panda's dataframes,)59.5 543.15 ms
(and MPI for non bit arrays. Running experiment often require)59.5 530.15 ms
(a lot of kernel restarts, which leads to reloading all the data.)59.5 517.15 ms
( And loading class CSC files)59.5 504.15 ms
(may take minutes while loading data from HDF5 or MPI formats)59.5 491.15 ms
(is performed in a matter of seconds. Another important matter is)59.5 478.15 ms
( that by)59.5 465.15 ms
(default, Panda is known to store data in 64-bit arrays, which is)59.5 452.15 ms
(unnecessary in most of the situations. Downcasting everything to)59.5 439.15 ms
( 32 bits will)59.5 426.15 ms
(result in two-fold memory saving. Also keep in mind that Panda's)59.5 413.15 ms
( support)59.5 400.15 ms
(out of the box data relink by chunks, via chunks ice parameter)59.5 387.15 ms
(in recess fee function. So most of the data sets may be)59.5 374.15 ms
(processed without a lot of memory. When it comes to performance )59.5 361.15 ms
(evaluation, I)59.5 348.15 ms
(am not a big fan of extensive validation. Even for medium-sized )59.5 335.15 ms
(datasets)59.5 322.15 ms
(like 50,000 or 100,000 rows. You can validate your models)59.5 309.15 ms
(with a simple train test split instead of full cross validation )59.5 296.15 ms
(loop. Switch to full CV only)59.5 283.15 ms
(when it is really needed. For example,)59.5 270.15 ms
(when you've already hit some limits and can move forward only wi)59.5 257.15 ms
(th)59.5 244.15 ms
(some marginal improvements. Same logic applies to)59.5 231.15 ms
(initial model choice. I usually start with LightGBM,)59.5 218.15 ms
(find some reasonably good parameters, and evaluate performance o)59.5 205.15 ms
(f my features. I want to emphasize that)59.5 192.15 ms
(I use early stopping, so I don't need to tune number)59.5 179.15 ms
(of boosting iterations. And god forbid start ESVMs,)59.5 166.15 ms
(random forks, or neural networks, you will waste too)59.5 153.15 ms
(much time just waiting for them to feed. I switch to tuning the )59.5 140.15 ms
(models,)59.5 127.15 ms
(and sampling, and staking, only when I am satisfied)59.5 114.15 ms
(with feature engineering. In some ways, I describe my approach a)59.5 101.15 ms
(s,)59.5 88.15 ms
(fast and dirty, always better. Try focusing on what is really im)59.5 75.15 ms
(portant,)59.5 62.15 ms
(the data. Do ED, try different features. Google domain-specific )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 14 14
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 14)59.5 790.15 ms
F0 sf
(knowledge. Your code is secondary. Creating unnecessary classes )59.5 764.15 ms
(and personal frame box may only make)59.5 751.15 ms
(things harder to change and will result in wasting your time, so)59.5 738.15 ms
(keep things simple and reasonable. Don't track every little chan)59.5 725.15 ms
(ge. By the end of competition, I usually)59.5 712.15 ms
(have only a couple of notebooks for model training and to want n)59.5 699.15 ms
(otebooks)59.5 686.15 ms
(specifically for EDA purposes. Finally, if you feel really uncom)59.5 673.15 ms
(fortable)59.5 660.15 ms
(with given computational resources, don't struggle for weeks,)59.5 647.15 ms
(just rent a larger server. Every competition I start with)59.5 634.15 ms
(a very simple basic solution that can be even primitive. The mai)59.5 621.15 ms
(n purpose of such solution)59.5 608.15 ms
(is not to build a good model but to debug full pipeline from ver)59.5 595.15 ms
(y beginning of the data to the very end when we write)59.5 582.15 ms
(the submit file into decided format. I advise you to start with)59.5 569.15 ms
(construction of the initial pipeline. Often you can find it in b)59.5 556.15 ms
(aseline)59.5 543.15 ms
(solutions provided by organizers or in kernels. I encourage you )59.5 530.15 ms
(to read carefully and)59.5 517.15 ms
(write your own. Also I advise you to follow from simple)59.5 504.15 ms
(to complex approach in other things. For example, I prefer to st)59.5 491.15 ms
(art)59.5 478.15 ms
(with Random Forest rather than Gradient Boosted Decision Trees. )59.5 465.15 ms
(At least Random Forest)59.5 452.15 ms
(works quite fast and requires almost no tuning)59.5 439.15 ms
(of hybrid parameters. Participation in data science competition)59.5 426.15 ms
(implies the analysis of data and generation of features and)59.5 413.15 ms
(manipulations with models. This process is very similar in spiri)59.5 400.15 ms
(t)59.5 387.15 ms
(to the development of software and there are many good practices)59.5 374.15 ms
(that I advise you to follow. I will name just a few of them. Fir)59.5 361.15 ms
(st of all, use good variable names. No matter how ingenious you )59.5 348.15 ms
(are,)59.5 335.15 ms
(if your code is written badly, you will surely get confused in i)59.5 322.15 ms
(t and)59.5 309.15 ms
(you'll have a problem sooner or later. Second, keep your researc)59.5 296.15 ms
(h reproducible. FIx all random seeds. Write down exactly how)59.5 283.15 ms
(a feature was generated, and store the code under version)59.5 270.15 ms
(control system like git. Very often there are situation when you)59.5 257.15 ms
(need to go back to the model that you built two weeks ago and)59.5 244.15 ms
(edit to the ensemble width. The last and probably the most)59.5 231.15 ms
(important thing, reuse your code. It's really important to use t)59.5 218.15 ms
(he same)59.5 205.15 ms
(code at training and testing stages. For example, features shoul)59.5 192.15 ms
(d be prepared)59.5 179.15 ms
(and transforming by the same code in order to guarantee that the)59.5 166.15 ms
(y're)59.5 153.15 ms
(produced in a consistent manner. Here in such places are very)59.5 140.15 ms
(difficult to catch, so it's better to be very careful with it. I)59.5 127.15 ms
( recommend to move reusable code into)59.5 114.15 ms
(separate functions or even separate model. In addition, I advise)59.5 101.15 ms
( you to read)59.5 88.15 ms
(scientific articles on the topic of the competition. They can pr)59.5 75.15 ms
(ovide you with)59.5 62.15 ms
(information about machine and correlated things like for example)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 15 15
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 15)59.5 790.15 ms
F0 sf
( how)59.5 764.15 ms
(to better optimize a measure, or AUC. Or, provide the main)59.5 751.15 ms
(knowledge of the problem. This is often very useful for)59.5 738.15 ms
(future generations. For example, during Microsoft Mobile)59.5 725.15 ms
(competition, I read article about mobile detection and used idea)59.5 712.15 ms
(s from)59.5 699.15 ms
(them to generate new features. >> I usually start the competitio)59.5 686.15 ms
(n by)59.5 673.15 ms
(monitoring the forums and kernels. It happens that a competition)59.5 660.15 ms
( starts,)59.5 647.15 ms
(someone finds a bug in the data. And the competition data is)59.5 634.15 ms
(then completely changed, so I never join a competition)59.5 621.15 ms
(at its very beginning. I usually start a competition with)59.5 608.15 ms
(a quick EDA and a simple baseline. I tried to check the data for)59.5 595.15 ms
(various leakages. For me, the leaks are one of the most)59.5 582.15 ms
(interesting parts in the competition. I then usually do several )59.5 569.15 ms
(submissions)59.5 556.15 ms
(to check if validation score correlates with publicly the board )59.5 543.15 ms
(score. Usually, I try to come up with a list)59.5 530.15 ms
(of things to try in a competition, and I more or less try to fol)59.5 517.15 ms
(low it. But sometimes I just try to generate)59.5 504.15 ms
(as many features as possible, put them in extra boost and)59.5 491.15 ms
(study what helps and what does not. When tuning parameters,)59.5 478.15 ms
(I first try to make model overfit to the training set and only t)59.5 465.15 ms
(hen I)59.5 452.15 ms
(change parameters to constrain the model. I had situations when )59.5 439.15 ms
(I could not)59.5 426.15 ms
(reproduce one of my submissions. I accidentally changed somethin)59.5 413.15 ms
(g in)59.5 400.15 ms
(the code and I could not remember what exactly, so nowadays I'm )59.5 387.15 ms
(very)59.5 374.15 ms
(careful about my code and script. Another problem? Long executio)59.5 361.15 ms
(n history in notebooks leads)59.5 348.15 ms
(to lots of defined global variables. And global variables surely)59.5 335.15 ms
( lead to bugs. So remember to sometimes)59.5 322.15 ms
(restart your notebooks. It's okay to have ugly code, unless you)59.5 309.15 ms
(do not use this to produce a submission. It would be easier for )59.5 296.15 ms
(you to get into this code later if)59.5 283.15 ms
(it has a descriptive variable names. I always use git and)59.5 270.15 ms
(try to make the code for submissions as transparent as possible.)59.5 257.15 ms
( I usually create a separate notebook for every submission so I )59.5 244.15 ms
(can always run)59.5 231.15 ms
(the previous solution and compare. And I treat the submission)59.5 218.15 ms
(notebooks as script. I restart the kernel and)59.5 205.15 ms
(always run them from top to bottom. I found a convenient way to )59.5 192.15 ms
(validate)59.5 179.15 ms
(the models that allows to use validation code with minimal chang)59.5 166.15 ms
(es to retrain)59.5 153.15 ms
(a model on the whole dataset. In the competition, we are provide)59.5 140.15 ms
(d)59.5 127.15 ms
(with training and test CSV files. You see we load them in the fi)59.5 114.15 ms
(rst cell. In the second cell, we split)59.5 101.15 ms
(training set and actual training and validation sets, and)59.5 88.15 ms
(save those to disk as CSV files with the same structure as given)59.5 75.15 ms
( train CSV and)59.5 62.15 ms
(test CSV. Now, at the top of the notebook,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 16 16
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 16)59.5 790.15 ms
F0 sf
(with my model, I define variables. Path is to train and test set)59.5 764.15 ms
(s. I set them to create a training and validation sets when work)59.5 751.15 ms
(ing with)59.5 738.15 ms
(the model and validating it. And then it only takes me to switch)59.5 725.15 ms
(those paths to original train CSV and test CSV to produce a subm)59.5 712.15 ms
(ission. I also use macros. At one point I was really tired of)59.5 699.15 ms
(typing import numpy as np, every time. So I found that it's poss)59.5 686.15 ms
(ible to define)59.5 673.15 ms
(a macro which will load everything for me. In my case, it takes )59.5 660.15 ms
(only five symbols to type the macro name and this macro)59.5 647.15 ms
(immediately loads me everything. Very convenient. And finally, I)59.5 634.15 ms
( have developed my library)59.5 621.15 ms
(with frequently used functions, and training code for models. I )59.5 608.15 ms
(personally find it useful, as the code,)59.5 595.15 ms
(it now becomes much shorter, and I do not need to remember how)59.5 582.15 ms
(to import a particular model. In my case I just specify)59.5 569.15 ms
(a model with its name, and as an output I get all the informatio)59.5 556.15 ms
(n)59.5 543.15 ms
(about training that I would possibly need. [SOUND] [MUSIC]Hello )59.5 530.15 ms
(everyone. This is Marios. Today I would like to show you the Pip)59.5 517.15 ms
(eline or like the approach I have used to tackle more than 100 m)59.5 504.15 ms
(achine learning competitions in cargo and obviously has helped m)59.5 491.15 ms
(e to do quite well. Before I start, let me state that I'm not cl)59.5 478.15 ms
(aiming this is the best pipeline out there, is just the one I us)59.5 465.15 ms
(e. You might find some parts of it useful. So roughly, the Pipel)59.5 452.15 ms
(ine is, as you see it on the screen, here this is a summary and )59.5 439.15 ms
(we will go through it in more detail later on. But briefly, I sp)59.5 426.15 ms
(end the first day in order to understand the problem and make th)59.5 413.15 ms
(e necessary preparations in order to deal with this. Then, maybe)59.5 400.15 ms
( one, two days in order to understand a bit about the data, what)59.5 387.15 ms
( are my features, what I have available, trying to understand ot)59.5 374.15 ms
(her dynamics about the data, which will lead me to define a good)59.5 361.15 ms
( cross validation strategy and we will see later why this is imp)59.5 348.15 ms
(ortant. And then, once I have specified the cross validation str)59.5 335.15 ms
(ategy, I will spend all the days until 3-4 days before the end o)59.5 322.15 ms
(f the competition and I will keep iterating, doing different fea)59.5 309.15 ms
(ture engineering and applying different machine bearing models. )59.5 296.15 ms
(Now, something that I need to to highlight is that, when I start)59.5 283.15 ms
( this process I do it myself, shut from the outside world. So, I)59.5 270.15 ms
( close my ears, and I just focus on how I would tackle this prob)59.5 257.15 ms
(lem. That's because I don't want to get affected by what the oth)59.5 244.15 ms
(ers are doing. Because I might be able to find something that ot)59.5 231.15 ms
(hers will not. I mean, I might take a completely different appro)59.5 218.15 ms
(ach and this always leads me to gain, when I then combine with t)59.5 205.15 ms
(he rest of the people. For example, through merges or when I use)59.5 192.15 ms
( other people's kernels. So, I think this is important, because )59.5 179.15 ms
(it gives you the chance to create an intuitive approach about th)59.5 166.15 ms
(e data, and then also leverage the fact that other people have d)59.5 153.15 ms
(ifferent approaches and you will get more diverse results. And i)59.5 140.15 ms
(n the last 3 to 4 days, I would start exploring different ways t)59.5 127.15 ms
(o combine all the models of field, in order to get the best resu)59.5 114.15 ms
(lts. Now, if people have seen me in competitions, you should kno)59.5 101.15 ms
(w that you might have noticed that in the last 3-2 days I do a r)59.5 88.15 ms
(apid jump in the little box and that's exactly because I leave a)59.5 75.15 ms
(ssembling at the end. I normally don't do it. I have confidence )59.5 62.15 ms
(that it will work and I spend more time in feature engineering a)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 17 17
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 17)59.5 790.15 ms
F0 sf
(nd modeling, up until this point. So, let's take all these steps)59.5 764.15 ms
( one by one. Initially I try to understand the problem. First of)59.5 751.15 ms
( all, what type of problem it is. Is it image classification, so)59.5 738.15 ms
( try to find what object is presented on an image. This is sound)59.5 725.15 ms
( classification, like which type of bird appears in a sound file)59.5 712.15 ms
(. Is it text classification? Like who has written the specific t)59.5 699.15 ms
(ext, or what this text is about. Is it an optimization problem, )59.5 686.15 ms
(like giving some constraints how can I get from point A to point)59.5 673.15 ms
( B etc. Is it a tabular dataset, so that's like data which you c)59.5 660.15 ms
(an represent in Excel for example, with rows and columns, with v)59.5 647.15 ms
(arious types of features, categorical or numerical. Is it time s)59.5 634.15 ms
(eries problem? Is time important? All these questions are very v)59.5 621.15 ms
(ery important and that's why I look at the dataset and I try to )59.5 608.15 ms
(understand, because it defines in many ways what resources I wou)59.5 595.15 ms
(ld need, where do I need to look and what kind of hardware and s)59.5 582.15 ms
(oftware I will need. Also, I do this sort of preparation along w)59.5 569.15 ms
(ith controlling the volume of the data. How much is it. Because )59.5 556.15 ms
(again, this will define how I need to, what preparations I need )59.5 543.15 ms
(to do in order to solve this problem. Once I understand what typ)59.5 530.15 ms
(e of problem it is, then I need to reserve hardware to solve thi)59.5 517.15 ms
(s. So, in many cases I can escape without using GPUs, so just a )59.5 504.15 ms
(few CPUs would do the trick. But in problems like image classifi)59.5 491.15 ms
(cation of sound, then generally anywhere you would need to use d)59.5 478.15 ms
(eep learning. You definitely need to invest a lot in CPU, RAM an)59.5 465.15 ms
(d disk space. So, that's why this screening is important. It wil)59.5 452.15 ms
(l make me understand what type of machine I will need in order t)59.5 439.15 ms
(o solve this and whether I have this processing power at this po)59.5 426.15 ms
(int in order to solve this. Once this has been specified and I k)59.5 413.15 ms
(now how many CPUs, GPUs, RAM and disk space I'm going to need, t)59.5 400.15 ms
(hen I need to prepare the software. So, different software is su)59.5 387.15 ms
(ited for different types of problems. Keras and TensorFlow is ob)59.5 374.15 ms
(viously really good for when solving an image classification or )59.5 361.15 ms
(sound classification and text problems that you can pretty much )59.5 348.15 ms
(use it in any other problem as well. Then you most probably if y)59.5 335.15 ms
(ou use Python, you need scikit learn and XGBoost, Lighgbm. This )59.5 322.15 ms
(is the pinnacle of machine learning right now. And how do I set )59.5 309.15 ms
(this up? Normally I create either an anaconda environment or a v)59.5 296.15 ms
(irtual environment in general, and how a different one for its c)59.5 283.15 ms
(ompetition, because it's easy to set this up. So, you just set t)59.5 270.15 ms
(his up, you download the necessary packages you need, and then y)59.5 257.15 ms
(ou're good to go. This is a good way, a clean way to keep everyt)59.5 244.15 ms
(hing tidy and to really know what you used and what you find use)59.5 231.15 ms
(ful in the particular competitions. It's also a good validation )59.5 218.15 ms
(for later on, when we will have to do this again, to find an env)59.5 205.15 ms
(ironment that has worked well for this type of problem and possi)59.5 192.15 ms
(bly reuse it. Another question I ask at this point is what the m)59.5 179.15 ms
(etric I'm being tested on. Again, is it a regression program, is)59.5 166.15 ms
( it a classification program, it is root mean square error, it i)59.5 153.15 ms
(s mean absolute error. I ask these questions because I try to fi)59.5 140.15 ms
(nd if there's any similar competition with similar type of data )59.5 127.15 ms
(that I may have dealt with in the past, because this will make t)59.5 114.15 ms
(his preparation much much better, because I'll go backwards, fin)59.5 101.15 ms
(d what I had used in the past and capitalize on it. So, reuse it)59.5 88.15 ms
(, improve it, or even if I don't have something myself, I can ju)59.5 75.15 ms
(st find other similar competitions or explanations of these type)59.5 62.15 ms
( of problem from the web and try to see what people had used in )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 18 18
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 18)59.5 790.15 ms
F0 sf
(order to integrate it to my approaches. So, this is what it mean)59.5 764.15 ms
(s to understand the problem at this point. It's more about doing)59.5 751.15 ms
( the screen search, this screening in order to understand what t)59.5 738.15 ms
(ype of preparation I need to do, and actually do this preparatio)59.5 725.15 ms
(n, in order to be able to solve this problem competitively, in t)59.5 712.15 ms
(erms of hardware, software and other resources, past resources i)59.5 699.15 ms
(n dealing with these types of problems. Then I spent the next on)59.5 686.15 ms
(e or two days to do some exploratory data analysis. The first th)59.5 673.15 ms
(ing that I do is I see all my features, assuming a tabular data )59.5 660.15 ms
(set, in the training and the test data and to see how consistent)59.5 647.15 ms
( they are. I tend to plot distributions and to try to find if th)59.5 634.15 ms
(ere are any discrepancies. So is this variable in the training d)59.5 621.15 ms
(ata set very different than the same variable in the task set? B)59.5 608.15 ms
(ecause if there are discrepancies or differences, this is someth)59.5 595.15 ms
(ing I have to deal with. Maybe I need to remove these variables )59.5 582.15 ms
(or scale them in a specific way. In any case, big discrepancies )59.5 569.15 ms
(can cause problems to the model, so that's why I spend some time)59.5 556.15 ms
( here and do some plotting in order to detect these differences.)59.5 543.15 ms
( The other thing that I do is I tend to plot features versus the)59.5 530.15 ms
( target variable and possibly versus time, if time is available.)59.5 517.15 ms
( And again, this tells me to understand the effect of time, how )59.5 504.15 ms
(important is time or date in this data set. And at the same time)59.5 491.15 ms
( it helps me to understand which are like the most predictive in)59.5 478.15 ms
(puts, the most predictive variables. This is important because i)59.5 465.15 ms
(t generally gives me intuition about the problem. How exactly th)59.5 452.15 ms
(is helps me is not always clear. Sometimes it may help me define)59.5 439.15 ms
( a gross validation strategy or help me create some really good )59.5 426.15 ms
(features but in general, this kind of knowledge really helps to )59.5 413.15 ms
(understand the problem. I tend to create cross tabs for example )59.5 400.15 ms
(with the categorical variables and the target variable and also )59.5 387.15 ms
(creates unpredictability metrics like information value and you )59.5 374.15 ms
(see chi square for example, in order to see what's useful and wh)59.5 361.15 ms
(ether I can make hypothesis about the data, whether I understand)59.5 348.15 ms
( the data and how they relate with the target variable. The more)59.5 335.15 ms
( understanding I create at this point, most probably will lead t)59.5 322.15 ms
(o better features for better models applied on this data. Also w)59.5 309.15 ms
(hile I do this, I like to bin numerical features into bands in o)59.5 296.15 ms
(rder to understand if there nonlinear R.A.T's. When I say nonlin)59.5 283.15 ms
(ear R.A.T's, whether the value of a feature is low, target varia)59.5 270.15 ms
(ble is high, then as the value increases the target variable dec)59.5 257.15 ms
(reases as well. So whether there are strange relationships trend)59.5 244.15 ms
(s, patterns, or correlations between features and the target var)59.5 231.15 ms
(iable, in order to see how best to handle this later on and get )59.5 218.15 ms
(an intuition about which type of problems or which type of model)59.5 205.15 ms
(s would work better. Once I have understood the data, to some ex)59.5 192.15 ms
(tent, then it's time for me to define a cross validation strateg)59.5 179.15 ms
(y. I think this is a really important step and there have been c)59.5 166.15 ms
(ompetitions where people were able to win just because they were)59.5 153.15 ms
( able to find the best way to validate or to create a good cross)59.5 140.15 ms
( validation strategy. And by cross validation strategy, I mean t)59.5 127.15 ms
(o create a validation approach that best resembles what you're b)59.5 114.15 ms
(eing tested on. If you manage to create this internally then you)59.5 101.15 ms
( can create many different models and create many different feat)59.5 88.15 ms
(ures and anything you do, you can have the confidence that is wo)59.5 75.15 ms
(rking or it's not working, if you've managed to build the cross )59.5 62.15 ms
(validation strategy in a consistent way with what you're being t)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 19 19
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 19)59.5 790.15 ms
F0 sf
(ested on so consistency is the key word here. The first thing I )59.5 764.15 ms
(ask is, "Is time important in this data?" So do I have a feature)59.5 751.15 ms
( which is called date or time? If this is important then I need )59.5 738.15 ms
(to switch to a time-based validation. Always have past data pred)59.5 725.15 ms
(icting future data, and even the intervals, they need to be simi)59.5 712.15 ms
(lar with the test data. So if the test data is three months in t)59.5 699.15 ms
(he future, I need to build my training and validation to account)59.5 686.15 ms
( for this time interval. So my validation data always need to be)59.5 673.15 ms
( three months in the future and compared to the training data. Y)59.5 660.15 ms
(ou need to be consistent in order to have the most consistent re)59.5 647.15 ms
(sults with what you are been tested on. The other thing that I a)59.5 634.15 ms
(sk is, "Are there different entities between the train and the t)59.5 621.15 ms
(est data?" Imagine if you have different customers in the traini)59.5 608.15 ms
(ng data and different in the test data. Ideally, you need to for)59.5 595.15 ms
(mulate your cross validation strategy so that in the validation )59.5 582.15 ms
(data, you always have different customers running in training da)59.5 569.15 ms
(ta otherwise you are not really testing in a fair way. Your vali)59.5 556.15 ms
(dation method would not be consistent with the test data. Obviou)59.5 543.15 ms
(sly, if you know a customer and you try to predict it, him or he)59.5 530.15 ms
(r, why you have that customer in your training data, this is a b)59.5 517.15 ms
(iased prediction when compared to the test data, that you don't )59.5 504.15 ms
(have this information available. And this is the type of questio)59.5 491.15 ms
(ns you need to ask yourself when you are at this point, "Am I ma)59.5 478.15 ms
(king a validation which is really consistent with what am I bein)59.5 465.15 ms
(g tested on?" The other thing that is often the case is that the)59.5 452.15 ms
( training and the test data are completely random. I'm sorry, I )59.5 439.15 ms
(just shortened my data and I took a random part, put it on train)59.5 426.15 ms
(ing, the other for test so in that case, is any random type of c)59.5 413.15 ms
(ross validation could help for example, just do a random K-fold.)59.5 400.15 ms
( There are cases where you may have to use a combination of all )59.5 387.15 ms
(the above so you have strong temporal elements at the same time )59.5 374.15 ms
(you have different entities, so different customers to predict f)59.5 361.15 ms
(or past and future and at the same time, there is a random eleme)59.5 348.15 ms
(nt too. You might need to incorporate all of them do make a good)59.5 335.15 ms
( strategy. What I do is I often start with a random validation a)59.5 322.15 ms
(nd just see how it fares with the test leader board, and see how)59.5 309.15 ms
( consistent the result is with what they have internally, and se)59.5 296.15 ms
(e if improvements in my validation lead to improvements to the l)59.5 283.15 ms
(eader board. If that doesn't happen, I make a deeper investigati)59.5 270.15 ms
(on and try to understand why. It may be that the time element is)59.5 257.15 ms
( very strong and I need to take it into account or there are dif)59.5 244.15 ms
(ferent entities between the train and test data. These kinds of )59.5 231.15 ms
(questions in order to formulate a better validation strategy. On)59.5 218.15 ms
(ce the validation strategy has been defined, now I start creatin)59.5 205.15 ms
(g many different features. I'm sorry for bombarding you with loa)59.5 192.15 ms
(ds of information in one slide but I wanted this to be standalon)59.5 179.15 ms
(e. It says give you the different type of future engineering you)59.5 166.15 ms
( can use in different types of problems, and also suggestions fo)59.5 153.15 ms
(r the competition to look up which was quite representative of t)59.5 140.15 ms
(his time. But you can ignore these for now. Look at it later. Th)59.5 127.15 ms
(e main point is different problem requires different feature eng)59.5 114.15 ms
(ineering and I put everything when I say feature engineering. I )59.5 101.15 ms
(put the day data cleaning and preparation as well, how you handl)59.5 88.15 ms
(e missing values, and the features you generate out of this. The)59.5 75.15 ms
( thing is, every problem has its own corpus of different techniq)59.5 62.15 ms
(ues you use to derive or create new features. It's not easy to k)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 20 20
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 20)59.5 790.15 ms
F0 sf
(now everything because sometimes it's too much, I don't remember)59.5 764.15 ms
( it myself so what I tend to do is go back to similar competitio)59.5 751.15 ms
(ns and see what people are using or what people have used in the)59.5 738.15 ms
( past and I incorporate into my code. If I have dealt with this )59.5 725.15 ms
(or a similar problem in the past then I look at my code to see w)59.5 712.15 ms
(hat I had done in the past, but still looking for ways to improv)59.5 699.15 ms
(e this. I think that's the best way to be able to handle any pro)59.5 686.15 ms
(blem. The good thing is that a lot of the feature engineering ca)59.5 673.15 ms
(n be automated. You probably have already seen that but, as long)59.5 660.15 ms
( as your cross validation strategy is consistent with the test d)59.5 647.15 ms
(ata and reliable, then you can potentially try all sorts of tran)59.5 634.15 ms
(sformations and see how they work in your validation environment)59.5 621.15 ms
(. If they work well, you can be confident that this type of feat)59.5 608.15 ms
(ure engineering is useful and use it for further modeling. If no)59.5 595.15 ms
(t, you discard and try something else. Also the combinations of )59.5 582.15 ms
(what you can do in terms of feature engineering can be quite vas)59.5 569.15 ms
(t in different types of problems so obviously time is a factor h)59.5 556.15 ms
(ere, and scalability too. You need to be able to use your resour)59.5 543.15 ms
(ces well in order to be able to search as much as you can in ord)59.5 530.15 ms
(er to get the best outcome. This is what I do. Normally if I hav)59.5 517.15 ms
(e more time to do this feature engineering in a competition, I t)59.5 504.15 ms
(end to do better because I explore more things.And the modeling )59.5 491.15 ms
(is pretty much the same story. So, it's type problem has its own)59.5 478.15 ms
( type of model that works best. Now, I don't want to go through )59.5 465.15 ms
(that list again, I put it here so that you can use it for refere)59.5 452.15 ms
(nce. But, again, the way you work this out is you look for liter)59.5 439.15 ms
(ature, you sense other previous competitions that were similar a)59.5 426.15 ms
(nd you try to find which type of problem, which type of model or)59.5 413.15 ms
( best for its type of problem. And it's not surprise that for ty)59.5 400.15 ms
(pical dataset, when I say typical dataset I mean, tabular datase)59.5 387.15 ms
(t rather boosting machines in the form of [inaudible] turned to )59.5 374.15 ms
(rock fest for problems like aim as classification sound classifi)59.5 361.15 ms
(cation, deep learning in the form of convolutional neural networ)59.5 348.15 ms
(ks tend to work better. So, this is roughly what you need to kno)59.5 335.15 ms
(w. New techniques are being developed so, I think your best chan)59.5 322.15 ms
(ce here or what I have used in order to do well in the past was )59.5 309.15 ms
(knowing what's tends to work well with its problem, and going ba)59.5 296.15 ms
(ckwards and trying to find other code or other implementations a)59.5 283.15 ms
(nd similar problems in order to integrate it with mine and try t)59.5 270.15 ms
(o get a better result. I should mention that each of the previou)59.5 257.15 ms
(s models needs to be changed sometimes differently. So you need )59.5 244.15 ms
(to spend time within this cross-validation strategy in order to )59.5 231.15 ms
(find the best parameters, and then we move onto Ensembling. Ever)59.5 218.15 ms
(y time you apply your cross-validation procedure with a differen)59.5 205.15 ms
(t feature engineering and a different joint model, it's time, yo)59.5 192.15 ms
(u saved two types of predictions, you save predictions for the v)59.5 179.15 ms
(alidation data and you save predictions for the test data. So no)59.5 166.15 ms
(w that you have saved all these predictions and by the way this )59.5 153.15 ms
(is the point that if you collaborate with others that tend to se)59.5 140.15 ms
(nd you the predictions, and you'll be surprised that sometime th)59.5 127.15 ms
(at collaboration is just this. So people just sending these pred)59.5 114.15 ms
(iction files for the validation and the test data. So now you ca)59.5 101.15 ms
(n find the best way to combine these models in order to get the )59.5 88.15 ms
(best results. And since you already have predictions for the val)59.5 75.15 ms
(idation data, you know the target variable for the validation da)59.5 62.15 ms
(ta, so you can explore different ways to combine them. The metho)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 21 21
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 21)59.5 790.15 ms
F0 sf
(ds could be simple, could be an average, or already average, or )59.5 764.15 ms
(it can go up to a multilayer stacking in general. Generally, wha)59.5 751.15 ms
(t you need to know is that from my experience, smaller data requ)59.5 738.15 ms
(ires simple ensemble techniques like averaging. And also what te)59.5 725.15 ms
(nds to show is to look at correlation between predictions. So fi)59.5 712.15 ms
(nd it here that work well, but they tend to be quite diverse. So)59.5 699.15 ms
(, when you use fusion correlation, the correlation is not very h)59.5 686.15 ms
(igh. That means they are likely to bring new information, and so)59.5 673.15 ms
( when you combine you get the most out of it. But if you have bi)59.5 660.15 ms
(gger data there are, you got pretty must try all sorts of things)59.5 647.15 ms
(. What I like to think of is it is that, when you have really bi)59.5 634.15 ms
(g data, the stacking process that impedes the modeling process. )59.5 621.15 ms
(By that, I mean that you have a new set of features this time th)59.5 608.15 ms
(ey are predictions of models, but you can apply the same process)59.5 595.15 ms
( you have used before. So you can do feature engineering, you ca)59.5 582.15 ms
(n create new features or you can remove the features/ prediction)59.5 569.15 ms
( that you no longer need and you can use this in order to improv)59.5 556.15 ms
(e the results for your validation data. This process can be quit)59.5 543.15 ms
(e exhaustive, but well, again, it can be automated to some exten)59.5 530.15 ms
(t. So, the more time you have here, most probably the better you)59.5 517.15 ms
( will do. But from my experience, 2, 3 days is good in order to )59.5 504.15 ms
(get the best out of all the models you have built and depends ob)59.5 491.15 ms
(viously on the volume of data or volume of predictions you have )59.5 478.15 ms
(generated up until this point. At this point I would like to sha)59.5 465.15 ms
(re a few thoughts about collaboration. Many people have asked me)59.5 452.15 ms
( this and I think this is a good point to share. These ideas has)59.5 439.15 ms
( greatly helped me to do well in competitions. The first thing i)59.5 426.15 ms
(s that it makes things more fun. I mean you are not alone, you'r)59.5 413.15 ms
(e with other people and that's always more energizing, it's alwa)59.5 400.15 ms
(ys more interesting, it's more fun, you can communicate with the)59.5 387.15 ms
( others through times like Skype, and yeah I think it's more col)59.5 374.15 ms
(laborative as the world says, it is better. You learn more. I me)59.5 361.15 ms
(an you can be really good, but, you know, you always always lear)59.5 348.15 ms
(n from others. No way to know everything yourself. So it's reall)59.5 335.15 ms
(y good to be able to share points with other people, see what th)59.5 322.15 ms
(ey do learn from them and become better and grow as a data scien)59.5 309.15 ms
(tist, as a model. From my experience you score far better than t)59.5 296.15 ms
(rying to solve a problem alone, and I think these happens for ma)59.5 283.15 ms
(inly for two ways. There are more but these are main two. First )59.5 270.15 ms
(you can cover more ground because, you can say, you can focus on)59.5 257.15 ms
( ensembling, I will focus on feature engineering or you will foc)59.5 244.15 ms
(us on joining this type of model and I will focus on another typ)59.5 231.15 ms
(e of model. So, you can generally cover more ground. You can div)59.5 218.15 ms
(ide task and you can search, you can cover more ground in terms )59.5 205.15 ms
(of the possible things you can try in a competition. The second )59.5 192.15 ms
(thing is that every person sees the problem from different angle)59.5 179.15 ms
(s. So, that's very likely to generate more diverse predictions. )59.5 166.15 ms
(So something we do is although we kind of define together by the)59.5 153.15 ms
( different strategy when we form teams, then we would like to wo)59.5 140.15 ms
(rk for maybe one week separately without discussing with one ano)59.5 127.15 ms
(ther, because this helps to create diversity. Otherwise, if we o)59.5 114.15 ms
(ver discuss this, we might generate pretty much the same things.)59.5 101.15 ms
( So, in other words, our solutions might be too correlated to ad)59.5 88.15 ms
(d more value. So, this is a good way in order to leverage the di)59.5 75.15 ms
(fferent mindset each person has in solving these problems. So, f)59.5 62.15 ms
(or one week, each one works separately and then after some point)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 22 22
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 22)59.5 790.15 ms
F0 sf
(, we start combining or work more closely. I would advise people)59.5 764.15 ms
( to start collaborating after getting some experience, and I say)59.5 751.15 ms
( here two or three competitions just because Cargo has some rule)59.5 738.15 ms
(s. Sometimes, it is easy to make mistakes. I think it's better t)59.5 725.15 ms
(o understand the environment, the competition environment well b)59.5 712.15 ms
(efore exploring these options in order to make certain that, no )59.5 699.15 ms
(mistakes are done, no violation of the rules. Sometimes new peop)59.5 686.15 ms
(le tend to make these mistakes. So, it's good to have this exper)59.5 673.15 ms
(ience prior to trying to collaborating. I advise people to start)59.5 660.15 ms
( forming teams with people around their rank because sometimes i)59.5 647.15 ms
(t is frustrating when you join a high rank or a very experienced)59.5 634.15 ms
( team I would say. It's bad to say experience from rank, because)59.5 621.15 ms
( you don't know sometimes how to contribute, you still don't und)59.5 608.15 ms
(erstand all the competition dynamics and it might stall your pro)59.5 595.15 ms
(gress, if you join a team and you're not able to contribute. So,)59.5 582.15 ms
( I think it's better to, in most cases, to try and find people a)59.5 569.15 ms
(round your rank or around your experience and grow together. Thi)59.5 556.15 ms
(s way is the best form of collaboration I think. Another tip for)59.5 543.15 ms
( collaborating is to try to collaborate with people that are lik)59.5 530.15 ms
(ely to take diverse approaches or different approaches than your)59.5 517.15 ms
(self. You learn more this way and it is more likely that when yo)59.5 504.15 ms
(u combine, you will get a better score. So, such for people who )59.5 491.15 ms
(are sort of famous for doing well certain things and in order to)59.5 478.15 ms
( get the most out of it, to learn more from each other and get b)59.5 465.15 ms
(etter results in the leader board. About selecting submissions, )59.5 452.15 ms
(I have employed a strategy that many people have done. So normal)59.5 439.15 ms
(ly, I select the best submissions I see in my internal result an)59.5 426.15 ms
(d the one that work best on the leader board. At the same time, )59.5 413.15 ms
(I also look for correlations. So, if two submissions, they tend )59.5 400.15 ms
(to be the same pretty much. So, the one that was the best submis)59.5 387.15 ms
(sion locally, was also the best on leader boards, I try to find )59.5 374.15 ms
(other submissions that still work well but they are likely to be)59.5 361.15 ms
( quite diverse. So, they have low correlations with my best subm)59.5 348.15 ms
(ission because this way, I might capture, I might be lucky, it m)59.5 335.15 ms
(aybe be a special type of test data set and just by having a div)59.5 322.15 ms
(erse submission, I might be lucky to get a good score. So that's)59.5 309.15 ms
( the main idea about this. Some tips I would like to share now i)59.5 296.15 ms
(n general about competitive modeling, on land modeling and in Ca)59.5 283.15 ms
(rgo specifically. In these challenges, you never lose. [inaudibl)59.5 270.15 ms
(e] lose, yes you may not win prize money. Out of 5000 people, so)59.5 257.15 ms
(metimes it's difficult to be, almost to impossible to be in the )59.5 244.15 ms
(top three or four that gives prizes but you always gain in terms)59.5 231.15 ms
( of knowledge, in terms of experience. You get to collaborate wi)59.5 218.15 ms
(th other people which are talented in the field, you get to add )59.5 205.15 ms
(it to your CV that you try to solve this particular problem, and)59.5 192.15 ms
( I can tell you there has been some criticists here, people doub)59.5 179.15 ms
(t that doing these competitions stops your employ-ability but I )59.5 166.15 ms
(can tell you that i know many examples and not want us, they rea)59.5 153.15 ms
(lly thought the Ocean Cargo like Master and Grand-master that ju)59.5 140.15 ms
(st by having kind of experience, they have been able to find ver)59.5 127.15 ms
(y decent jobs and even if they had completely diverse background)59.5 114.15 ms
(s to the science. So, I can tell you it matters. So, any time yo)59.5 101.15 ms
(u spend here, it's definitely a win for you. I don't see how you)59.5 88.15 ms
( can lose by competing in these challenges. You mean if this is )59.5 75.15 ms
(something you like right. The whole predictive modeling that the)59.5 62.15 ms
( science think. Coffee tempts to shop, because you tend to spend)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 23 23
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 23)59.5 790.15 ms
F0 sf
( longer hours. I tend to do this especially late at night. So it)59.5 764.15 ms
( definitely tells me something to consider or to be honest any o)59.5 751.15 ms
(ther beverage will do: depends what you like. I see it a bit lik)59.5 738.15 ms
(e a game and I advise you to do the same because if you see it l)59.5 725.15 ms
(ike a game, you never need to work for it. If you know what I me)59.5 712.15 ms
(an. So it looks a bit like NRPT. In some way, you have some tool)59.5 699.15 ms
(s or weapons. These are all the algorithms and feature engineeri)59.5 686.15 ms
(ng techniques you can use. And then you have this core leader bo)59.5 673.15 ms
(ard and you try to beat all the bad guys and to beat the score a)59.5 660.15 ms
(nd rise above them. So in a way does look like a game. You know )59.5 647.15 ms
(you try to use all the tools, all the skills that you have to tr)59.5 634.15 ms
(y to beat the score. So, I think if you see it like a game it re)59.5 621.15 ms
(ally helps you. You don't get tired and you enjoy the process mo)59.5 608.15 ms
(re. I do advise you to take a break though, from my experience y)59.5 595.15 ms
(ou may spend long hours hitting on it and that's not good for yo)59.5 582.15 ms
(ur body. You definitely need to take some breaks and do some phy)59.5 569.15 ms
(sical exercise. Go out for a walk. I think it can help most of t)59.5 556.15 ms
(he times by resting your mind this way can actually help to do b)59.5 543.15 ms
(etter. You have more rested heart, more clear thinking. So, I de)59.5 530.15 ms
(finitely advise you to do this, generally don't overdo it. I hav)59.5 517.15 ms
(e overnighted in the past but i advise you not to do the same. A)59.5 504.15 ms
(nd now there is a thing that I would like to highlight is that t)59.5 491.15 ms
(he Cargo community is great. Is one of the most open and helpful)59.5 478.15 ms
( helpful communities have experience in any social context, mayb)59.5 465.15 ms
(e apart from Charities but if you have a question and you posted)59.5 452.15 ms
( on the forums or other associated channels like in Slug and peo)59.5 439.15 ms
(ple are always willing to help you.That's great, because there a)59.5 426.15 ms
(re so many people out there and most probably they know the answ)59.5 413.15 ms
(er or they can help you for a particular problem. And this is in)59.5 400.15 ms
(valuable. So many times i have really made use of this, of this )59.5 387.15 ms
(option and it really helps. You know this kind of mentality was )59.5 374.15 ms
(there even before the serine was gamified. When I say gamified, )59.5 361.15 ms
(now you get points by sharping in a way by sharing code or parti)59.5 348.15 ms
(cipating in discussions. But in the past, people were doing with)59.5 335.15 ms
(out really getting something out of it. It maybe the open source)59.5 322.15 ms
( mentality of data science that the fact that many people partic)59.5 309.15 ms
(ipating are researchers. I don't know but it really is a field t)59.5 296.15 ms
(hat sharing seems to be really important in helping others. So, )59.5 283.15 ms
(I do advise you to consider this and don't be afraid to ask in t)59.5 270.15 ms
(hese forums. Another thing that I do at shops, is that after the)59.5 257.15 ms
( competition has ended irrespective of how well or not you've do)59.5 244.15 ms
(ne, is go and look for other people and what they have done. Nor)59.5 231.15 ms
(mally, there are threads where people share their approaches, so)59.5 218.15 ms
(metimes they share the whole approach would go to sometimes it j)59.5 205.15 ms
(ust give tips and you know this is where you can upgrade your to)59.5 192.15 ms
(ols and you can see what other people have done and make improve)59.5 179.15 ms
(ments. And in tandem with this, you should have a notebook of us)59.5 166.15 ms
(eful methods that you keep updating it at the end of every compe)59.5 153.15 ms
(tition. So, you found an approach that was good, you just add it)59.5 140.15 ms
( to that notebook and next time you encounter the same or simila)59.5 127.15 ms
(r competition you get that notebook out and you apply the same t)59.5 114.15 ms
(echniques at work in the past and this is how you get better. Ac)59.5 101.15 ms
(tually, if i now start a competition without that notebook, i th)59.5 88.15 ms
(ink it will take me three or four times more in order to get to )59.5 75.15 ms
(the same score because a lot of the things that I do now depend )59.5 62.15 ms
(on stuff that i have done in the past. So, it's definitely helpf)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 24 24
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 24)59.5 790.15 ms
F0 sf
(ul, consider creating this notebook or library of all the approa)59.5 764.15 ms
(ches or approaches that have worked in the past in order to have)59.5 751.15 ms
( an easier time going on. And that was what I wanted to share wi)59.5 738.15 ms
(th you and thank you very much for bearing with me and to see yo)59.5 725.15 ms
(u next time, right.Hi everyone. This video is dedicated to the f)59.5 712.15 ms
(ollowing advanced feature engineering techniques. Calculating va)59.5 699.15 ms
(rious statistics of one feature grouped by another and features )59.5 686.15 ms
(derived from neighborhood analysis of a given point. To make it )59.5 673.15 ms
(a little bit clearer, let's consider a simple example. Here we h)59.5 660.15 ms
(ave a chunk of data for some CTR task. Let's forget about target)59.5 647.15 ms
( variable and focus on human features. Namely, User_ID, unique i)59.5 634.15 ms
(dentifier of a user, Page_ID, an identifier of a page user visit)59.5 621.15 ms
(ed, Ad_price, item prices in the ad, and Ad_position, relative p)59.5 608.15 ms
(osition of an ad on the web page. The most straightforward way t)59.5 595.15 ms
(o solve this problem is to label and call the Ad_position and fe)59.5 582.15 ms
(ed some classifier. It would be a very good classifier that coul)59.5 569.15 ms
(d take into account all the hidden relations between variables. )59.5 556.15 ms
(But no matter how good it is, it still treats all the data point)59.5 543.15 ms
(s independently. And this is where we can apply feature engineer)59.5 530.15 ms
(ing. We can imply that an ad with the lowest price on the page w)59.5 517.15 ms
(ill catch most of the attention. The rest of the ads on the page)59.5 504.15 ms
( won't be very attractive. It's pretty easy to calculate the fea)59.5 491.15 ms
(tures relevant to such an implication. We can add lowest and hig)59.5 478.15 ms
(hest prices for every user and page per ad. Position of an ad wi)59.5 465.15 ms
(th the lowest price could also be of use in such case. Here's on)59.5 452.15 ms
(e of the ways to implement statistical features with paid ads. I)59.5 439.15 ms
(f our data is stored in the data frame df, we call groupby metho)59.5 426.15 ms
(d like this to get maximum and minimum price values. Then store )59.5 413.15 ms
(this object in gb variable, and then join it back to the data fr)59.5 400.15 ms
(ame df. This is it. I want to emphasize that you should not stop)59.5 387.15 ms
( at this point. It's possible to add other useful features not n)59.5 374.15 ms
(ecessarily calculated within user and page per. It could be how )59.5 361.15 ms
(many pages user has visited, how many pages user has visited dur)59.5 348.15 ms
(ing the given session, and ID of the most visited page, how many)59.5 335.15 ms
( users have visited that page, and many, many more features. The)59.5 322.15 ms
( main idea is to introduce new information. By that means, we ca)59.5 309.15 ms
(n drastically increase the quality of the models. But what if th)59.5 296.15 ms
(ere is no features to use groupby on? Well, in such case, we can)59.5 283.15 ms
( replace grouping operations with finding the nearest neighbors.)59.5 270.15 ms
( On the one hand, it's much harder to implement and collect usef)59.5 257.15 ms
(ul information. On the other hand, the method is more flexible. )59.5 244.15 ms
(We can fine tune things like the size of relevant neighborhood o)59.5 231.15 ms
(r metric. The most common and natural example of neighborhood an)59.5 218.15 ms
(alysis arises from purposive pricing. Imagine that you need to p)59.5 205.15 ms
(redict rental prices. You would probably have some characteristi)59.5 192.15 ms
(cs like floor space, number of rooms, presence of a bus stop. Bu)59.5 179.15 ms
(t you need something more than that to create a really good mode)59.5 166.15 ms
(l. It could be the number of other houses in different neighborh)59.5 153.15 ms
(oods like in 500 meters, 1,000 meters, or 1,500 meters, or avera)59.5 140.15 ms
(ge price per square meter in such neighborhoods, or the number o)59.5 127.15 ms
(f schools, supermarkets, and parking lots in such neighborhoods.)59.5 114.15 ms
( The distances to the closest objects of interest like subway st)59.5 101.15 ms
(ations or gyms could also be of use. I think you've got the idea)59.5 88.15 ms
(. In the example, we've used a very simple case, where neighborh)59.5 75.15 ms
(oods were calculated in geographical space. But don't be afraid )59.5 62.15 ms
(to apply this method to some abstract or even anonymized feature)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 25 25
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 25)59.5 790.15 ms
F0 sf
( space. It still could be very useful. My team and I used this m)59.5 764.15 ms
(ethod in Spring Leaf competition. Furthermore, we did it in supe)59.5 751.15 ms
(rvised fashion. Here is how we have done it. First of all, we ap)59.5 738.15 ms
(plied mean encoding to all variables. By doing so, we created ho)59.5 725.15 ms
(mogeneous feature space so we did not worry about scaling and im)59.5 712.15 ms
(portance of each particular feature. After that, we calculated 2)59.5 699.15 ms
(,000 nearest neighbors with Bray-Curtis metric. Then we evaluate)59.5 686.15 ms
(d various features from those neighbors like mean target of near)59.5 673.15 ms
(est 5, 10, 15, 500, 2,000 neighbors, mean distance to 10 closest)59.5 660.15 ms
( neighbors, mean distance to 10 closest neighbors with target 1,)59.5 647.15 ms
( and mean distance to 10 closest neighbors with target 0, and, i)59.5 634.15 ms
(t worked great. In conclusion, I hope you embrace the main ideas)59.5 621.15 ms
( of both groupby and nearest neighbor methods and you would be a)59.5 608.15 ms
(ble to apply them in practice. Thank you for your attention.[MUS)59.5 595.15 ms
(IC] Hi everyone, in this video I will)59.5 582.15 ms
(talk about the application of matrix factorization technique)59.5 569.15 ms
(in feature extraction. You will see a few application)59.5 556.15 ms
(of the approach for feature extraction and)59.5 543.15 ms
(we will be able to apply it. I will show you several examples)59.5 530.15 ms
(along with practical details. Here's a classic example)59.5 517.15 ms
(of recommendations. Suppose we have some information about)59.5 504.15 ms
(user, like age, region, interest and items like gender, year len)59.5 491.15 ms
(gth. Also we know ratings that)59.5 478.15 ms
(users gave to some items. These ratings can be organized in a us)59.5 465.15 ms
(er)59.5 452.15 ms
(item matrix with row corresponding to users, and columns corresp)59.5 439.15 ms
(onding to items,)59.5 426.15 ms
(as shown in the picture. In a cell with coordinates i, j,)59.5 413.15 ms
(the user or agent can be chooser i, give the item j. Assume that)59.5 400.15 ms
( our user)59.5 387.15 ms
(have some features Ui. And jth item have is)59.5 374.15 ms
(corresponding feature Mj. And scalar product of these)59.5 361.15 ms
(features produce a rating Rij. Now we can apply matrix factoriza)59.5 348.15 ms
(tion)59.5 335.15 ms
(to learning those features for item and users. Sometimes these f)59.5 322.15 ms
(eatures)59.5 309.15 ms
(can have an interpretation. Like the first feature in item can)59.5 296.15 ms
(be measured of or something similar. But generally you should co)59.5 283.15 ms
(nsider them)59.5 270.15 ms
(as some extra features, which we can use to encode user in the s)59.5 257.15 ms
(ame way as we did)59.5 244.15 ms
(before with labeling coder or coder. Specifically our assumption)59.5 231.15 ms
( about)59.5 218.15 ms
(scale of product is the following. If we present all attributes )59.5 205.15 ms
(of user and items as matrixes, the matrix product will)59.5 192.15 ms
(be very close to the matrix's ratings. In other words,)59.5 179.15 ms
(which way to find matrix's U and M, such as their product)59.5 166.15 ms
(gives the matrix R. This way, this approach is called matrix)59.5 153.15 ms
(factorization or matrix composition. In previous examples, we us)59.5 140.15 ms
(ed both row and)59.5 127.15 ms
(column related features. But sometimes we don't let)59.5 114.15 ms
(the features correspond to rows. Let's consider another example.)59.5 101.15 ms
( Suppose that we are texts, do you)59.5 88.15 ms
(remember how we usually classify text? We extract features and e)59.5 75.15 ms
(ach document)59.5 62.15 ms
(was described by a large sparse reactor. If we do matrix factori)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 26 26
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 26)59.5 790.15 ms
F0 sf
(zation over)59.5 764.15 ms
(these parse features, we will get the representation for index d)59.5 751.15 ms
(isplayed)59.5 738.15 ms
(in yellow, and terms displayed in green. Although we can somehow)59.5 725.15 ms
(use representation for jumps, we are interested only)59.5 712.15 ms
(in representations for dogs. Now every document is described)59.5 699.15 ms
(by a small, dense reactor. These are our features, and we can us)59.5 686.15 ms
(e)59.5 673.15 ms
(them in a way similar to previous example. This case is often ca)59.5 660.15 ms
(lled)59.5 647.15 ms
(dimension energy reduction. It's quite an efficient way to reduc)59.5 634.15 ms
(e)59.5 621.15 ms
(the size of feature metric, and extract real valued features)59.5 608.15 ms
(from categorical ones. In competitions we often have)59.5 595.15 ms
(different options for purchasing. For example, using text data,)59.5 582.15 ms
(you can run back of big rams and so on. Using matrix optimizatio)59.5 569.15 ms
(n technique, you are able to extract features)59.5 556.15 ms
(from all of these matrices. Since the resulting matrices will be)59.5 543.15 ms
(small, we can easily join them and use togetherness of the featu)59.5 530.15 ms
(res)59.5 517.15 ms
(in tree-based models. Now I want to make a few comments)59.5 504.15 ms
(about matrix factorization. Not just that we are not)59.5 491.15 ms
(constrained to reduce whole matrix, you can apply factorization )59.5 478.15 ms
(to a subset)59.5 465.15 ms
(of a column and leave the other as is. Besides reduction you can)59.5 452.15 ms
(use pressure boards for getting another presentation)59.5 439.15 ms
(of the same data. This is especially useful for example since it)59.5 426.15 ms
( provides velocity)59.5 413.15 ms
(of its models and leads to a better. Of course matrix factorizat)59.5 400.15 ms
(ion)59.5 387.15 ms
(is a loss of transformation, in other words we will lose some)59.5 374.15 ms
(information after the search reduction. Efficiency of this appro)59.5 361.15 ms
(ach heavily)59.5 348.15 ms
(depends on a particular task and choose a number of latent facto)59.5 335.15 ms
(rs. The number should be considered as)59.5 322.15 ms
(a hyper parameter and needs to be tuned. It's a good practice to)59.5 309.15 ms
( choose)59.5 296.15 ms
(a number of factors between 5 and 100. Now, let's switch from ge)59.5 283.15 ms
(neral idea)59.5 270.15 ms
(to particular implementations. Several matrix factorization)59.5 257.15 ms
(methods are implemented in circuit as the most famous SVD and PC)59.5 244.15 ms
(A. In addition,)59.5 231.15 ms
(their use included TruncatedSVD, which can work with sparse matr)59.5 218.15 ms
(ices. It's very convenient for example,)59.5 205.15 ms
(in case of text datasets. Also there exists a so called non-nega)59.5 192.15 ms
(tive)59.5 179.15 ms
(matrix factorization, or NMF. It impose an additional restrictio)59.5 166.15 ms
(ns that)59.5 153.15 ms
(all hidden factors are non-negative, that is either zero or a po)59.5 140.15 ms
(sitive number. It can be applied only to)59.5 127.15 ms
(non-negative matrixes. For example matrix where all represented)59.5 114.15 ms
(occurence of each word in the document. NMF has an interesting p)59.5 101.15 ms
(roperty, it transforms data in a way that makes)59.5 88.15 ms
(data more suitable for decision trees. Take a look at the pictur)59.5 75.15 ms
(e from)59.5 62.15 ms
(Microsoft Mobile Classification Challenge. It can be seen that N)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 27 27
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 27)59.5 790.15 ms
F0 sf
(MF transform data)59.5 764.15 ms
(forms lines parallel to the axis. A few more notes on matrix fac)59.5 751.15 ms
(torizations. Essentially they are very)59.5 738.15 ms
(similar to linear models, so we can use the same transformation)59.5 725.15 ms
(tricks as we use for linear models. So in addition to standard N)59.5 712.15 ms
(MF, I advise you to apply)59.5 699.15 ms
(the factorization to transform data. Here's another plot from th)59.5 686.15 ms
(e competition. It's clear that these two transformations)59.5 673.15 ms
(produce different features, and we don't have to choose the best)59.5 660.15 ms
( one. Instead, it's beneficial)59.5 647.15 ms
(to use both of them. I want to note that matrix factorization)59.5 634.15 ms
(is a trainable transformation, and has its own parameters. So we)59.5 621.15 ms
( should be careful, and)59.5 608.15 ms
(use the same transformation for all parts of your data set. Read)59.5 595.15 ms
(ing and transforming each)59.5 582.15 ms
(part individually is wrong, because in that case you will get)59.5 569.15 ms
(two different transformations. This can lead to an error)59.5 556.15 ms
(which will be hard to find. The correct method is shown below,)59.5 543.15 ms
(first we need to the data information on all data and)59.5 530.15 ms
(only then apply to each individual piece. To sum up, matrix comp)59.5 517.15 ms
(osition is a very)59.5 504.15 ms
(general approach to dimensional reduction and feature extraction)59.5 491.15 ms
(. It can be used to transform)59.5 478.15 ms
(categorical feature into real ones. And tricks for linear models)59.5 465.15 ms
( are also)59.5 452.15 ms
(suitable for matrix factorizations. Thank you for your attention)59.5 439.15 ms
(. [MUSIC] [SOUND]Hi, everyone. The main topic of this video is F)59.5 426.15 ms
(eature Interactions. You will learn how to construct them and us)59.5 413.15 ms
(e in problem solving. Additionally, we will discuss them for fea)59.5 400.15 ms
(ture extraction from decision trees. Let's start with an example)59.5 387.15 ms
(. Suppose that we are building a model to predict the best adver)59.5 374.15 ms
(tisement banner to display on a website. Among available feature)59.5 361.15 ms
(s, there are two categorical ones that we will concentrate on. T)59.5 348.15 ms
(he category of the advertising banner itself and the category of)59.5 335.15 ms
( the site the banner will be showing on. Certainly, we can use t)59.5 322.15 ms
(he features as two independent ones, but a really important feat)59.5 309.15 ms
(ure is indeed the combination of them. We can explicitly constru)59.5 296.15 ms
(ct the combination in order to incorporate our knowledge into a )59.5 283.15 ms
(model. Let's construct new feature named ad_site that represents)59.5 270.15 ms
( the combination. It will be categorical as the old ones, but se)59.5 257.15 ms
(t of its values will be all possible combinations of two origina)59.5 244.15 ms
(l values. From a technical point of view, there are two ways to )59.5 231.15 ms
(construct such interaction. Let's look at a simple example. Cons)59.5 218.15 ms
(ider our first feature, f1, has values A or B. Another feature, )59.5 205.15 ms
(f2, has values X or Y or Z, and our data set consist of four dat)59.5 192.15 ms
(a points. The first approach is to concatenate the text values o)59.5 179.15 ms
(f f1 and f2, and use the result as a new categorical feature f_j)59.5 166.15 ms
(oin. We can then apply the OneHot according to it. The second ap)59.5 153.15 ms
(proach consist of two steps. Firstly, apply OneHot and connect t)59.5 140.15 ms
(o features f1 and f2. Secondly, construct new metrics by multipl)59.5 127.15 ms
(ying each column from f1 encoded metrics to each column from f2 )59.5 114.15 ms
(encoded metrics. It was nothing that both methods results in pra)59.5 101.15 ms
(ctically the same new feature representations. In the above exam)59.5 88.15 ms
(ple, we can consider as interactions between categorical feature)59.5 75.15 ms
(s, but similar ideas can be applied to real valued features. For)59.5 62.15 ms
( example, having two real valued features f1 and f2, interaction)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 28 28
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 28)59.5 790.15 ms
F0 sf
(s between them can be obtained by multiplications of f1 and f2. )59.5 764.15 ms
(In fact, we are not limited to use only multiply operation. Any )59.5 751.15 ms
(function taking two arguments like sum, difference, or division )59.5 738.15 ms
(is okay. The following transformations significantly enlarge fea)59.5 725.15 ms
(ture space and makes learning easier, but keep in mind that it m)59.5 712.15 ms
(akes or frequent easier too. It should be emphasized that for th)59.5 699.15 ms
(ree ways algorithms such as the random forest or gradient boost )59.5 686.15 ms
(decision trees it's difficult to extract such kind of dependenci)59.5 673.15 ms
(es. That's why they're buffer transformation are very efficient )59.5 660.15 ms
(for three based methods. Let's discuss practical details now. Wh)59.5 647.15 ms
(ere wise future generation approaches greatly increase the numbe)59.5 634.15 ms
(r of the features. If there were any original features, there wi)59.5 621.15 ms
(ll be n square. And will be even more features if several types )59.5 608.15 ms
(of interaction are used. There are two ways to moderate this, ei)59.5 595.15 ms
(ther do feature selection or dimensionality reduction. I prefer )59.5 582.15 ms
(doing the selection since not all but only a few interactions of)59.5 569.15 ms
(ten achieve the same quality as all combinations of features. Fo)59.5 556.15 ms
(r each type of interaction, I construct all piecewise feature in)59.5 543.15 ms
(teractions. Feature random forests over them and select several )59.5 530.15 ms
(most important features. Because number of resulting features fo)59.5 517.15 ms
(r each type is relatively small. It's possible to join them toge)59.5 504.15 ms
(ther along with original features and use as input for any machi)59.5 491.15 ms
(ne learning algorithm usually to be by use method. During the vi)59.5 478.15 ms
(deo, we have examined the method to construct second order inter)59.5 465.15 ms
(actions. But you can similarly produce throned order or higher. )59.5 452.15 ms
(Due to the fact that number of features grow rapidly with order,)59.5 439.15 ms
( it has become difficult to work with them. Therefore high order)59.5 426.15 ms
( directions are often constructed semi-manually. And this is an )59.5 413.15 ms
(art in some ways. Additionally, I would like to talk about metho)59.5 400.15 ms
(ds to construct categorical features from decision trees. Take a)59.5 387.15 ms
( look at the decision tree. Let's map each leaf into a binary fe)59.5 374.15 ms
(ature. The index of the object's leaf can be used as a value for)59.5 361.15 ms
( a new categorical feature. If we use not a single tree but an e)59.5 348.15 ms
(nsemble of them. For example, a random forest, then such operati)59.5 335.15 ms
(on can be applied to each of entries. This is a powerful way to )59.5 322.15 ms
(extract high order interactions. This technique is quite simple )59.5 309.15 ms
(to implement. Tree-based poodles from sklearn library have an ap)59.5 296.15 ms
(ply method which takes as input feature metrics and rituals corr)59.5 283.15 ms
(esponding indices of leaves. In xgboost, also support to why a p)59.5 270.15 ms
(arameter breed leaf in predict method. I suggest we need to coll)59.5 257.15 ms
(aborate documentations in order to get more information about th)59.5 244.15 ms
(ese methods and IPIs. In the end of this video, I will tackle th)59.5 231.15 ms
(e main points. We examined method to construct an interactions o)59.5 218.15 ms
(f categorical features. Also, we extend the approach to real-val)59.5 205.15 ms
(ued features. And we have learned how to use trees to extract hi)59.5 192.15 ms
(gh order interactions. Thank you for your attention.Hi, everyone)59.5 179.15 ms
(. Today, we will discuss this new method for visualizing data in)59.5 166.15 ms
(tegrating features. At the end of this video, you will be able t)59.5 153.15 ms
(o use tSNE in your products. In the previous video, we learned a)59.5 140.15 ms
(bout metaphysician technique that is predatory very close to lin)59.5 127.15 ms
(ear models. In this video, we will touch the subject of non-line)59.5 114.15 ms
(ar methods of dimensionality reduction. That says in general are)59.5 101.15 ms
( called manifold learning. For example, look at the data in form)59.5 88.15 ms
( of letter S on the left side. On the right, we can see results )59.5 75.15 ms
(of running different manifold learning algorithm on the data. Th)59.5 62.15 ms
(is new result is placed at the right bottom corner on the slide.)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 29 29
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 29)59.5 790.15 ms
F0 sf
( This new algorithm is the main topic of the lecture, as it tell)59.5 764.15 ms
(s of how this really works won't be explained here. But you will)59.5 751.15 ms
( come to look at additional materials for the details. Let's jus)59.5 738.15 ms
(t say that this is a method that tries to project points from hi)59.5 725.15 ms
(gh dimensional space into small dimensional space so that the di)59.5 712.15 ms
(stances between points are approximately preserved. Let's look a)59.5 699.15 ms
(t the example of the tSNE on the MNIST dataset. Here are points )59.5 686.15 ms
(from 700 dimensional space that are projected into two dimension)59.5 673.15 ms
(al space. You can see that such projection forms explicit cluste)59.5 660.15 ms
(rs. Coolest shows that these clusters are meaningful and corresp)59.5 647.15 ms
(onds to the target numbers well. Moreover, neighbor clusters cor)59.5 634.15 ms
(responds to a visually similar numbers. For example, cluster of )59.5 621.15 ms
(three is located next to the cluster of five which in chance is )59.5 608.15 ms
(adjustment to the cluster of six and eight. If data has explicit)59.5 595.15 ms
( structure as in case of MNIST dataset, it's likely to be reflec)59.5 582.15 ms
(ted on tSNE plot. For the reason tSNE is widely used in explorat)59.5 569.15 ms
(ory data analysis. However, do not assume that tSNE is a magic w)59.5 556.15 ms
(ant that always helps. For example, a misfortune choice of hyper)59.5 543.15 ms
(parameters may lead to poor results. Consider an example, in the)59.5 530.15 ms
( center is the least presented a tSNE projection of exactly the )59.5 517.15 ms
(same MNIST data as in previous example, only perplexity paramete)59.5 504.15 ms
(r has been changed. On the left, for comparison, we have plots f)59.5 491.15 ms
(rom previous right. On the right, so it present a tSNE projectio)59.5 478.15 ms
(n of random data. We can see as a choice of hybrid parameters ch)59.5 465.15 ms
(ange projection of MNIST data significantly so that we cannot se)59.5 452.15 ms
(e clusters. Moreover, new projection become more similar to rand)59.5 439.15 ms
(om data rather than to the original. Let's find out what depends)59.5 426.15 ms
( on the perplexity hyperparameter value. On the left, we have pe)59.5 413.15 ms
(rplexity=3, in the center=10, and on the right= 150. I want to e)59.5 400.15 ms
(mphasize that these projections are all made for the same data. )59.5 387.15 ms
(The illustration shows that these new results strongly depends o)59.5 374.15 ms
(n its parameters, and the interpretation of the results is not a)59.5 361.15 ms
( simple task. In particular, one cannot infer the size of origin)59.5 348.15 ms
(al clusters using the size of projected clusters. Similar propos)59.5 335.15 ms
(ition is valid for a distance between clusters. Blog distill.pub)59.5 322.15 ms
( contain a post about how to understand and interpret the result)59.5 309.15 ms
(s of tSNE. Also, it contains a great interactive demo that will )59.5 296.15 ms
(help you to get into issues of how tSNE works. I strongly advise)59.5 283.15 ms
( you to take a look at it. In addition to exploratory data analy)59.5 270.15 ms
(sis, tSNE can be considered as a method to obtain new features f)59.5 257.15 ms
(rom data. You should just concatenate the transformers coordinat)59.5 244.15 ms
(es to the original feature matrix. Now if you've heard this abou)59.5 231.15 ms
(t practical details, as it has been shown earlier, the results o)59.5 218.15 ms
(f tSNE algorithm, it strongly depends on hyperparameters. It is )59.5 205.15 ms
(good practice to use several projections with different perplexi)59.5 192.15 ms
(ties. In addition, because of stochastic of this methods results)59.5 179.15 ms
( in different projections even with the same data and hyperparam)59.5 166.15 ms
(eters. This means the train and test sets should be projected to)59.5 153.15 ms
(gether rather than separately. Also, tSNE will run for a long ti)59.5 140.15 ms
(me if you have a lot of features. If the number of features is g)59.5 127.15 ms
(reater than 500, you should use one of dimensionality reduction )59.5 114.15 ms
(approach and reduce number of features, for example, to 100. Imp)59.5 101.15 ms
(lementation of tSNE can be found in the sklearn library. But per)59.5 88.15 ms
(sonally, I prefer to use another implementation from a separate )59.5 75.15 ms
(Python package called tSNE, since it provide a way more efficien)59.5 62.15 ms
(t implementation. In conclusion, I want to remind you the basic )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 30 30
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 30)59.5 790.15 ms
F0 sf
(points of the lecture. TSNE is an excellent tool for visualizing)59.5 764.15 ms
( data. If data has an explicit structure, then it likely be [ina)59.5 751.15 ms
(udible] on tSNE projection. However, it requires to be cautious )59.5 738.15 ms
(with interpretation of tSNE results. Sometimes you can see struc)59.5 725.15 ms
(ture where it does not exist or vice versa, see none where struc)59.5 712.15 ms
(ture is actually present. It's a good practice to do several tSN)59.5 699.15 ms
(E projections with different perplexities. And in addition to EJ)59.5 686.15 ms
(, tSNE is working very well as a feature for feeding models. Tha)59.5 673.15 ms
(nk you for your attention.Hello everyone, this is Marios)59.5 660.15 ms
(Michailidis, and this will be the first video in a series that w)59.5 647.15 ms
(e will be)59.5 634.15 ms
(discussing on ensemble methods for machine learning. To tell you)59.5 621.15 ms
( a bit about me, I work as)59.5 608.15 ms
(Research Data Scientist for H2Oai. In fact,)59.5 595.15 ms
(my PhD is about assemble methods, and they used to be ranked)59.5 582.15 ms
(number one in cargo and ensemble methods have greatly)59.5 569.15 ms
(helped me to achieve this spot. So you might find the course int)59.5 556.15 ms
(eresting. So what is ensemble modelling? I think with this term,)59.5 543.15 ms
( we refer to)59.5 530.15 ms
(combining many different machine learning models in order to get)59.5 517.15 ms
(a more powerful prediction. And later on we will see)59.5 504.15 ms
(examples that this happens, that we combine different models and)59.5 491.15 ms
(we do get better predictions. There are various ensemble methods)59.5 478.15 ms
(. Here we'll discuss a few, those that)59.5 465.15 ms
(we encounter quite often, in predictive modelling competitions, )59.5 452.15 ms
(and they tend)59.5 439.15 ms
(to be, in general, quite competitive. We will start with simple )59.5 426.15 ms
(averaging)59.5 413.15 ms
(methods, then we'll go to weighted averaging methods, and we wil)59.5 400.15 ms
(l also)59.5 387.15 ms
(examine conditional averaging. And then we will move to some mor)59.5 374.15 ms
(e)59.5 361.15 ms
(typical ones like bagging, or the very, very popular, boosting,)59.5 348.15 ms
(then stacking and StackNet, which is the result of my research. )59.5 335.15 ms
(But as I said,)59.5 322.15 ms
(these will be a series of videos, and we will initially start)59.5 309.15 ms
(with the averaging methods. So, in order to help you understand)59.5 296.15 ms
(a bit more about the averaging methods, let's take an example. L)59.5 283.15 ms
(et's say we have a variable called age,)59.5 270.15 ms
(as in age years, and we try to predict this. We have a model tha)59.5 257.15 ms
(t yields prediction for)59.5 244.15 ms
(age. Let's assume that)59.5 231.15 ms
(the relationship between the two, the actual age in our predicti)59.5 218.15 ms
(on,)59.5 205.15 ms
(looks like in the graph, as in the graph. So you can see that th)59.5 192.15 ms
(e model boasts)59.5 179.15 ms
(quite a higher square of a value of 0.91, but it doesn't do so)59.5 166.15 ms
(well in the whole range of values. So when age is less than 50,)59.5 153.15 ms
(the model actually does quite well. But when age is more than 50)59.5 140.15 ms
(, you can see that the average)59.5 127.15 ms
(error is higher. Now let's take another example. Let's assume we)59.5 114.15 ms
( have a second model)59.5 101.15 ms
(that also tries to predict age, but this one looks like that. As)59.5 88.15 ms
( you can see, this model does quite)59.5 75.15 ms
(well when age is higher than 50, but not so well when age is les)59.5 62.15 ms
(s than 50,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 31 31
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 31)59.5 790.15 ms
F0 sf
(nevertheless, it scores again 0.91. So we have two models that h)59.5 764.15 ms
(ave)59.5 751.15 ms
(a similar predictive power, but they look quite different. It's )59.5 738.15 ms
(quite obvious that they do)59.5 725.15 ms
(better in different parts of the distribution of age. So what wi)59.5 712.15 ms
(ll happen if we)59.5 699.15 ms
(were to try to combine this two with a simple averaging method,)59.5 686.15 ms
(in other words, just say \(model 1 + model two\) / 2,)59.5 673.15 ms
(so a simple averaging method. The end result will look)59.5 660.15 ms
(as in the new graph. So, our square has moved to 0.95,)59.5 647.15 ms
(which is a considerable improvement versus the 0.91 we had befor)59.5 634.15 ms
(e,)59.5 621.15 ms
(and as you can see, on average, the points tend to)59.5 608.15 ms
(be closer with the reality. So the average error is smaller. How)59.5 595.15 ms
(ever, as you can see, the model doesn't)59.5 582.15 ms
(do better as an individual models for the areas where the models)59.5 569.15 ms
(were doing really well, nevertheless, it does better on average.)59.5 556.15 ms
( This is something we need to understand, that there is potentia)59.5 543.15 ms
(lly a better)59.5 530.15 ms
(way to combine these models. We could try to take a weighting av)59.5 517.15 ms
(erage. So say, I'm going to take 70% of)59.5 504.15 ms
(the first model prediction and 30% of the second model predictio)59.5 491.15 ms
(n. In other words,)59.5 478.15 ms
(\(model 1x0.7 + model 2x0.3\), and the end result would)59.5 465.15 ms
(look as in the graph. So you can see their square is no better)59.5 452.15 ms
(and that makes sense, because the models have quite similar pred)59.5 439.15 ms
(ictive power and)59.5 426.15 ms
(it doesn't make sense to rely more in one. And also it is quite )59.5 413.15 ms
(clear that)59.5 400.15 ms
(it looks more with model 1, because it has better predictions)59.5 387.15 ms
(when age is less than 50, and worse predictions)59.5 374.15 ms
(when age is more than 50. As a theoretical exercise, what is the)59.5 361.15 ms
(theoretical best we could get out of this? We know we have a mod)59.5 348.15 ms
(el that scores)59.5 335.15 ms
(really well when age is less than 50, and another model that sco)59.5 322.15 ms
(res really)59.5 309.15 ms
(well when age is more than 50. So ideally, we would like to)59.5 296.15 ms
(get to something like that. This is how we leverage the two)59.5 283.15 ms
(models in the best possible way here by using a simple)59.5 270.15 ms
(conditioning method. So if less than 50 is one I'll just)59.5 257.15 ms
(use the other, and we will see later on that there are ensemble )59.5 244.15 ms
(methods)59.5 231.15 ms
(that are very good at finding these relationships of two or more)59.5 218.15 ms
( predictions)59.5 205.15 ms
(in respect to the target variable. But, this will be a topic for)59.5 192.15 ms
(another discussion. Here we discuss simple averaging methods, ho)59.5 179.15 ms
(pefully you found it useful, and)59.5 166.15 ms
(stay here for the next session to come. Thank you very much.Hell)59.5 153.15 ms
(o everyone. This is Marios Michailidis and we will continue our )59.5 140.15 ms
(discussion in regards to ensemble methods. Previously, we saw so)59.5 127.15 ms
(me simple averaging methods. This time, we'll discuss about bagg)59.5 114.15 ms
(ing, which is a very popular and efficient form of ensembling. W)59.5 101.15 ms
(hat is bagging? bagging refers to averaging slightly different v)59.5 88.15 ms
(ersions of the same model as a means to improve the predictive p)59.5 75.15 ms
(ower. A common and quite successful application of bagging is th)59.5 62.15 ms
(e Random Forest. Where you would run many different versions of )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 32 32
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 32)59.5 790.15 ms
F0 sf
(decision trees in order to get a better prediction. Why should w)59.5 764.15 ms
(e consider bagging? Generally, in the modeling process, there ar)59.5 751.15 ms
(e two main sources of error. There are errors due to bias often )59.5 738.15 ms
(referred to as underfitting, and errors due to variance often re)59.5 725.15 ms
(ferred to as overfitting. In order to better understand this, I')59.5 712.15 ms
(ll give you two opposite examples. One with high bias and low va)59.5 699.15 ms
(riance and vice versa in order to understand the concept better.)59.5 686.15 ms
( Let's take an example of high bias and low variance. We have a )59.5 673.15 ms
(person who is let's say young, less than 30 years old and we kno)59.5 660.15 ms
(w this person is quite rich and we're trying to find him, this p)59.5 647.15 ms
(erson who'll buy a racing or an expensive car. Our model has hig)59.5 634.15 ms
(h variance, has high bias if it says that this person is young a)59.5 621.15 ms
(nd I think he's not going to buy an expensive car. What the mode)59.5 608.15 ms
(l has done here is that it hasn't explore very deep relationship)59.5 595.15 ms
( within the data. It doesn't matter that this person is young if)59.5 582.15 ms
( it has a lots of money when it comes to buying a car. It hasn't)59.5 569.15 ms
( explored different relationships. In other words, it has been u)59.5 556.15 ms
(nderfitted. However, this is also associated with low variance b)59.5 543.15 ms
(ecause this relationship, the fact that a young person generally)59.5 530.15 ms
( doesn't buy an expensive car is generally true so we would expe)59.5 517.15 ms
(ct this information to generalize well enough in a foreseen data)59.5 504.15 ms
(. Therefore, the variance is low in this example. Now, let's try)59.5 491.15 ms
( to see the other way around, an example with high variance and )59.5 478.15 ms
(low bias. Let's assume we have a person. His name is John. He li)59.5 465.15 ms
(ves in a green house, has brown eyes, and we want to see he will)59.5 452.15 ms
( buy a car. A model that has gone so deep in order to find these)59.5 439.15 ms
( relationships actually has a low bias because it has really exp)59.5 426.15 ms
(lored a lots of information about the training data. However, it)59.5 413.15 ms
( is making the mistake that every person that has these characte)59.5 400.15 ms
(ristics is going to buy a car. Therefore, it generalizes for som)59.5 387.15 ms
(ething that it shouldn't. In other words, it has already exhaust)59.5 374.15 ms
(ed the information in the training data and the results are not )59.5 361.15 ms
(significant. So, here, we actually have high variance but low bi)59.5 348.15 ms
(as. If we were to visualize the relationship between prediction )59.5 335.15 ms
(error and model complexity, it would look like that. When we beg)59.5 322.15 ms
(in the training of the model, we can see that the training error)59.5 309.15 ms
( make the error in that training data gets reduced and the same )59.5 296.15 ms
(happens in the test data because the predictions are easily gene)59.5 283.15 ms
(ralizable. They are simple. However, after a point, any improvem)59.5 270.15 ms
(ents in the training error are not realized into test data. This)59.5 257.15 ms
( is the point where the model starts over exhausting information)59.5 244.15 ms
(, creates predictions that are not generalizable. This is where )59.5 231.15 ms
(bagging actually comes into play and offers it's utmost value. B)59.5 218.15 ms
(y making slightly different or let say randomized models, we ens)59.5 205.15 ms
(ure that the predictions do not read very high variance. They're)59.5 192.15 ms
( generally more generalizable. We don't over exhaust the informa)59.5 179.15 ms
(tion in the training data. At the same time, we saw before that )59.5 166.15 ms
(when you average slightly different models, we are generally abl)59.5 153.15 ms
(e to get better predictions and we can assume that in 10 models,)59.5 140.15 ms
( we are still able to find quite significant information about t)59.5 127.15 ms
(he training data. Therefore, this is why bagging tends to work q)59.5 114.15 ms
(uite well and personally, I always use bagging. When I say, "I f)59.5 101.15 ms
(it a model," I have actually not fit a model I have fit a baggin)59.5 88.15 ms
(g version of this model so probably that different models. Which)59.5 75.15 ms
( parameters are associated with bagging? The first is the seed. )59.5 62.15 ms
(We can understand that many algorithms have some randomized proc)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 33 33
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 33)59.5 790.15 ms
F0 sf
(edures so by changing the seed you ensure that they are made sli)59.5 764.15 ms
(ghtly differently. At the same time, you can run a model with le)59.5 751.15 ms
(ss rows or you could use bootstrapping. Bootstrapping is differe)59.5 738.15 ms
(nt from row sub-sampling in the sense that you create an artific)59.5 725.15 ms
(ial dataset so you might let's say data row the training data th)59.5 712.15 ms
(ree or four times. You create a random dataset from the training)59.5 699.15 ms
( data. A different form of randomness can be imputed with shuffl)59.5 686.15 ms
(ing. There are some algorithms, which are sensitive to the order)59.5 673.15 ms
( of the data. By changing the order you ensure that the models b)59.5 660.15 ms
(ecome quite different. Another way is to dating a random sample )59.5 647.15 ms
(of columns so bid models on different features or different vari)59.5 634.15 ms
(ables of the data. Then you have model-specific parameters. For )59.5 621.15 ms
(example, in a linear model, you will try to build 10 different l)59.5 608.15 ms
(et's say logistic regression with slightly different regularizat)59.5 595.15 ms
(ion parameters. Obviously, you could also control the number of )59.5 582.15 ms
(models you include in your ensemble or in this case we call them)59.5 569.15 ms
( bags. Normally, we put a value more than 10 here but, in princi)59.5 556.15 ms
(ple, the more bags you put, it doesn't hurt you. It makes result)59.5 543.15 ms
(s better but after some point, performance start plateauing. So )59.5 530.15 ms
(there is a cost benefit with time but, in principle, more bags i)59.5 517.15 ms
(s generally better and optionally, you can also apply parallelis)59.5 504.15 ms
(m. Bagging models are independent to each other, which means you)59.5 491.15 ms
( can build many of them at the same time and make full use of yo)59.5 478.15 ms
(ur computation power. Now, we can see an example about bagging b)59.5 465.15 ms
(ut before I do that, just to let you know that a bagging estimat)59.5 452.15 ms
(ors that scikit-learn has in Python are actually quite cool. The)59.5 439.15 ms
(refore, I recommend them. This is a typical 15 lines of code tha)59.5 426.15 ms
(t I use quite often. They seem really simple but they're actuall)59.5 413.15 ms
(y quite efficient. Assuming you have a training at the test data)59.5 400.15 ms
(set and to target variable, what you do is you specify some bagg)59.5 387.15 ms
(ing parameters. What is the model I'm going to use at random for)59.5 374.15 ms
(est? How many bags I'm going to run? 10. What will be my seed? O)59.5 361.15 ms
(ne. Then you create an object, an empty object that will save th)59.5 348.15 ms
(e predictions and then you run a loop for as many bags as you ha)59.5 335.15 ms
(ve specified. In this loop, you repeat the same. You change the )59.5 322.15 ms
(seed, you feed the model, you make predictions in the test data )59.5 309.15 ms
(and you save these predictions and then, you just take an averag)59.5 296.15 ms
(e of these predictions. This is the end of the session. In this )59.5 283.15 ms
(session, we discussed bagging as a popular form of ensembling. W)59.5 270.15 ms
(e saw bagging in association with variants and bias and we also )59.5 257.15 ms
(saw in the example about how to use it. Thank you very much. The)59.5 244.15 ms
( next session we will describe boosting, which is also very popu)59.5 231.15 ms
(lar so stay in tune and have a good day.Hello, everyone. This is)59.5 218.15 ms
( Marios Michailidis. And today, we'll continue our discussion wi)59.5 205.15 ms
(th ensemble methods, and specifically, with a very popular form )59.5 192.15 ms
(of ensembling called boosting. What is boosting? Boosting is a f)59.5 179.15 ms
(orm of weighted averaging of models where each model is built se)59.5 166.15 ms
(quentially in a way that it takes into account previous model pe)59.5 153.15 ms
(rformance. In order to understand this better, remember that bef)59.5 140.15 ms
(ore, we discussed about biking, and we saw that we can have it a)59.5 127.15 ms
(t many different models, which are independent to each other in )59.5 114.15 ms
(order to get a better prediction. Boosting does something differ)59.5 101.15 ms
(ent. It says, now I tried to make a model, but I take into accou)59.5 88.15 ms
(nt how well the previous models have done in order to make a bet)59.5 75.15 ms
(ter prediction. So, every model we add sequentially to the ensem)59.5 62.15 ms
(ble, it takes into account how well the previous models have don)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 34 34
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 34)59.5 790.15 ms
F0 sf
(e in order to make better predictions. There are two main boosti)59.5 764.15 ms
(ng type of algorithms. One is based on weight, and the other is )59.5 751.15 ms
(based on residual error, and we will discuss both of them one by)59.5 738.15 ms
( one. For weight boosting, it's better to see an example in orde)59.5 725.15 ms
(r to understand it better. Let's say we have a tabular data set,)59.5 712.15 ms
( with four features. Let's call them x0, x1, x2, and x3, and we )59.5 699.15 ms
(want to use these features to predict a target variable, y. What)59.5 686.15 ms
( we are going to do in weight boosting is, we are going to fit a)59.5 673.15 ms
( model, and we will generate predictions. Let's call them pred. )59.5 660.15 ms
(These predictions have a certain margin of error. We can calcula)59.5 647.15 ms
(te these absolute error, and when I say absolute error, is absol)59.5 634.15 ms
(ute of y minus our prediction. You can see there are predictions)59.5 621.15 ms
( which are very, very far off, like row number five, but there a)59.5 608.15 ms
(re others like number six, which the model has actually done qui)59.5 595.15 ms
(te well. So what we do based on this is we generate, let's say, )59.5 582.15 ms
(a new column or a new vector, where we create a weight column, a)59.5 569.15 ms
(nd we say that this weight is 1 plus the absolute error. There a)59.5 556.15 ms
(re different ways to calculate this weight. Now, I'm just giving)59.5 543.15 ms
( you this as an example. You can infer that there are different )59.5 530.15 ms
(ways to do this, but the overall principle is very similar. So w)59.5 517.15 ms
(hat you're going to do next is, you're going to fit a new model )59.5 504.15 ms
(using the same features and the same target variable, but you're)59.5 491.15 ms
( going to also add this weight. What weight says to the model is)59.5 478.15 ms
(, I want you to put more significance into a certain role. You c)59.5 465.15 ms
(an almost interpret weight has the number of times that a certai)59.5 452.15 ms
(n row appears in my data. So let's say weight was 2, this means )59.5 439.15 ms
(that this row appears twice, and therefore, has bigger contribut)59.5 426.15 ms
(ion to the total error. You can keep repeating this process. You)59.5 413.15 ms
( can, again, calculate a new error based on this error. You calc)59.5 400.15 ms
(ulate new weights, and this is how you sequentially add models t)59.5 387.15 ms
(o the ensemble that take into account how well each model has do)59.5 374.15 ms
(ne in certain cases, maximizing the focus from where the previou)59.5 361.15 ms
(s models have done more wrong. There are certain parameters asso)59.5 348.15 ms
(ciated with this type of boosting. One is the learning rate. We )59.5 335.15 ms
(can also call it shrinkage or eta. It has different names. Now, )59.5 322.15 ms
(if you recall, I explained boosting as a form of weighted averag)59.5 309.15 ms
(ing. And this is true, because normally what this learning rate.)59.5 296.15 ms
( So what we say is, every new model we built, we shouldn't trust)59.5 283.15 ms
( it 100%. We should trust it a little bit. This ensures that we )59.5 270.15 ms
(don't have one model generally having too much contribution, and)59.5 257.15 ms
( completely making something that is not very generalizable. So )59.5 244.15 ms
(this ensures that we don't over-trust one model, we trust many m)59.5 231.15 ms
(odels a little bit. It is very good to control over fitting. The)59.5 218.15 ms
( second parameter we look at is the number of estimators. This i)59.5 205.15 ms
(s quite important. And normally, there is an inverse relationshi)59.5 192.15 ms
(p, an opposite relationship, with the learning rate. So the more)59.5 179.15 ms
( estimators we add to these type of ensemble, the smaller learni)59.5 166.15 ms
(ng rate we need to put. This is sometimes quite difficult to fin)59.5 153.15 ms
(d the right values, and we do it with the help of cross-validati)59.5 140.15 ms
(on. So normally, we start with a fixed number of estimators, let)59.5 127.15 ms
('s say, 100, and then, we try to find the optimal learning rate )59.5 114.15 ms
(for this 100 estimators. Let's say, based on cross-validation pe)59.5 101.15 ms
(rformance, we find this to be 0.1. What we can do then is, let's)59.5 88.15 ms
( say, we can double the number of estimators, make it 200, and d)59.5 75.15 ms
(ivide the learning rate by 2, so we can put 0.05, and then we ta)59.5 62.15 ms
(ke performance. It may be that the relationship is not as linear)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 35 35
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 35)59.5 790.15 ms
F0 sf
( as I explained, and the best learning rate may be 0.04 or 0.06 )59.5 764.15 ms
(after duplicating the estimators, but this is roughly the logic.)59.5 751.15 ms
( This is how we work in order to increase estimators, and try to)59.5 738.15 ms
( see more estimators give us better performance without losing s)59.5 725.15 ms
(o much time, every time, trying to find the best learning rate. )59.5 712.15 ms
(Another thing we look at is the type of input model. And general)59.5 699.15 ms
(ly, we can perform boosting with any type of estimator. The only)59.5 686.15 ms
( condition is that it needs to accept weight in it's modeling pr)59.5 673.15 ms
(ocess. So I weigh to say how much we should rely in each role of)59.5 660.15 ms
( our data set. And then, we have various boosting types. As I sa)59.5 647.15 ms
(id, I roughly explained to you how we can use the weight as a me)59.5 634.15 ms
(ans to focus on different rows, different cases the model has do)59.5 621.15 ms
(ne wrong, but there are different ways to express this. For exam)59.5 608.15 ms
(ple, there are certain boosting algorithm that do not care about)59.5 595.15 ms
( the margin of error, they only care if you did the classificati)59.5 582.15 ms
(on correct or not. So there are different variations. One I real)59.5 569.15 ms
(ly like is the AdaBoost, and there is a very good implementation)59.5 556.15 ms
( in sklearn, where you can choose any input algorithm. I think i)59.5 543.15 ms
(t's really good. And another one I really like is, normally, it')59.5 530.15 ms
(s only good for logistic regression, and there is a very good im)59.5 517.15 ms
(plementation in Weka for Java if you want to try. Now, let's mov)59.5 504.15 ms
(e onto the our time of boosting, which has been the most success)59.5 491.15 ms
(ful. I believe that in any predictive modeling competition that )59.5 478.15 ms
(was not image classification or predicting videos. This has been)59.5 465.15 ms
( the most dominant type of algorithm that actually has one most )59.5 452.15 ms
(in these challenges so this type of boosting has been extremely )59.5 439.15 ms
(successful, but what is it? I'll try to give you again a similar)59.5 426.15 ms
( example in order to understand the concept. Let's say we have a)59.5 413.15 ms
(gain the same dataset, same features, again when trying to predi)59.5 400.15 ms
(ct a y variable, we fit a model, we make predictions. What we do)59.5 387.15 ms
( next, is we'll calculate the error of these predictions but thi)59.5 374.15 ms
(s time, not in absolute terms because we're interested about the)59.5 361.15 ms
( direction of the error. What we do next is we take this error a)59.5 348.15 ms
(nd we make it adding new y variable so the error now becomes the)59.5 335.15 ms
( new target variable and we use the same features in order to pr)59.5 322.15 ms
(edict this error. It's an interesting concept and if we wanted, )59.5 309.15 ms
(let's say to make predictions for Rownum equals one, what we wou)59.5 296.15 ms
(ld do is we will take our initial prediction and then we'll add )59.5 283.15 ms
(the new prediction, which is based on the error of the first pre)59.5 270.15 ms
(diction. So initially, we have 0.75 and then we predicted 0.2. I)59.5 257.15 ms
(n order to make a final prediction, we would say one plus the ot)59.5 244.15 ms
(her equals 0.95. If you recall, the target for this row, it was )59.5 231.15 ms
(one. Using two models, we were able to get closer to the actual )59.5 218.15 ms
(answer. This form of boosting works really, really well to minim)59.5 205.15 ms
(ize the error. There are certain parameters again which are asso)59.5 192.15 ms
(ciated with this type of boosting. The first is again the learni)59.5 179.15 ms
(ng rate and it works pretty much as I explained it before. What )59.5 166.15 ms
(you need to take into account is how this is applied. Let's say )59.5 153.15 ms
(we have a learning rate of 0.1. In the previous example, where t)59.5 140.15 ms
(he prediction was 0.2 for the second model, what you will say is)59.5 127.15 ms
( I want to move my prediction towards that direction only 10 per)59.5 114.15 ms
(cent. If you remember the prediction was 0.2, 10 percent of this)59.5 101.15 ms
( is 0.02. This is how much we would move towards the prediction )59.5 88.15 ms
(of the error. This is a good way to control over fitting. Again,)59.5 75.15 ms
( we ensure we don't over rely in one model. Again, how many esti)59.5 62.15 ms
(mators you put is quite important. Normally, more is better but )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 36 36
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 36)59.5 790.15 ms
F0 sf
(you need to offset this with the right learning rate. You need t)59.5 764.15 ms
(o make certain that every model has the right contribution. If y)59.5 751.15 ms
(ou intent to put many, then you need to make sure that your mode)59.5 738.15 ms
(ls have very, very small contribution. Again, you decide these p)59.5 725.15 ms
(arameters based on cross-validation and the logic is very simila)59.5 712.15 ms
(r as explained before. Other things that work really well is tak)59.5 699.15 ms
(ing a subset of rows or a subset of columns when you build its m)59.5 686.15 ms
(odel. Actually, there is no reason why we wouldn't use this with)59.5 673.15 ms
( the previous algorithm. The way its based, it is more common wi)59.5 660.15 ms
(th this type of boosting, and internally works quite well. For i)59.5 647.15 ms
(nput model, I have seen that this method works really well with )59.5 634.15 ms
(this increase but theoretically, you can put anything you want. )59.5 621.15 ms
(Again, there are various boosting types. I think the two most co)59.5 608.15 ms
(mmon or more successful right now in a predictive modeling conte)59.5 595.15 ms
(xt is the gradient based, which is actually what I explained wit)59.5 582.15 ms
(h you how the prediction and you don't move 100 percent with tha)59.5 569.15 ms
(t direction if you apply the learning rate. The other very inter)59.5 556.15 ms
(esting one, which I've actually find it very efficient especiall)59.5 543.15 ms
(y in classification problems is the dart. Dart, it imposes a dro)59.5 530.15 ms
(p out mechanism in order to control the contribution of the tree)59.5 517.15 ms
(s. This is a concept derived from deep learning where you say, ")59.5 504.15 ms
(Every time I make a new prediction in my sample, every time I ad)59.5 491.15 ms
(d a new estimate or I'm not relying on all previous estimators b)59.5 478.15 ms
(ut only on a subset of them." Just to give you an example, let's)59.5 465.15 ms
( say we have a drop out rate of 20 percent. So far, we have buil)59.5 452.15 ms
(t 10 trees, we want to or 10 models and then we try to see, we t)59.5 439.15 ms
(ry to build a new, an 11th one. What we'll do is we will randoml)59.5 426.15 ms
(y exclude two trees when we generate a prediction for that 11th )59.5 413.15 ms
(tree or that 11th model. By randomly excluding some models, by i)59.5 400.15 ms
(ntroducing this kind of randomness, it works as a form of regula)59.5 387.15 ms
(rization. Therefore, it helps a lot to make a model that general)59.5 374.15 ms
(izes quite well enough for same data. This concept tends to work)59.5 361.15 ms
( quite well because this type of boosting algorithm has been so )59.5 348.15 ms
(successful. There have been many implementations to try to impro)59.5 335.15 ms
(ve on different parts of these algorithms. One really successful)59.5 322.15 ms
( application especially in the comparative predictive modeling w)59.5 309.15 ms
(orld is the Xgboost. It is very scalable and it supports many lo)59.5 296.15 ms
(ss functions. At the same time, is available in all major progra)59.5 283.15 ms
(mming languages for data science. Another good implementation is)59.5 270.15 ms
( Lightgbm. As the name connotes, it is lightning fast. Also, it )59.5 257.15 ms
(is supported by many programming languages and supports many los)59.5 244.15 ms
(s functions. Another interesting case is the Gradient Boosting M)59.5 231.15 ms
(achine from H2O. What's really interesting about this implementa)59.5 218.15 ms
(tion is that it can handle categorical variables out of the box )59.5 205.15 ms
(and it also comes with a real set of parameters where you can co)59.5 192.15 ms
(ntrol the modeling process quite thoroughly. Another interesting)59.5 179.15 ms
( case, which is also fairly new is the Catboost. What's really g)59.5 166.15 ms
(ood about this is that it comes with the strong initial set of p)59.5 153.15 ms
(arameters. Therefore, you don't need to spend so much time tunin)59.5 140.15 ms
(g. As I mentioned before, this can be quite a time consuming pro)59.5 127.15 ms
(cess. It can also handle categorical variables out of the box. U)59.5 114.15 ms
(ltimately, I really like the Gradient Boosting Machine implement)59.5 101.15 ms
(ation of Scikit-learn. What I really like about this is that you)59.5 88.15 ms
( can put any scikit-learn estimator as a base. This is the end o)59.5 75.15 ms
(f this video. In the next session, we will discuss docking, whic)59.5 62.15 ms
(h is also very popular, so stay tuned.Continuing our discussion )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 37 37
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 37)59.5 790.15 ms
F0 sf
(with ensemble)59.5 764.15 ms
(methods, next one up is stacking. Stacking is a very, very popul)59.5 751.15 ms
(ar form of ensembling using)59.5 738.15 ms
(predictive modeling competitions. And I believe in most competit)59.5 725.15 ms
(ions,)59.5 712.15 ms
(there is a form of stacking in the end in order to boost)59.5 699.15 ms
(your performance as best as you can. Going through the definitio)59.5 686.15 ms
(n of stacking,)59.5 673.15 ms
(it essentially means making several predictions)59.5 660.15 ms
(with hold-out data sets. And then collecting or)59.5 647.15 ms
(stacking these predictions to form a new data set,)59.5 634.15 ms
(where you can fit a new model on it, on this newly-formed data)59.5 621.15 ms
(set from predictions. I would like to take you through)59.5 608.15 ms
(a very simple, I would say naive, example to show you how,)59.5 595.15 ms
(conceptually, this can work. I mean, we have so far seen that yo)59.5 582.15 ms
(u)59.5 569.15 ms
(can use previous models' predictions to affect a new model, but)59.5 556.15 ms
(always in relation with the input data. This is a new concept be)59.5 543.15 ms
(cause we're)59.5 530.15 ms
(only going to use the predictions of some models in order)59.5 517.15 ms
(to make a better model. So let's see how these could)59.5 504.15 ms
(work in a real life scenario. Let's assume we have three kids,)59.5 491.15 ms
(let's name them LR, SVM, KNN, and)59.5 478.15 ms
(they argue about a physics question. So each one believes the an)59.5 465.15 ms
(swer to)59.5 452.15 ms
(a physics question is different. First one says 13, second 18, t)59.5 439.15 ms
(hird 11, they don't know how to)59.5 426.15 ms
(solve this disagreement. They do the honorable thing,)59.5 413.15 ms
(they say let's take an average, which in this case is 14. So you)59.5 400.15 ms
( can almost see the kids, there's different models here,)59.5 387.15 ms
(they take input data. In this case,)59.5 374.15 ms
(it's the question about physics. They process it based on)59.5 361.15 ms
(historical information and and they are able to output an estima)59.5 348.15 ms
(te,)59.5 335.15 ms
(a prediction. Have they done it optimally, though? Another way t)59.5 322.15 ms
(o say this is)59.5 309.15 ms
(to say there was a teacher, Miss DL, who had seen this discussio)59.5 296.15 ms
(n,)59.5 283.15 ms
(and she decided to step up. While she didn't hear the question,)59.5 270.15 ms
(she does know the students quite well, she knows the strength an)59.5 257.15 ms
(d)59.5 244.15 ms
(weaknesses of each one. She knows how well they have done)59.5 231.15 ms
(historically in physics questions. And from the range of values )59.5 218.15 ms
(they have)59.5 205.15 ms
(provided, she is able to give an estimate. Let's say that in thi)59.5 192.15 ms
(s concept, she knows)59.5 179.15 ms
(that SVM is really good in physics, and her father works in the )59.5 166.15 ms
(department)59.5 153.15 ms
(of Physics of Excellence. And therefore she should have)59.5 140.15 ms
(a bigger contribution to this ensemble than every other kid,)59.5 127.15 ms
(therefore the answer is 17. And this is how a meta model works,)59.5 114.15 ms
(it doesn't need to know the input data. It just knows how the mo)59.5 101.15 ms
(dels)59.5 88.15 ms
(have done historically, in order to find the best)59.5 75.15 ms
(way to combine them. And this can work quite well in practice. S)59.5 62.15 ms
(o, let's go more into)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 38 38
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 38)59.5 790.15 ms
F0 sf
(the methodology of stacking. Wolpert introduced stacking in 1992)59.5 764.15 ms
(, as a meta modeling technique)59.5 751.15 ms
(to combine different models. It consists of several steps. The f)59.5 738.15 ms
(irst step is,)59.5 725.15 ms
(let's assume we have a train data set, let's divide it into two )59.5 712.15 ms
(parts; so)59.5 699.15 ms
(a training and the validation. Then you take the training part,)59.5 686.15 ms
(and you train several models. And then you make predictions for)59.5 673.15 ms
(the second part, let's say the validation data set. Then you col)59.5 660.15 ms
(lect all these predictions,)59.5 647.15 ms
(or you stack these predictions. You form a new data set and)59.5 634.15 ms
(you use this as inputs to a new model. Normally we call this a m)59.5 621.15 ms
(eta model,)59.5 608.15 ms
(and the models we run into, we call them base model or base lear)59.5 595.15 ms
(ners. If you're still confused about stacking,)59.5 582.15 ms
(consider the following animation. So let's assume we have three )59.5 569.15 ms
(data sets A,)59.5 556.15 ms
(B, and C. In this case, A will serve)59.5 543.15 ms
(the role of the training data set, B will be the validation data)59.5 530.15 ms
( set, and C will be the test data sets where we)59.5 517.15 ms
(want to make the final predictions. They all have similar archit)59.5 504.15 ms
(ectural,)59.5 491.15 ms
(four features, and one target variable we try to predict. So in )59.5 478.15 ms
(this case,)59.5 465.15 ms
(we can choose an algorithm to train a model based on data set 1,)59.5 452.15 ms
( and then we make predictions for)59.5 439.15 ms
(B and C at the same time. Now we take these predictions, and)59.5 426.15 ms
(we put them into a new data set. So we create a data set to stor)59.5 413.15 ms
(e the)59.5 400.15 ms
(predictions for the validation data in B1. And a data set called)59.5 387.15 ms
( C1 to save)59.5 374.15 ms
(predictions for the test data, called C1. Then we're going to re)59.5 361.15 ms
(peat the process, now we're going to choose)59.5 348.15 ms
(another algorithm. Again, we will fit it on A data set. We will )59.5 335.15 ms
(make predictions on B and)59.5 322.15 ms
(C at the same time, and we will save these predictions)59.5 309.15 ms
(into the newly-formed data sets. And we essentially append them,)59.5 296.15 ms
(we stack them next to each other, this is where stacking takes i)59.5 283.15 ms
(ts name. And we can continue this even more,)59.5 270.15 ms
(do it with a third algorithm. Again the same, fit on A,)59.5 257.15 ms
(predict on B and C, same predictions. What we do then is we take)59.5 244.15 ms
( the target)59.5 231.15 ms
(variable for the B data set, or the validation datadset,)59.5 218.15 ms
(which we already knew. And we are going to fit a new model on B1)59.5 205.15 ms
(with the target of the validation data, and then we will make pr)59.5 192.15 ms
(edictions from C1. And this is how we combine)59.5 179.15 ms
(different models with stacking, to hopefully make better predict)59.5 166.15 ms
(ions for)59.5 153.15 ms
(the test or the unobserved data. Let us go through an example,)59.5 140.15 ms
(a simple example in Python, in order to understand better,)59.5 127.15 ms
(as in in code, how it would work. It is quite simple, so even pe)59.5 114.15 ms
(ople not very experienced with)59.5 101.15 ms
(Python hopefully can understand this. The main logic is that we )59.5 88.15 ms
(will use)59.5 75.15 ms
(two base learners on some input data, a random forest and a line)59.5 62.15 ms
(ar regression. And then, we will try to combine)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 39 39
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 39)59.5 790.15 ms
F0 sf
(the results, starting with a meta learner, again, it will be lin)59.5 764.15 ms
(ear regression. Let's assume we again have)59.5 751.15 ms
(a train data set, and a target variable for)59.5 738.15 ms
(this data set, and a test data set. Maybe the code seems a bit i)59.5 725.15 ms
(ntimidating,)59.5 712.15 ms
(but we will go step by step. What we do initially is we take the)59.5 699.15 ms
( train)59.5 686.15 ms
(data set and we split it in two parts. So we create a training a)59.5 673.15 ms
(nd)59.5 660.15 ms
(a valid data set out of this, and we also split the target varia)59.5 647.15 ms
(ble. So we create ytraining and)59.5 634.15 ms
(yvalid, and we split this by 50%. We could have chosen something)59.5 621.15 ms
( else,)59.5 608.15 ms
(let's say 50%. Then we specify our base learners,)59.5 595.15 ms
(so model1 is the random forest in this case, and)59.5 582.15 ms
(model2 is a linear regression. What we do then is we fit)59.5 569.15 ms
(the both models using the training data and the training target.)59.5 556.15 ms
( And we make predictions for)59.5 543.15 ms
(the validation data for both models, and at the same time we'll )59.5 530.15 ms
(make)59.5 517.15 ms
(predictions for the test data. Again, for both models,)59.5 504.15 ms
(we save these as preds1, preds2, and for the test data,)59.5 491.15 ms
(test_preds1 and test_preds2. Then we are going to)59.5 478.15 ms
(collect the predictions, we are going to stack the predictions a)59.5 465.15 ms
(nd)59.5 452.15 ms
(create two new data sets. One for validation,)59.5 439.15 ms
(where we call it stacked_predictions, which consists of preds1 a)59.5 426.15 ms
(nd preds2. And then for the data set for)59.5 413.15 ms
(for the test predictions, called stacked_test_predictions, where)59.5 400.15 ms
(we stack test_preds1 and test_preds2. Then we specify a meta lea)59.5 387.15 ms
(rner, let's call it meta_model,)59.5 374.15 ms
(which is a linear regression. And we fit this model on the predi)59.5 361.15 ms
(ctions)59.5 348.15 ms
(made on the validation data and the target for the validation da)59.5 335.15 ms
(ta, which)59.5 322.15 ms
(was our holdout data set all this time. And then we can generate)59.5 309.15 ms
( predictions for the test data by applying this model)59.5 296.15 ms
(on the stacked_test_predictions. This is how it works. Now, I th)59.5 283.15 ms
(ink this is a good)59.5 270.15 ms
(time to revisit an old example we used in the first session,)59.5 257.15 ms
(about simple averaging. If you remember,)59.5 244.15 ms
(we had a prediction that was doing quite well to predict age whe)59.5 231.15 ms
(n)59.5 218.15 ms
(the age was less than 50, and another prediction that was doing)59.5 205.15 ms
(quite well when age was more than 50. And we did something trick)59.5 192.15 ms
(y,)59.5 179.15 ms
(we said if it is less than 50, we'll use the first one, if age i)59.5 166.15 ms
(s more)59.5 153.15 ms
(than 50, we will use the other one. The reason this is tricky is)59.5 140.15 ms
(because normally we use the target information to make this deci)59.5 127.15 ms
(sion. Where in an ideal world, this is what)59.5 114.15 ms
(you try to predict, you don't know it. We have done it in order )59.5 101.15 ms
(to show what)59.5 88.15 ms
(is the theoretical best we could get, or yeah, the best. So taki)59.5 75.15 ms
(ng the same predictions and applying stacking, this is what the )59.5 62.15 ms
(end)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 40 40
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 40)59.5 790.15 ms
F0 sf
(result would actually look like. As you can see,)59.5 764.15 ms
(it has done pretty similarly. The only area that there is some)59.5 751.15 ms
(error is around the threshold of 50. And that makes sense, becau)59.5 738.15 ms
(se the model)59.5 725.15 ms
(doesn't see the target variable, is not able to identify)59.5 712.15 ms
(this cut of 50 exactly. So it tries to do it only)59.5 699.15 ms
(based on the input models, and there is some overlap around this)59.5 686.15 ms
( area. But you can see that stacking)59.5 673.15 ms
(is able to identify this, and use it in order to)59.5 660.15 ms
(make better predictions. There are certain things you need to)59.5 647.15 ms
(be mindful of when using stacking. One is when you have time-sen)59.5 634.15 ms
(sitive data,)59.5 621.15 ms
(as in let's say, time series, you need to formulate your)59.5 608.15 ms
(stacking so that you respect time. What I mean is, when you crea)59.5 595.15 ms
(te)59.5 582.15 ms
(your train and validation data, you need to make certain that yo)59.5 569.15 ms
(ur train)59.5 556.15 ms
(is in the past and your validation is in the future, and ideally)59.5 543.15 ms
( your)59.5 530.15 ms
(test data is also in the future. So you need to respect this)59.5 517.15 ms
(time element in order to make certain your model generalizes wel)59.5 504.15 ms
(l. The other thing you need to look at is,)59.5 491.15 ms
(obviously, single model performance is important. But the other )59.5 478.15 ms
(thing that is)59.5 465.15 ms
(also very important is model diversity, how different)59.5 452.15 ms
(a model is to each other. What is the new information each)59.5 439.15 ms
(model brings into the table? Now, because stacking, and dependin)59.5 426.15 ms
(g)59.5 413.15 ms
(on the algorithms you will use for stacking, can go quite deep)59.5 400.15 ms
(into exploring relationships. It will find when a model is good,)59.5 387.15 ms
( and when a model is actually bad or)59.5 374.15 ms
(fairly weak. So you don't need to worry too much)59.5 361.15 ms
(to make all the models really strong, stacking can actually extr)59.5 348.15 ms
(act)59.5 335.15 ms
(the juice from each prediction. Therefore, what you really need )59.5 322.15 ms
(to focus)59.5 309.15 ms
(is, am I making a model that brings some information,)59.5 296.15 ms
(even though it is generally weak? And this is true, there have b)59.5 283.15 ms
(een many)59.5 270.15 ms
(situations where I've made, I've had some quite weak models in m)59.5 257.15 ms
(y ensemble,)59.5 244.15 ms
(I mean, compared to the top performance. And nevertheless, they )59.5 231.15 ms
(were actually)59.5 218.15 ms
(adding lots of value in stacking. They were bringing in new info)59.5 205.15 ms
(rmation)59.5 192.15 ms
(that the meta model could leverage. Normally, you introduce)59.5 179.15 ms
(diversity from two forms, one is by choosing a different algorit)59.5 166.15 ms
(hm. Which makes sense, certain algorithms capitalize on)59.5 153.15 ms
(different relationships within the data. For example, a linear m)59.5 140.15 ms
(odel will)59.5 127.15 ms
(focus on a linear relationship, a non-linear model can capture)59.5 114.15 ms
(better a non-linear relationships. So predictions may come a bit)59.5 101.15 ms
( different. The other thing is you can)59.5 88.15 ms
(even run the same model, but you try to run it on different)59.5 75.15 ms
(transformation of input data, either less features or)59.5 62.15 ms
(completely different transformation. For example, in one data se)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 41 41
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 41)59.5 790.15 ms
F0 sf
(t you may treat categorical)59.5 764.15 ms
(features as one whole encoding. In another,)59.5 751.15 ms
(you may just use label in coding, and the result will probably p)59.5 738.15 ms
(roduce)59.5 725.15 ms
(a model that is very different. Generally, there is no limit to)59.5 712.15 ms
(how many models you can stack. But you can expect that)59.5 699.15 ms
(there is a plateauing after certain models have been added. So i)59.5 686.15 ms
(nitially, you will see some)59.5 673.15 ms
(significant uplift in whatever metric you are testing on every)59.5 660.15 ms
(time you run the model. But after some point, the incremental)59.5 647.15 ms
(uplift will be fairly small. Generally, there's no)59.5 634.15 ms
(way to know this before, exactly what is the number of models)59.5 621.15 ms
(where we will start plateauing. But generally, this is a affecte)59.5 608.15 ms
(d by how)59.5 595.15 ms
(many features you have in your data, how much diversity you mana)59.5 582.15 ms
(ged)59.5 569.15 ms
(to introduce into your models, quite often how many)59.5 556.15 ms
(rows of data you have. So it is tough to know this beforehand,)59.5 543.15 ms
(but generally this is)59.5 530.15 ms
(something to be mindful of. But there is a point where adding mo)59.5 517.15 ms
(re)59.5 504.15 ms
(models actually does not add that much value. And because the me)59.5 491.15 ms
(ta model, the meta model will only use predictions of other mode)59.5 478.15 ms
(ls. We can assume that the other)59.5 465.15 ms
(models have done, let's say, a deep work or)59.5 452.15 ms
(a deep job to scrutinize the data. And therefore the meta model)59.5 439.15 ms
(doesn't need to be so deep. Normally, you have predictions with)59.5 426.15 ms
(are correlated with the target. And the only thing it needs to d)59.5 413.15 ms
(o is)59.5 400.15 ms
(just to find a way to combine them, and that is normally not so )59.5 387.15 ms
(complicated. Therefore, it is quite often that)59.5 374.15 ms
(the meta model is generally simpler. So if I was to express this)59.5 361.15 ms
(in a random forest context, it will have lower depth than what w)59.5 348.15 ms
(as the)59.5 335.15 ms
(best one you found in your base models. This was the end of the )59.5 322.15 ms
(session,)59.5 309.15 ms
(here we discussed stacking. In the next one, we will discuss a v)59.5 296.15 ms
(ery)59.5 283.15 ms
(interesting concept about stacking and extending it on multiple )59.5 270.15 ms
(levels,)59.5 257.15 ms
(called stack net. So stay in tune.We can continue our discussion)59.5 244.15 ms
( with StackNet. StackNet is a scalable meta modeling methodology)59.5 231.15 ms
( that utilizes stacking to combine multiple models in a neural n)59.5 218.15 ms
(etwork architecture of multiple levels. It is scalable because w)59.5 205.15 ms
(ithin the same level, we can run all the models in parallel. It )59.5 192.15 ms
(utilizes stacking because it makes use of this technique we ment)59.5 179.15 ms
(ioned before where we split the data, we make predictions so som)59.5 166.15 ms
(e hold out data, and then we use another model to train on those)59.5 153.15 ms
( predictions. And as we will see later on, this resembles a lot )59.5 140.15 ms
(in neural network. Now let us continue that naive example we gav)59.5 127.15 ms
(e before with the students and the teacher, in order to understa)59.5 114.15 ms
(nd what conceptually, in a real world, would need to add another)59.5 101.15 ms
( layer. So in that example, we have a teacher that she was tryin)59.5 88.15 ms
(g to combine the answers of different students and she was outpu)59.5 75.15 ms
(tting an estimate of 17 under certain assumptions. We can make t)59.5 62.15 ms
(his example more interesting by introducing one more meta learne)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 42 42
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 42)59.5 790.15 ms
F0 sf
(r. Let's call him Mr. RF, who's also a physics teacher. Mr. RF b)59.5 764.15 ms
(elieves that LR should have a bigger contribution to the ensembl)59.5 751.15 ms
(e because he has been doing private lessons with him and he know)59.5 738.15 ms
(s he couldn't be that far off. So he's able to see the data from)59.5 725.15 ms
( slightly different ways to capitalize on different parts of the)59.5 712.15 ms
(se predictions and make a different estimate. Whereas, the teach)59.5 699.15 ms
(ers could work it out and take an average, we could create or we)59.5 686.15 ms
( can introduce a higher authority or another layer of modeling h)59.5 673.15 ms
(ere. Let's call it the headmaster, GBM, in order to shop, make b)59.5 660.15 ms
(etter predictions. And GBM doesn't need to know the answers that)59.5 647.15 ms
( the students have given. The only thing he needs to know is the)59.5 634.15 ms
( input from the teachers. And in this case, he's more keen to tr)59.5 621.15 ms
(ust his physics teacher by outputting a 16.2 prediction. Why wou)59.5 608.15 ms
(ld this be of any use to people? I mean, isn't that already comp)59.5 595.15 ms
(licated? Why would we want to ever try something so complicated?)59.5 582.15 ms
( I'm giving you an example of a competition my team used, four l)59.5 569.15 ms
(ayer of stacking, in order to win. And we used two different sou)59.5 556.15 ms
(rces of input data. We generated multiple models. Normally, exit)59.5 543.15 ms
( boost and logistic regressions, and then we fed those into a fo)59.5 530.15 ms
(ur-layer architecture in order to get the top score. And althoug)59.5 517.15 ms
(h we could have escaped without using that fourth layer, we stil)59.5 504.15 ms
(l need it up to level three in order to win. So you can understa)59.5 491.15 ms
(nd the usefulness of deploying deep stacking. Another example is)59.5 478.15 ms
( the Homesite competition organized by Homesite insurance where )59.5 465.15 ms
(again, we created many different views of the data. So we had di)59.5 452.15 ms
(fferent transformations. We generated many models. We fed those )59.5 439.15 ms
(models into a three-level architecture. I think we didn't need t)59.5 426.15 ms
(he third layer again. Probably, we could have escaped with only )59.5 413.15 ms
(two levels but again, deep stacking was necessary in order to wi)59.5 400.15 ms
(n. So there is your answer, deep stacking on multiple levels rea)59.5 387.15 ms
(lly helps you to win competitions. In the spirit of fairness and)59.5 374.15 ms
( openness, there has been some criticism about large ensembles t)59.5 361.15 ms
(hat maybe they don't have commercial value, they are confidentia)59.5 348.15 ms
(lly expensive. I have to add three things on that. The first is,)59.5 335.15 ms
( what is considered expensive today may not be expensive tomorro)59.5 322.15 ms
(w and we have seen that, for example, with the deep learning, wh)59.5 309.15 ms
(ere with the advent of GPUs, they have become 100 times faster a)59.5 296.15 ms
(nd now they have become again very, very popular. The other thin)59.5 283.15 ms
(g is, you don't need to always build very, very deep ensembles b)59.5 270.15 ms
(ut still, small ensembles would still really help. So knowing ho)59.5 257.15 ms
(w to do them can add value to businesses, again based on differe)59.5 244.15 ms
(nt assumptions about how fast they want the decisions, how much )59.5 231.15 ms
(is the uplift you can see from stacking, which may vary, sometim)59.5 218.15 ms
(es it's more, sometime is less. And generally, how much computin)59.5 205.15 ms
(g power they have. We can make a case that even stacking on mult)59.5 192.15 ms
(iple layers can be very useful. And the last point is that these)59.5 179.15 ms
( are predictive modeling competitions so it is a bit like the Ol)59.5 166.15 ms
(ympics. It is nice to be able to see the theoretical best you ca)59.5 153.15 ms
(n get because this is how innovation takes over. This is how we )59.5 140.15 ms
(move forward. We can express StackNet as a neural network. So no)59.5 127.15 ms
(rmally, in a neural network, we have these architecture of hidde)59.5 114.15 ms
(n units where they are connected with input with the form of lin)59.5 101.15 ms
(ear regression. So actually, it looks pretty much like a linear )59.5 88.15 ms
(regression. So whether you have a set of coefficients and you ha)59.5 75.15 ms
(ve a constant value where you call it bias in neaural networks, )59.5 62.15 ms
(and this is how your output predictions which one of the hidden )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 43 43
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 43)59.5 790.15 ms
F0 sf
(units which are then taken, collected, to create the output. The)59.5 764.15 ms
( concept of StackNet is actually not that much different. The on)59.5 751.15 ms
(ly thing we want to do is, we don't want to be limited to that l)59.5 738.15 ms
(inear regression or to that perception. We want to be able to us)59.5 725.15 ms
(e any machine learning algorithm. Putting that aside, the archit)59.5 712.15 ms
(ecture should be exactly the same, could be fairly similar. So h)59.5 699.15 ms
(ow to train this? In a typical neural network, we use bipropagat)59.5 686.15 ms
(ion. Here in this context, this is not feasible. I mean in the c)59.5 673.15 ms
(ontext of trying to make this network work with any input model )59.5 660.15 ms
(because not all are differentiable. So this is why we can use st)59.5 647.15 ms
(acking. Stacking here is a way to link the output, the predictio)59.5 634.15 ms
(n, the output of the node, with target variable. This is how the)59.5 621.15 ms
( link also is made from the input features with a node. However,)59.5 608.15 ms
( if you remember the way that stacking works is you have some tr)59.5 595.15 ms
(ain data. And then, you need to divide it into two halves. So, y)59.5 582.15 ms
(ou use the first part called, training, in order to make predict)59.5 569.15 ms
(ions to the other part called, valid. If we, assuming that addin)59.5 556.15 ms
(g more layers gives us some uplift, if we wanted to do this agai)59.5 543.15 ms
(n, we would have re-split the valid data into two parts. Let's c)59.5 530.15 ms
(all it, mini train, and mini valid. And you can see the problem )59.5 517.15 ms
(here. I mean, assuming if we have really big data, then this may)59.5 504.15 ms
( not really be an issue. But in certain situations where we don')59.5 491.15 ms
(t have that much data. Ideally, we would like to do this without)59.5 478.15 ms
( having to constantly re-split our data. And therefore minimizin)59.5 465.15 ms
(g the training data set. So, this is why we use a K-Fold paradig)59.5 452.15 ms
(m. Let's assume we have a training data set with four features x)59.5 439.15 ms
(0, x1, x2, x3, and the y variable, or target. If we are use k-fo)59.5 426.15 ms
(ld where k = 4, this is a hyper-parameter which is what to put h)59.5 413.15 ms
(ere. We would make four different parts out of these datasets. H)59.5 400.15 ms
(ere I have put different colors, colors to each one of these par)59.5 387.15 ms
(ts. What we would do then in order to commence the training, is )59.5 374.15 ms
(we will create an empty vector that has the same size as rows, a)59.5 361.15 ms
(s in the training data, but for now is empty. And then, for each)59.5 348.15 ms
( one of the folds, we would start, we will take a subset of the )59.5 335.15 ms
(training data. In this case, we will start with red, yellow, and)59.5 322.15 ms
( green. We will train a model, and then we will take the blue pa)59.5 309.15 ms
(rt, and will make predictions. And we will take these prediction)59.5 296.15 ms
(s, and we will put them in the corresponding location in the pre)59.5 283.15 ms
(diction array which was empty. Now, we are going to repeat the s)59.5 270.15 ms
(ame process always using this rotation. So, we are now going to )59.5 257.15 ms
(use the blue, the yellow, and the green part, and we will keep t)59.5 244.15 ms
(o create a model, and we will keep the red part for prediction. )59.5 231.15 ms
(Again, we will take these predictions and put it into the corres)59.5 218.15 ms
(ponding part in the prediction array. And we will repeat again w)59.5 205.15 ms
(ith the yellow, and the green. Something that I need to mention )59.5 192.15 ms
(is that the K-Fold doesn't need to be sequential as a date. So, )59.5 179.15 ms
(it would have been shuffled. I did it as this way in order to il)59.5 166.15 ms
(lustrate it better. But once we have finished and we have genera)59.5 153.15 ms
(ted a whole prediction for the whole training data, then we can )59.5 140.15 ms
(use the whole training data, in order to fit one last model and )59.5 127.15 ms
(make now predictions for the test data. Another way we could hav)59.5 114.15 ms
(e done this is for each one of the four models we were making pr)59.5 101.15 ms
(edictions for the validation data. At the same time, we could ha)59.5 88.15 ms
(ve been making predictions for the whole test data. And after fo)59.5 75.15 ms
(ur models, we will just take an average at the end. We'll just d)59.5 62.15 ms
(ivide the test predictions by four. But a different way to do it)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 44 44
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week4.txt                                                Page 44)59.5 790.15 ms
F0 sf
(, I have found this way I just explained better with neural netw)59.5 764.15 ms
(orks, and the method where you use the whole training data to ge)59.5 751.15 ms
(nerate predictions for test better with tree-based methods. So, )59.5 738.15 ms
(once we finish the predictions with the test, you can start agai)59.5 725.15 ms
(n with another model this time. So you will generate an empty pr)59.5 712.15 ms
(ediction, you will stack it next to your previous one. And you w)59.5 699.15 ms
(ill repeat the same process. You will essentially repeat this un)59.5 686.15 ms
(til you're finished with all models for the same layer. And then)59.5 673.15 ms
(, this will become your new training data set and you will gener)59.5 660.15 ms
(ally begin all over again if you have a new layer. This is gener)59.5 647.15 ms
(ally the concept. Though we could say this, in order to extend o)59.5 634.15 ms
(n many layers, we use this K-Fold paradigm. However, normally, n)59.5 621.15 ms
(eural networks we have this notion of epochs. We have iterations)59.5 608.15 ms
( which help us to re-calibrate the weights between the nodes. He)59.5 595.15 ms
(re we don't have this option, the way stacking is. However, we c)59.5 582.15 ms
(an introduce this ability of revisiting the initial data through)59.5 569.15 ms
( connections. So, a typical way to connect the nodes is the one )59.5 556.15 ms
(we have already explored where you have it input nodes, each nod)59.5 543.15 ms
(e is directly related with the nodes of the previous layer. Anot)59.5 530.15 ms
(her way to do this is to say, a node is not only affected, conne)59.5 517.15 ms
(cted with the nodes of the directly previous layer, but from all)59.5 504.15 ms
( previous nodes from any previous layer. So, in order to illustr)59.5 491.15 ms
(ate this better, if you remember the example with the headmaster)59.5 478.15 ms
( where he was using predictions from the teachers, he could have)59.5 465.15 ms
( been using also predictions from the students at the same time.)59.5 452.15 ms
( This actually can work quite well. And you can also refit the i)59.5 439.15 ms
(nitial data. Not just the predictions, you can actually put your)59.5 426.15 ms
( initial x data set, and append it to your predictions. This can)59.5 413.15 ms
( work really well if you haven't made many models. So that way, )59.5 400.15 ms
(you get the chance to revisit that initial training data, and tr)59.5 387.15 ms
(y to capture more informations. And because we already have meta)59.5 374.15 ms
(l-models present, the model tries to focus on where we can explo)59.5 361.15 ms
(re any new information. So in this kind of situation it works qu)59.5 348.15 ms
(ite well. Also, this is very similar to target encoding or many )59.5 335.15 ms
(encoding you've seen before where you use some part of the data,)59.5 322.15 ms
( let's say, a code on a categorical column, given some cross-val)59.5 309.15 ms
(idation, you generate some estimates for the target variable. An)59.5 296.15 ms
(d then, you insert this into your training data. Okay, you don't)59.5 283.15 ms
( stack it, as in you don't create a new column, but essentially )59.5 270.15 ms
(you replace one column with hold out predictions of your target )59.5 257.15 ms
(variable which is essentially very similar. You have created the)59.5 244.15 ms
( logic for the target variable, and you are essentially insertin)59.5 231.15 ms
(g it into your training data idea.)59.5 218.15 ms
re sp
%%PageTrailer
%%Trailer
%%Pages: 44
%%EOF
