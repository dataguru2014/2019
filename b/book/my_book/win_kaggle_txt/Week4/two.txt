[SOUND] Hi, to this moment,
we have already discussed all basics new things which build up to
a big solution like featured generation, validation, minimalist codings and so on. We went through several
competitions together and tried our best to unite everything
we learn into one huge framework. But as with any other set of tools,
there are a lot of heuristics which people often find only with a trial and
error approach, spending significant time on learning
how to use these tools efficiently. So to help you out here, in this video we'll share things we
learned the hard way, by experience. These things may vary from
one person to another. So we decided that everyone on class will
present his own guidelines personally, to stress the possible
diversity in a broad issues and to make an accent on different moments. Some notes might seem obvious to you,
some may not. But be sure for even some of them or
at least no one involve them. Can save you a lot of time. So, let's start. When we want to enter a competition,
define your goals and try to estimate what you can
get out of your participation. You may want to learn more
about an interesting problem. You may want to get acquainted
with new software tools and packages, or
you may want to try to hunt for a medal. Each of these goals will influence what
competition you choose to participate in. If you want to learn more
about an interesting problem, you may want the competition to have
a wide discussion on the forums. For example, if you are interested in
data science, in application to medicine, you can try to predict lung cancer
in the Data Science Bowl 2017. Or to predict seizures in long
term human EEG recordings. In the Melbourne University
Seizure Prediction Competition. If you want to get acquainted
with new software tools, you may want the competition
to have required tutorials. For example, if you want to
learn a neural networks library. You may choose any of competitions with
images like the nature conservancy features, monitoring competition. Or the planet, understanding
the Amazon from space competition. And if you want to try to hunt for a medal, you may want to check how
many submissions do participants have. And if the points that people have
over one hundred submissions, it can be a clear sign of legible
problem or difficulties in validation includes an inconsistency of
validation and leaderboard scores. On the other hand, if there are people
with few submissions in the top, that usually means there should be
a non-trivial approach to this competition or it's discovered only by few people. Beside that, you may want to pay
attention to the size of the top teams. If leaderboard mostly consists of
teams with only one participant, you'll probably have enough
chances if you gather a good team. Now, let's move to the next step
after you chose a competition. As soon as you get familiar with the data, start to write down your ideas about
what you may want to try later. What things could work here? What approaches you may want to take. After you're done, read forums and
highlight interesting posts and topics. Remember, you can get a lot of information
and meet new people on forums. So I strongly encourage you to
participate in these discussions. After the initial pipeline is ready and you roll down few ideas, you may want
to start improving your solution. Personally, I like to organize
these ideas into some structure. So you may want to sort
ideas into priority order. Most important and
promising needs to be implemented first. Or you may want to organize
these ideas into topics. Ideas about feature generation,
validation, metric optimization. And so on. Now pick up an idea and implement it. Try to derive some insights on the way. Especially, try to understand why
something does or doesn't work. For example, you have an idea about trying a deep
gradient boosting decision tree model. To your joy, it works. Now, ask yourself why? Is there some hidden data
structure we didn't notice before? Maybe you have categorical features
with a lot of unique values. If this is the case, you as well can make a conclusion that
mean encodings may work great here. So in some sense,
the ability to analyze the work and derive conclusions while
you're trying out your ideas will get you on the right track to
reveal hidden data patterns and leaks. After we checked out most important ideas, you may want to switch
to parameter training. I personally like the view,
everything is a parameter. From the number of features, through
gradient boosting decision through depth. From the number of layers in
convolutional neural network, to the coefficient you finally
submit is multiplied by. To understand what I should tune and change first, I like to sort all
parameters by these principles. First, importance. Arrange parameters from
important to not useful at all. Tune in this order. These may depend on data structure,
on target, on metric, and so on. Second, feasibility. Rate parameters from, it is easy to tune,
to, tuning this can take forever. Third, understanding. Rate parameters from, I know what
it's doing, to, I have no idea. Here it is important to understand
what each parameter will change in the whole pipeline. For example, if you increase
the number of features significantly, you may want to change ratio of columns
which is used to find the best split in gradient boosting decision tree. Or, if you change number of layers
in convolution neural network, you will need more reports to train it,
and so on. So let's see, these were some
of my practical guidelines, I hope they will prove useful for
you as well. Every problem starts with data loading and
preprocessing. I usually don't pay much attention to
some sub optimal usage of computational resources but this particular
case is of crucial importance. Doing things right at the very beginning
will make your life much simpler and will allow you to save a lot of time and
computational resources. I usually start with basic data
preprocessing like labeling, coding, category recovery,
both enjoying additional data. Then, I dump resulting data into HDF5 or
MPI format. HDF5 for Panda's dataframes,
and MPI for non bit arrays. Running experiment often require
a lot of kernel restarts, which leads to reloading all the data. And loading class CSC files
may take minutes while loading data from HDF5 or MPI formats
is performed in a matter of seconds. Another important matter is that by
default, Panda is known to store data in 64-bit arrays, which is
unnecessary in most of the situations. Downcasting everything to 32 bits will
result in two-fold memory saving. Also keep in mind that Panda's support
out of the box data relink by chunks, via chunks ice parameter
in recess fee function. So most of the data sets may be
processed without a lot of memory. When it comes to performance evaluation, I
am not a big fan of extensive validation. Even for medium-sized datasets
like 50,000 or 100,000 rows. You can validate your models
with a simple train test split instead of full cross validation loop. Switch to full CV only
when it is really needed. For example,
when you've already hit some limits and can move forward only with
some marginal improvements. Same logic applies to
initial model choice. I usually start with LightGBM,
find some reasonably good parameters, and evaluate performance of my features. I want to emphasize that
I use early stopping, so I don't need to tune number
of boosting iterations. And god forbid start ESVMs,
random forks, or neural networks, you will waste too
much time just waiting for them to feed. I switch to tuning the models,
and sampling, and staking, only when I am satisfied
with feature engineering. In some ways, I describe my approach as,
fast and dirty, always better. Try focusing on what is really important,
the data. Do ED, try different features. Google domain-specific knowledge. Your code is secondary. Creating unnecessary classes and personal frame box may only make
things harder to change and will result in wasting your time, so
keep things simple and reasonable. Don't track every little change. By the end of competition, I usually
have only a couple of notebooks for model training and to want notebooks
specifically for EDA purposes. Finally, if you feel really uncomfortable
with given computational resources, don't struggle for weeks,
just rent a larger server. Every competition I start with
a very simple basic solution that can be even primitive. The main purpose of such solution
is not to build a good model but to debug full pipeline from very beginning of the data to the very end when we write
the submit file into decided format. I advise you to start with
construction of the initial pipeline. Often you can find it in baseline
solutions provided by organizers or in kernels. I encourage you to read carefully and
write your own. Also I advise you to follow from simple
to complex approach in other things. For example, I prefer to start
with Random Forest rather than Gradient Boosted Decision Trees. At least Random Forest
works quite fast and requires almost no tuning
of hybrid parameters. Participation in data science competition
implies the analysis of data and generation of features and
manipulations with models. This process is very similar in spirit
to the development of software and there are many good practices
that I advise you to follow. I will name just a few of them. First of all, use good variable names. No matter how ingenious you are,
if your code is written badly, you will surely get confused in it and
you'll have a problem sooner or later. Second, keep your research reproducible. FIx all random seeds. Write down exactly how
a feature was generated, and store the code under version
control system like git. Very often there are situation when you
need to go back to the model that you built two weeks ago and
edit to the ensemble width. The last and probably the most
important thing, reuse your code. It's really important to use the same
code at training and testing stages. For example, features should be prepared
and transforming by the same code in order to guarantee that they're
produced in a consistent manner. Here in such places are very
difficult to catch, so it's better to be very careful with it. I recommend to move reusable code into
separate functions or even separate model. In addition, I advise you to read
scientific articles on the topic of the competition. They can provide you with
information about machine and correlated things like for example how
to better optimize a measure, or AUC. Or, provide the main
knowledge of the problem. This is often very useful for
future generations. For example, during Microsoft Mobile
competition, I read article about mobile detection and used ideas from
them to generate new features. >> I usually start the competition by
monitoring the forums and kernels. It happens that a competition starts,
someone finds a bug in the data. And the competition data is
then completely changed, so I never join a competition
at its very beginning. I usually start a competition with
a quick EDA and a simple baseline. I tried to check the data for
various leakages. For me, the leaks are one of the most
interesting parts in the competition. I then usually do several submissions
to check if validation score correlates with publicly the board score. Usually, I try to come up with a list
of things to try in a competition, and I more or less try to follow it. But sometimes I just try to generate
as many features as possible, put them in extra boost and
study what helps and what does not. When tuning parameters,
I first try to make model overfit to the training set and only then I
change parameters to constrain the model. I had situations when I could not
reproduce one of my submissions. I accidentally changed something in
the code and I could not remember what exactly, so nowadays I'm very
careful about my code and script. Another problem? Long execution history in notebooks leads
to lots of defined global variables. And global variables surely lead to bugs. So remember to sometimes
restart your notebooks. It's okay to have ugly code, unless you
do not use this to produce a submission. It would be easier for you to get into this code later if
it has a descriptive variable names. I always use git and
try to make the code for submissions as transparent as possible. I usually create a separate notebook for every submission so I can always run
the previous solution and compare. And I treat the submission
notebooks as script. I restart the kernel and
always run them from top to bottom. I found a convenient way to validate
the models that allows to use validation code with minimal changes to retrain
a model on the whole dataset. In the competition, we are provided
with training and test CSV files. You see we load them in the first cell. In the second cell, we split
training set and actual training and validation sets, and
save those to disk as CSV files with the same structure as given train CSV and
test CSV. Now, at the top of the notebook,
with my model, I define variables. Path is to train and test sets. I set them to create a training and validation sets when working with
the model and validating it. And then it only takes me to switch
those paths to original train CSV and test CSV to produce a submission. I also use macros. At one point I was really tired of
typing import numpy as np, every time. So I found that it's possible to define
a macro which will load everything for me. In my case, it takes only five symbols to type the macro name and this macro
immediately loads me everything. Very convenient. And finally, I have developed my library
with frequently used functions, and training code for models. I personally find it useful, as the code,
it now becomes much shorter, and I do not need to remember how
to import a particular model. In my case I just specify
a model with its name, and as an output I get all the information
about training that I would possibly need. [SOUND] [MUSIC]Hello everyone. This is Marios. Today I would like to show you the Pipeline or like the approach I have used to tackle more than 100 machine learning competitions in cargo and obviously has helped me to do quite well. Before I start, let me state that I'm not claiming this is the best pipeline out there, is just the one I use. You might find some parts of it useful. So roughly, the Pipeline is, as you see it on the screen, here this is a summary and we will go through it in more detail later on. But briefly, I spend the first day in order to understand the problem and make the necessary preparations in order to deal with this. Then, maybe one, two days in order to understand a bit about the data, what are my features, what I have available, trying to understand other dynamics about the data, which will lead me to define a good cross validation strategy and we will see later why this is important. And then, once I have specified the cross validation strategy, I will spend all the days until 3-4 days before the end of the competition and I will keep iterating, doing different feature engineering and applying different machine bearing models. Now, something that I need to to highlight is that, when I start this process I do it myself, shut from the outside world. So, I close my ears, and I just focus on how I would tackle this problem. That's because I don't want to get affected by what the others are doing. Because I might be able to find something that others will not. I mean, I might take a completely different approach and this always leads me to gain, when I then combine with the rest of the people. For example, through merges or when I use other people's kernels. So, I think this is important, because it gives you the chance to create an intuitive approach about the data, and then also leverage the fact that other people have different approaches and you will get more diverse results. And in the last 3 to 4 days, I would start exploring different ways to combine all the models of field, in order to get the best results. Now, if people have seen me in competitions, you should know that you might have noticed that in the last 3-2 days I do a rapid jump in the little box and that's exactly because I leave assembling at the end. I normally don't do it. I have confidence that it will work and I spend more time in feature engineering and modeling, up until this point. So, let's take all these steps one by one. Initially I try to understand the problem. First of all, what type of problem it is. Is it image classification, so try to find what object is presented on an image. This is sound classification, like which type of bird appears in a sound file. Is it text classification? Like who has written the specific text, or what this text is about. Is it an optimization problem, like giving some constraints how can I get from point A to point B etc. Is it a tabular dataset, so that's like data which you can represent in Excel for example, with rows and columns, with various types of features, categorical or numerical. Is it time series problem? Is time important? All these questions are very very important and that's why I look at the dataset and I try to understand, because it defines in many ways what resources I would need, where do I need to look and what kind of hardware and software I will need. Also, I do this sort of preparation along with controlling the volume of the data. How much is it. Because again, this will define how I need to, what preparations I need to do in order to solve this problem. Once I understand what type of problem it is, then I need to reserve hardware to solve this. So, in many cases I can escape without using GPUs, so just a few CPUs would do the trick. But in problems like image classification of sound, then generally anywhere you would need to use deep learning. You definitely need to invest a lot in CPU, RAM and disk space. So, that's why this screening is important. It will make me understand what type of machine I will need in order to solve this and whether I have this processing power at this point in order to solve this. Once this has been specified and I know how many CPUs, GPUs, RAM and disk space I'm going to need, then I need to prepare the software. So, different software is suited for different types of problems. Keras and TensorFlow is obviously really good for when solving an image classification or sound classification and text problems that you can pretty much use it in any other problem as well. Then you most probably if you use Python, you need scikit learn and XGBoost, Lighgbm. This is the pinnacle of machine learning right now. And how do I set this up? Normally I create either an anaconda environment or a virtual environment in general, and how a different one for its competition, because it's easy to set this up. So, you just set this up, you download the necessary packages you need, and then you're good to go. This is a good way, a clean way to keep everything tidy and to really know what you used and what you find useful in the particular competitions. It's also a good validation for later on, when we will have to do this again, to find an environment that has worked well for this type of problem and possibly reuse it. Another question I ask at this point is what the metric I'm being tested on. Again, is it a regression program, is it a classification program, it is root mean square error, it is mean absolute error. I ask these questions because I try to find if there's any similar competition with similar type of data that I may have dealt with in the past, because this will make this preparation much much better, because I'll go backwards, find what I had used in the past and capitalize on it. So, reuse it, improve it, or even if I don't have something myself, I can just find other similar competitions or explanations of these type of problem from the web and try to see what people had used in order to integrate it to my approaches. So, this is what it means to understand the problem at this point. It's more about doing the screen search, this screening in order to understand what type of preparation I need to do, and actually do this preparation, in order to be able to solve this problem competitively, in terms of hardware, software and other resources, past resources in dealing with these types of problems. Then I spent the next one or two days to do some exploratory data analysis. The first thing that I do is I see all my features, assuming a tabular data set, in the training and the test data and to see how consistent they are. I tend to plot distributions and to try to find if there are any discrepancies. So is this variable in the training data set very different than the same variable in the task set? Because if there are discrepancies or differences, this is something I have to deal with. Maybe I need to remove these variables or scale them in a specific way. In any case, big discrepancies can cause problems to the model, so that's why I spend some time here and do some plotting in order to detect these differences. The other thing that I do is I tend to plot features versus the target variable and possibly versus time, if time is available. And again, this tells me to understand the effect of time, how important is time or date in this data set. And at the same time it helps me to understand which are like the most predictive inputs, the most predictive variables. This is important because it generally gives me intuition about the problem. How exactly this helps me is not always clear. Sometimes it may help me define a gross validation strategy or help me create some really good features but in general, this kind of knowledge really helps to understand the problem. I tend to create cross tabs for example with the categorical variables and the target variable and also creates unpredictability metrics like information value and you see chi square for example, in order to see what's useful and whether I can make hypothesis about the data, whether I understand the data and how they relate with the target variable. The more understanding I create at this point, most probably will lead to better features for better models applied on this data. Also while I do this, I like to bin numerical features into bands in order to understand if there nonlinear R.A.T's. When I say nonlinear R.A.T's, whether the value of a feature is low, target variable is high, then as the value increases the target variable decreases as well. So whether there are strange relationships trends, patterns, or correlations between features and the target variable, in order to see how best to handle this later on and get an intuition about which type of problems or which type of models would work better. Once I have understood the data, to some extent, then it's time for me to define a cross validation strategy. I think this is a really important step and there have been competitions where people were able to win just because they were able to find the best way to validate or to create a good cross validation strategy. And by cross validation strategy, I mean to create a validation approach that best resembles what you're being tested on. If you manage to create this internally then you can create many different models and create many different features and anything you do, you can have the confidence that is working or it's not working, if you've managed to build the cross validation strategy in a consistent way with what you're being tested on so consistency is the key word here. The first thing I ask is, "Is time important in this data?" So do I have a feature which is called date or time? If this is important then I need to switch to a time-based validation. Always have past data predicting future data, and even the intervals, they need to be similar with the test data. So if the test data is three months in the future, I need to build my training and validation to account for this time interval. So my validation data always need to be three months in the future and compared to the training data. You need to be consistent in order to have the most consistent results with what you are been tested on. The other thing that I ask is, "Are there different entities between the train and the test data?" Imagine if you have different customers in the training data and different in the test data. Ideally, you need to formulate your cross validation strategy so that in the validation data, you always have different customers running in training data otherwise you are not really testing in a fair way. Your validation method would not be consistent with the test data. Obviously, if you know a customer and you try to predict it, him or her, why you have that customer in your training data, this is a biased prediction when compared to the test data, that you don't have this information available. And this is the type of questions you need to ask yourself when you are at this point, "Am I making a validation which is really consistent with what am I being tested on?" The other thing that is often the case is that the training and the test data are completely random. I'm sorry, I just shortened my data and I took a random part, put it on training, the other for test so in that case, is any random type of cross validation could help for example, just do a random K-fold. There are cases where you may have to use a combination of all the above so you have strong temporal elements at the same time you have different entities, so different customers to predict for past and future and at the same time, there is a random element too. You might need to incorporate all of them do make a good strategy. What I do is I often start with a random validation and just see how it fares with the test leader board, and see how consistent the result is with what they have internally, and see if improvements in my validation lead to improvements to the leader board. If that doesn't happen, I make a deeper investigation and try to understand why. It may be that the time element is very strong and I need to take it into account or there are different entities between the train and test data. These kinds of questions in order to formulate a better validation strategy. Once the validation strategy has been defined, now I start creating many different features. I'm sorry for bombarding you with loads of information in one slide but I wanted this to be standalone. It says give you the different type of future engineering you can use in different types of problems, and also suggestions for the competition to look up which was quite representative of this time. But you can ignore these for now. Look at it later. The main point is different problem requires different feature engineering and I put everything when I say feature engineering. I put the day data cleaning and preparation as well, how you handle missing values, and the features you generate out of this. The thing is, every problem has its own corpus of different techniques you use to derive or create new features. It's not easy to know everything because sometimes it's too much, I don't remember it myself so what I tend to do is go back to similar competitions and see what people are using or what people have used in the past and I incorporate into my code. If I have dealt with this or a similar problem in the past then I look at my code to see what I had done in the past, but still looking for ways to improve this. I think that's the best way to be able to handle any problem. The good thing is that a lot of the feature engineering can be automated. You probably have already seen that but, as long as your cross validation strategy is consistent with the test data and reliable, then you can potentially try all sorts of transformations and see how they work in your validation environment. If they work well, you can be confident that this type of feature engineering is useful and use it for further modeling. If not, you discard and try something else. Also the combinations of what you can do in terms of feature engineering can be quite vast in different types of problems so obviously time is a factor here, and scalability too. You need to be able to use your resources well in order to be able to search as much as you can in order to get the best outcome. This is what I do. Normally if I have more time to do this feature engineering in a competition, I tend to do better because I explore more things.And the modeling is pretty much the same story. So, it's type problem has its own type of model that works best. Now, I don't want to go through that list again, I put it here so that you can use it for reference. But, again, the way you work this out is you look for literature, you sense other previous competitions that were similar and you try to find which type of problem, which type of model or best for its type of problem. And it's not surprise that for typical dataset, when I say typical dataset I mean, tabular dataset rather boosting machines in the form of [inaudible] turned to rock fest for problems like aim as classification sound classification, deep learning in the form of convolutional neural networks tend to work better. So, this is roughly what you need to know. New techniques are being developed so, I think your best chance here or what I have used in order to do well in the past was knowing what's tends to work well with its problem, and going backwards and trying to find other code or other implementations and similar problems in order to integrate it with mine and try to get a better result. I should mention that each of the previous models needs to be changed sometimes differently. So you need to spend time within this cross-validation strategy in order to find the best parameters, and then we move onto Ensembling. Every time you apply your cross-validation procedure with a different feature engineering and a different joint model, it's time, you saved two types of predictions, you save predictions for the validation data and you save predictions for the test data. So now that you have saved all these predictions and by the way this is the point that if you collaborate with others that tend to send you the predictions, and you'll be surprised that sometime that collaboration is just this. So people just sending these prediction files for the validation and the test data. So now you can find the best way to combine these models in order to get the best results. And since you already have predictions for the validation data, you know the target variable for the validation data, so you can explore different ways to combine them. The methods could be simple, could be an average, or already average, or it can go up to a multilayer stacking in general. Generally, what you need to know is that from my experience, smaller data requires simple ensemble techniques like averaging. And also what tends to show is to look at correlation between predictions. So find it here that work well, but they tend to be quite diverse. So, when you use fusion correlation, the correlation is not very high. That means they are likely to bring new information, and so when you combine you get the most out of it. But if you have bigger data there are, you got pretty must try all sorts of things. What I like to think of is it is that, when you have really big data, the stacking process that impedes the modeling process. By that, I mean that you have a new set of features this time they are predictions of models, but you can apply the same process you have used before. So you can do feature engineering, you can create new features or you can remove the features/ prediction that you no longer need and you can use this in order to improve the results for your validation data. This process can be quite exhaustive, but well, again, it can be automated to some extent. So, the more time you have here, most probably the better you will do. But from my experience, 2, 3 days is good in order to get the best out of all the models you have built and depends obviously on the volume of data or volume of predictions you have generated up until this point. At this point I would like to share a few thoughts about collaboration. Many people have asked me this and I think this is a good point to share. These ideas has greatly helped me to do well in competitions. The first thing is that it makes things more fun. I mean you are not alone, you're with other people and that's always more energizing, it's always more interesting, it's more fun, you can communicate with the others through times like Skype, and yeah I think it's more collaborative as the world says, it is better. You learn more. I mean you can be really good, but, you know, you always always learn from others. No way to know everything yourself. So it's really good to be able to share points with other people, see what they do learn from them and become better and grow as a data scientist, as a model. From my experience you score far better than trying to solve a problem alone, and I think these happens for mainly for two ways. There are more but these are main two. First you can cover more ground because, you can say, you can focus on ensembling, I will focus on feature engineering or you will focus on joining this type of model and I will focus on another type of model. So, you can generally cover more ground. You can divide task and you can search, you can cover more ground in terms of the possible things you can try in a competition. The second thing is that every person sees the problem from different angles. So, that's very likely to generate more diverse predictions. So something we do is although we kind of define together by the different strategy when we form teams, then we would like to work for maybe one week separately without discussing with one another, because this helps to create diversity. Otherwise, if we over discuss this, we might generate pretty much the same things. So, in other words, our solutions might be too correlated to add more value. So, this is a good way in order to leverage the different mindset each person has in solving these problems. So, for one week, each one works separately and then after some point, we start combining or work more closely. I would advise people to start collaborating after getting some experience, and I say here two or three competitions just because Cargo has some rules. Sometimes, it is easy to make mistakes. I think it's better to understand the environment, the competition environment well before exploring these options in order to make certain that, no mistakes are done, no violation of the rules. Sometimes new people tend to make these mistakes. So, it's good to have this experience prior to trying to collaborating. I advise people to start forming teams with people around their rank because sometimes it is frustrating when you join a high rank or a very experienced team I would say. It's bad to say experience from rank, because you don't know sometimes how to contribute, you still don't understand all the competition dynamics and it might stall your progress, if you join a team and you're not able to contribute. So, I think it's better to, in most cases, to try and find people around your rank or around your experience and grow together. This way is the best form of collaboration I think. Another tip for collaborating is to try to collaborate with people that are likely to take diverse approaches or different approaches than yourself. You learn more this way and it is more likely that when you combine, you will get a better score. So, such for people who are sort of famous for doing well certain things and in order to get the most out of it, to learn more from each other and get better results in the leader board. About selecting submissions, I have employed a strategy that many people have done. So normally, I select the best submissions I see in my internal result and the one that work best on the leader board. At the same time, I also look for correlations. So, if two submissions, they tend to be the same pretty much. So, the one that was the best submission locally, was also the best on leader boards, I try to find other submissions that still work well but they are likely to be quite diverse. So, they have low correlations with my best submission because this way, I might capture, I might be lucky, it maybe be a special type of test data set and just by having a diverse submission, I might be lucky to get a good score. So that's the main idea about this. Some tips I would like to share now in general about competitive modeling, on land modeling and in Cargo specifically. In these challenges, you never lose. [inaudible] lose, yes you may not win prize money. Out of 5000 people, sometimes it's difficult to be, almost to impossible to be in the top three or four that gives prizes but you always gain in terms of knowledge, in terms of experience. You get to collaborate with other people which are talented in the field, you get to add it to your CV that you try to solve this particular problem, and I can tell you there has been some criticists here, people doubt that doing these competitions stops your employ-ability but I can tell you that i know many examples and not want us, they really thought the Ocean Cargo like Master and Grand-master that just by having kind of experience, they have been able to find very decent jobs and even if they had completely diverse backgrounds to the science. So, I can tell you it matters. So, any time you spend here, it's definitely a win for you. I don't see how you can lose by competing in these challenges. You mean if this is something you like right. The whole predictive modeling that the science think. Coffee tempts to shop, because you tend to spend longer hours. I tend to do this especially late at night. So it definitely tells me something to consider or to be honest any other beverage will do: depends what you like. I see it a bit like a game and I advise you to do the same because if you see it like a game, you never need to work for it. If you know what I mean. So it looks a bit like NRPT. In some way, you have some tools or weapons. These are all the algorithms and feature engineering techniques you can use. And then you have this core leader board and you try to beat all the bad guys and to beat the score and rise above them. So in a way does look like a game. You know you try to use all the tools, all the skills that you have to try to beat the score. So, I think if you see it like a game it really helps you. You don't get tired and you enjoy the process more. I do advise you to take a break though, from my experience you may spend long hours hitting on it and that's not good for your body. You definitely need to take some breaks and do some physical exercise. Go out for a walk. I think it can help most of the times by resting your mind this way can actually help to do better. You have more rested heart, more clear thinking. So, I definitely advise you to do this, generally don't overdo it. I have overnighted in the past but i advise you not to do the same. And now there is a thing that I would like to highlight is that the Cargo community is great. Is one of the most open and helpful helpful communities have experience in any social context, maybe apart from Charities but if you have a question and you posted on the forums or other associated channels like in Slug and people are always willing to help you.That's great, because there are so many people out there and most probably they know the answer or they can help you for a particular problem. And this is invaluable. So many times i have really made use of this, of this option and it really helps. You know this kind of mentality was there even before the serine was gamified. When I say gamified, now you get points by sharping in a way by sharing code or participating in discussions. But in the past, people were doing without really getting something out of it. It maybe the open source mentality of data science that the fact that many people participating are researchers. I don't know but it really is a field that sharing seems to be really important in helping others. So, I do advise you to consider this and don't be afraid to ask in these forums. Another thing that I do at shops, is that after the competition has ended irrespective of how well or not you've done, is go and look for other people and what they have done. Normally, there are threads where people share their approaches, sometimes they share the whole approach would go to sometimes it just give tips and you know this is where you can upgrade your tools and you can see what other people have done and make improvements. And in tandem with this, you should have a notebook of useful methods that you keep updating it at the end of every competition. So, you found an approach that was good, you just add it to that notebook and next time you encounter the same or similar competition you get that notebook out and you apply the same techniques at work in the past and this is how you get better. Actually, if i now start a competition without that notebook, i think it will take me three or four times more in order to get to the same score because a lot of the things that I do now depend on stuff that i have done in the past. So, it's definitely helpful, consider creating this notebook or library of all the approaches or approaches that have worked in the past in order to have an easier time going on. And that was what I wanted to share with you and thank you very much for bearing with me and to see you next time, right.