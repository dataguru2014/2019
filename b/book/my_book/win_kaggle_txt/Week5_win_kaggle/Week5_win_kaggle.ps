%!PS-Adobe-3.0
%%Title: Week5_win_kaggle.txt
%%For: wb
%%Creator: VIM - Vi IMproved 8.0 (2016 Sep 12)
%%CreationDate: Mon Dec 23 18:54:10 2019
%%DocumentData: Clean8Bit
%%Orientation: Portrait
%%Pages: (atend)
%%PageOrder: Ascend
%%BoundingBox: 59 45 559 800
%%DocumentMedia: A4 595 842 0 () ()
%%DocumentNeededResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%DocumentSuppliedResources: procset VIM-Prolog 1.4 1
%%+ encoding VIM-latin1 1.0 0
%%Requirements: duplex collate
%%EndComments
%%BeginDefaults
%%PageResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%PageMedia: A4
%%EndDefaults
%%BeginProlog
%%BeginResource: procset VIM-Prolog
%%BeginDocument: /usr/share/vim/vim80/print/prolog.ps
%!PS-Adobe-3.0 Resource-ProcSet
%%Title: VIM-Prolog
%%Version: 1.4 1
%%EndComments
% Editing of this file is NOT RECOMMENDED.  You run a very good risk of causing
% all PostScript printing from VIM failing if you do.  PostScript is not called
% a write-only language for nothing!
/packedarray where not{userdict begin/setpacking/pop load def/currentpacking
false def end}{pop}ifelse/CP currentpacking def true setpacking
/bd{bind def}bind def/ld{load def}bd/ed{exch def}bd/d/def ld
/db{dict begin}bd/cde{currentdict end}bd
/T true d/F false d
/SO null d/sv{/SO save d}bd/re{SO restore}bd
/L2 systemdict/languagelevel 2 copy known{get exec}{pop pop 1}ifelse 2 ge d
/m/moveto ld/s/show ld /ms{m s}bd /g/setgray ld/r/setrgbcolor ld/sp{showpage}bd
/gs/gsave ld/gr/grestore ld/cp/currentpoint ld
/ul{gs UW setlinewidth cp UO add 2 copy newpath m 3 1 roll add exch lineto
stroke gr}bd
/bg{gs r cp BO add 4 -2 roll rectfill gr}bd
/sl{90 rotate 0 exch translate}bd
L2{
/sspd{mark exch{setpagedevice}stopped cleartomark}bd
/nc{1 db/NumCopies ed cde sspd}bd
/sps{3 db/Orientation ed[3 1 roll]/PageSize ed/ImagingBBox null d cde sspd}bd
/dt{2 db/Tumble ed/Duplex ed cde sspd}bd
/c{1 db/Collate ed cde sspd}bd
}{
/nc{/#copies ed}bd
/sps{statusdict/setpage get exec}bd
/dt{statusdict/settumble 2 copy known{get exec}{pop pop pop}ifelse
statusdict/setduplexmode 2 copy known{get exec}{pop pop pop}ifelse}bd
/c{pop}bd
}ifelse
/ffs{findfont exch scalefont d}bd/sf{setfont}bd
/ref{1 db findfont dup maxlength dict/NFD ed{exch dup/FID ne{exch NFD 3 1 roll
put}{pop pop}ifelse}forall/Encoding findresource dup length 256 eq{NFD/Encoding
3 -1 roll put}{pop}ifelse NFD dup/FontType get 3 ne{/CharStrings}{/CharProcs}
ifelse 2 copy known{2 copy get dup maxlength dict copy[/questiondown/space]{2
copy known{2 copy get 2 index/.notdef 3 -1 roll put pop exit}if pop}forall put
}{pop pop}ifelse dup NFD/FontName 3 -1 roll put NFD definefont pop end}bd
CP setpacking
(\004)cvn{}bd
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%BeginResource: encoding VIM-latin1
%%BeginDocument: /usr/share/vim/vim80/print/latin1.ps
%!PS-Adobe-3.0 Resource-Encoding
%%Title: VIM-latin1
%%Version: 1.0 0
%%EndComments
/VIM-latin1[
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclam /quotedbl /numbersign /dollar /percent /ampersand /quotesingle
/parenleft /parenright /asterisk /plus /comma /minus /period /slash
/zero /one /two /three /four /five /six /seven
/eight /nine /colon /semicolon /less /equal /greater /question
/at /A /B /C /D /E /F /G
/H /I /J /K /L /M /N /O
/P /Q /R /S /T /U /V /W
/X /Y /Z /bracketleft /backslash /bracketright /asciicircum /underscore
/grave /a /b /c /d /e /f /g
/h /i /j /k /l /m /n /o
/p /q /r /s /t /u /v /w
/x /y /z /braceleft /bar /braceright /asciitilde /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclamdown /cent /sterling /currency /yen /brokenbar /section
/dieresis /copyright /ordfeminine /guillemotleft /logicalnot /hyphen /registered /macron
/degree /plusminus /twosuperior /threesuperior /acute /mu /paragraph /periodcentered
/cedilla /onesuperior /ordmasculine /guillemotright /onequarter /onehalf /threequarters /questiondown
/Agrave /Aacute /Acircumflex /Atilde /Adieresis /Aring /AE /Ccedilla
/Egrave /Eacute /Ecircumflex /Edieresis /Igrave /Iacute /Icircumflex /Idieresis
/Eth /Ntilde /Ograve /Oacute /Ocircumflex /Otilde /Odieresis /multiply
/Oslash /Ugrave /Uacute /Ucircumflex /Udieresis /Yacute /Thorn /germandbls
/agrave /aacute /acircumflex /atilde /adieresis /aring /ae /ccedilla
/egrave /eacute /ecircumflex /edieresis /igrave /iacute /icircumflex /idieresis
/eth /ntilde /ograve /oacute /ocircumflex /otilde /odieresis /divide
/oslash /ugrave /uacute /ucircumflex /udieresis /yacute /thorn /ydieresis]
/Encoding defineresource pop
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%EndProlog
%%BeginSetup
595 842 0 sps
1 nc
T F dt
T c
%%IncludeResource: font Courier
/_F0 /VIM-latin1 /Courier ref
/F0 13 /_F0 ffs
%%IncludeResource: font Courier-Bold
/_F1 /VIM-latin1 /Courier-Bold ref
/F1 13 /_F1 ffs
%%IncludeResource: font Courier-Oblique
/_F2 /VIM-latin1 /Courier-Oblique ref
/F2 13 /_F2 ffs
%%IncludeResource: font Courier-BoldOblique
/_F3 /VIM-latin1 /Courier-BoldOblique ref
/F3 13 /_F3 ffs
/UO -1.3 d
/UW 0.65 d
/BO -3.25 d
%%EndSetup
%%Page: 1 1
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                      Page 1)59.5 790.15 ms
F0 sf
([MUSIC] Hello everyone. In this video we will analyze)59.5 764.15 ms
(the Crowdflower competition. We, I mean me, Stanislav Semenov an)59.5 751.15 ms
(d)59.5 738.15 ms
(Dmitry Altukhov participated as a team and took second place. I )59.5 725.15 ms
(will explain most important parts of)59.5 712.15 ms
(our solution along with some details. The outline of the video i)59.5 699.15 ms
(s as follows. First, I will tell you about the contest)59.5 686.15 ms
(itself, what kind of data and metrics were provided. After that )59.5 673.15 ms
(we will discuss are approach,)59.5 660.15 ms
(features and tricks. And then I will summarize what is worth)59.5 647.15 ms
(to kind of look for on the competition. Some of the competition )59.5 634.15 ms
(were)59.5 621.15 ms
(organized by CrowdFlower Company. The goal of this competition i)59.5 608.15 ms
(s to measure)59.5 595.15 ms
(relevance of the search results given the queries and resulting )59.5 582.15 ms
(product)59.5 569.15 ms
(descriptions from living e-commerce sites. The task is to evalua)59.5 556.15 ms
(te the accuracy)59.5 543.15 ms
(of their search algorithms. The challenge of the competition is)59.5 530.15 ms
(to predict the relevance score of a given query and problem desc)59.5 517.15 ms
(ription. On the picture we see assessor's)59.5 504.15 ms
(user interface, which has a query, a search query, and)59.5 491.15 ms
(some information about an item. Assessors were asked to give eac)59.5 478.15 ms
(h)59.5 465.15 ms
(query-product pairing a score of 1, 2, 3, or 4, with 4 indicatin)59.5 452.15 ms
(g the item)59.5 439.15 ms
(completely satisfied the search query, and 1 indicating the item)59.5 426.15 ms
( doesn't match. As the training data, we have only)59.5 413.15 ms
(median and variance of these scores. Data set consists of three )59.5 400.15 ms
(text fields,)59.5 387.15 ms
(request query, result and products title,)59.5 374.15 ms
(and product description, and two columns related to the target,)59.5 361.15 ms
(median and variance of scores. To ensure that the algorithm is r)59.5 348.15 ms
(obust)59.5 335.15 ms
(enough to handle any HTML snippets in the real world, the data p)59.5 322.15 ms
(rovided in)59.5 309.15 ms
(the program description field is raw and sometimes contain permi)59.5 296.15 ms
(ssions)59.5 283.15 ms
(that is relevant to the product. For example,)59.5 270.15 ms
(a string of text or object IDs. To discourage hand-labeling, the)59.5 257.15 ms
( actual)59.5 244.15 ms
(set was augmented with extra data. This additional data was)59.5 231.15 ms
(ignored to when the public and private leaderboard)59.5 218.15 ms
(scores were calculated. And of course, campaign scores have)59.5 205.15 ms
(no idea which objects we had already used to calculate the score)59.5 192.15 ms
(s. So we have 10,000 samples in train, and)59.5 179.15 ms
(20,000 in test, but it's good data. I mean, validation works wel)59.5 166.15 ms
(l and local scores are close enough)59.5 153.15 ms
(to leaderboard scores. Effective solutions, non-standard metric)59.5 140.15 ms
(was used, quadratic weighted kappa. Let's take a closer look at )59.5 127.15 ms
(it. You can find a detailed description of the)59.5 114.15 ms
(metric on the competition evaluation page. We have already discu)59.5 101.15 ms
(ssed the metric)59.5 88.15 ms
(in our course, but let me recall it. Submissions are scored base)59.5 75.15 ms
(d on the)59.5 62.15 ms
(quadratic weighted kappa which measures the agreement between tw)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 2 2
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                      Page 2)59.5 790.15 ms
F0 sf
(o ratings. This metric typically will rise from 0,)59.5 764.15 ms
(random agreement between raters, to 1, complete agreement betwee)59.5 751.15 ms
(n raters. In case there is less agreement between)59.5 738.15 ms
(the raters than expected by random, the metric may go below 0. I)59.5 725.15 ms
(n order to understand the metric, let's consider an example)59.5 712.15 ms
(of how to calculate it. First, we need N by n confusion matrix C)59.5 699.15 ms
(,)59.5 686.15 ms
(which constructed and normalized. Our vertical axis by its failu)59.5 673.15 ms
(res,)59.5 660.15 ms
(or horizontal predicted failures. In our case, N is equal to 4 a)59.5 647.15 ms
(s)59.5 634.15 ms
(the number of possible ratings. Second we need N by n histogram)59.5 621.15 ms
(matrix of expected matrix E, which is calculated assuming that t)59.5 608.15 ms
(here)59.5 595.15 ms
(is no correlation between ratings cost. This is calculated as wi)59.5 582.15 ms
(thin)59.5 569.15 ms
(histogram vectors of ratings. Also we need N by N matrix of weig)59.5 556.15 ms
(hts, W, which is calculated based)59.5 543.15 ms
(on its elements positions. This particular formula for weights u)59.5 530.15 ms
(ses)59.5 517.15 ms
(square distance between indexes i and j, so this is why the meth)59.5 504.15 ms
(od is)59.5 491.15 ms
(called quadratic weighted kappa. Finally, kappa can be calculate)59.5 478.15 ms
(d as one)59.5 465.15 ms
(minus a fraction of this weighted sum of confusion matrix in the)59.5 452.15 ms
( numerator, and weighted sum of expectation)59.5 439.15 ms
(matrix in the denominator. I want to notice that kappa has prope)59.5 426.15 ms
(rties)59.5 413.15 ms
(similar both to classification loss and regression loss. The mor)59.5 400.15 ms
(e distant predicted and true ratings are,)59.5 387.15 ms
(the more penalties there will be. Remember that thing,)59.5 374.15 ms
(we will use it later in our video. Okay, let's now talk)59.5 361.15 ms
(about my team solution. It's turned out to be quite complex,)59.5 348.15 ms
(consisting of several parts. Like extending of queries, per-quer)59.5 335.15 ms
(y)59.5 322.15 ms
(models, bumper features, and so on. I will tell you about main p)59.5 309.15 ms
(oints. So let's start with text features. We have three text fie)59.5 296.15 ms
(lds, a query,)59.5 283.15 ms
(a title, and a description. You can apply all techniques that)59.5 270.15 ms
(we discuss in our course and calculate various measures of simil)59.5 257.15 ms
(arity. That's exactly what we did. That is for query title and q)59.5 244.15 ms
(uery)59.5 231.15 ms
(description pair, we calculated the number of magic words, cosin)59.5 218.15 ms
(e distance)59.5 205.15 ms
(between TF-IDF representations, distance between the average wor)59.5 192.15 ms
(d2vec)59.5 179.15 ms
(vectors, and Levensthein distance. In fact, this is a standard s)59.5 166.15 ms
(et of)59.5 153.15 ms
(features that should be used for similar task. And there is noth)59.5 140.15 ms
(ing outstanding. The support should be)59.5 127.15 ms
(considered as a baseline. And now we turn to the interesting thi)59.5 114.15 ms
(ngs. In addition, we found it was)59.5 101.15 ms
(useful to use symbolic n-grams. You can work with them in)59.5 88.15 ms
(the same way as with word-based, if each letter is interpreted)59.5 75.15 ms
(as a separate word. We use symbolic n-grams from one letter,)59.5 62.15 ms
(to five letters. After getting a large parse metrics of)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 3 3
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                      Page 3)59.5 790.15 ms
F0 sf
(n-grams, we apply it as to them, and took only close to 300)59.5 764.15 ms
(combinations as features. You remember we discussed this portion)59.5 751.15 ms
(of our course, there is an example. Looking at the data we notic)59.5 738.15 ms
(e)59.5 725.15 ms
(three interesting properties. Queries are very short,)59.5 712.15 ms
(numbers of unique queries is 261, and queries are the same in tr)59.5 699.15 ms
(ain and)59.5 686.15 ms
(test sets. We decided to use these observations)59.5 673.15 ms
(to build extended versions of query as follows. For each query, )59.5 660.15 ms
(we get the most)59.5 647.15 ms
(relevant corresponding items, those with relevance equal to four)59.5 634.15 ms
(. We join all words from the title)59.5 621.15 ms
(of the relevant items and take ten most popular words. This is w)59.5 608.15 ms
(hat we called extended query and it's used to build all these te)59.5 595.15 ms
(xt)59.5 582.15 ms
(features that I mentioned earlier. Notice that this trick is app)59.5 569.15 ms
(licable)59.5 556.15 ms
(only within the context framework. Due to the fact that queries )59.5 543.15 ms
(in test)59.5 530.15 ms
(are exactly the same as in train. In real life we couldn't do so)59.5 517.15 ms
( because it's unrealistic to ignore)59.5 504.15 ms
(relevant results for every search query. The fact that sets of q)59.5 491.15 ms
(ueries in train and)59.5 478.15 ms
(test are the same, give us an opportunity to split our)59.5 465.15 ms
(problem into many small subtasks. Specifically, for each query,)59.5 452.15 ms
(you can build a separate model that will only predict relevance)59.5 439.15 ms
(of corresponding items. Again, in real life, such tricks can't b)59.5 426.15 ms
(e)59.5 413.15 ms
(applied, but within the context framework, it's totally fine. So)59.5 400.15 ms
(, for each unique query,)59.5 387.15 ms
(we get corresponding samples, calculate work to work similaritie)59.5 374.15 ms
(s,)59.5 361.15 ms
(back and forth presentation, and fit a random fourth classifier.)59.5 348.15 ms
( Finally, we have 261 model)59.5 335.15 ms
(which predictions were used as a feature in final example. Do re)59.5 322.15 ms
(member that every product)59.5 309.15 ms
(item is by several people. Median in a variance of the ratings. )59.5 296.15 ms
(The variance show in ratings)59.5 283.15 ms
(are inconsistent or not. Intuitively, if the variance is large, )59.5 270.15 ms
(then such an object need to be taken)59.5 257.15 ms
(into account with a smaller weight. One way to do it is to assig)59.5 244.15 ms
(n items)59.5 231.15 ms
(weight depending on query answers, which was a heuristics 1 /)59.5 218.15 ms
(1 + variance as the weight. Another method is to restore)59.5 205.15 ms
(individual observation using median and variance statistics. We )59.5 192.15 ms
(found that supposing)59.5 179.15 ms
(there are three assessors, we can almost always certainly)59.5 166.15 ms
(restore our original labels. For example,)59.5 153.15 ms
(if we have a median equal to three, and variance equal to 0.66,)59.5 140.15 ms
(there of course are two, three, and four, which by this approach)59.5 127.15 ms
( and for)59.5 114.15 ms
(each source sample, generated three once. However, using them as)59.5 101.15 ms
( training)59.5 88.15 ms
(data took longer to train, and did not improve the score. And si)59.5 75.15 ms
(mple heuristic works quite well,)59.5 62.15 ms
(and they use it in final solution. In the competition, you need )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 4 4
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                      Page 4)59.5 790.15 ms
F0 sf
(to choose)59.5 764.15 ms
(a metric, we need to predict class, but penalty for)59.5 751.15 ms
(the error is not symmetric. We decided to take it into account, )59.5 738.15 ms
(adding several artificially created binary)59.5 725.15 ms
(delimiters between classes as features. In other words, we're tr)59.5 712.15 ms
(ying to)59.5 699.15 ms
(classify to answer the question, is it true that target class nu)59.5 686.15 ms
(mber is)59.5 673.15 ms
(greater than 1, greater than 2, and so on. We call these feature)59.5 660.15 ms
(s bumpers, since they)59.5 647.15 ms
(are kind of separators between classes. We build them in fashion)59.5 634.15 ms
(, similar to)59.5 621.15 ms
(how we construct predictions instead. It was very useful for the)59.5 608.15 ms
( final solution. All mentioned features will)59.5 595.15 ms
(be used in an ensemble. We build several models on)59.5 582.15 ms
(different subsets of features. Some feel relatively simple like )59.5 569.15 ms
(SVM and)59.5 556.15 ms
(some look quite complex. You can see the part of)59.5 543.15 ms
(complex model created by me. It uses bumper features,)59.5 530.15 ms
(all sorts of similarities and query features in different combin)59.5 517.15 ms
(ations. Also there is a lot of frostage models, which are mixed )59.5 504.15 ms
(up with)59.5 491.15 ms
(second stage random forest. In fact, each participant of the tea)59.5 478.15 ms
(m)59.5 465.15 ms
(made his own model, and then for competition we simply mixed our)59.5 452.15 ms
(model linearly for final submission. Let's remember that the met)59.5 439.15 ms
(ric on the one)59.5 426.15 ms
(hand has some proper classification It's necessary to predict th)59.5 413.15 ms
(e class. But for regression answer,)59.5 400.15 ms
(we can analyze more. We have built models for regression, but we)59.5 387.15 ms
( have had to somehow turn)59.5 374.15 ms
(real-valued predictions into classes. A simple approach with wou)59.5 361.15 ms
(ld work poorly,)59.5 348.15 ms
(so we decided to pick up thresholds. For the purpose, we were ri)59.5 335.15 ms
(ght. Thresholds and choose the best one)59.5 322.15 ms
(weighted on cross-validation score. The buff procedure gave us a)59.5 309.15 ms
( very)59.5 296.15 ms
(significant increase in quality. in fact it often happens in com)59.5 283.15 ms
(petitions)59.5 270.15 ms
(with non-standard metrics that [INAUDIBLE] grades symmetric opti)59.5 257.15 ms
(mization)59.5 244.15 ms
(gives a significant improvement. So let's sum up. In the competi)59.5 231.15 ms
(tion it was really)59.5 218.15 ms
(important to use the [INAUDIBLE] ideas. First symbolic and grams)59.5 205.15 ms
(, once since)59.5 192.15 ms
(they give a significant increase in the score and it was not sol)59.5 179.15 ms
(ved)59.5 166.15 ms
(that you should use that. Second, expansion of queries led to)59.5 153.15 ms
(significant increase in this course. Also optimization of thresh)59.5 140.15 ms
(olds was)59.5 127.15 ms
(a crucial part of our solution. I hope you will re-use some of t)59.5 114.15 ms
(hese)59.5 101.15 ms
(tricks in your competitions, though. Thank you for the attention)59.5 88.15 ms
(. [MUSIC][MUSIC] Hi, throughout the course, we use)59.5 75.15 ms
(the Springleaf competition as a useful example of EDA, mean enco)59.5 62.15 ms
(dings and)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 5 5
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                      Page 5)59.5 790.15 ms
F0 sf
(features based on nearest neighbors. Back then, we took the thir)59.5 764.15 ms
(d place in)59.5 751.15 ms
(this competition together with and. And now in this video, I wil)59.5 738.15 ms
(l describe)59.5 725.15 ms
(the last part of our solution, which is the usage of stacking an)59.5 712.15 ms
(d)59.5 699.15 ms
(ensembles. On this picture, you can see the final)59.5 686.15 ms
(stacking scheme we produced on the level 0 features, on the firs)59.5 673.15 ms
(t level,)59.5 660.15 ms
(predictions by basic models. On the level one plus combination. )59.5 647.15 ms
(So these predictions and)59.5 634.15 ms
(some accurately chosen features on the second level models)59.5 621.15 ms
(on this new set of features. And finally, on the third level,)59.5 608.15 ms
(their linear combination. In this video, we will go through each)59.5 595.15 ms
( level as it builds)59.5 582.15 ms
(up to this non-trivial ensembled scheme. But first, let's quickl)59.5 569.15 ms
(y remind)59.5 556.15 ms
(ourselves about the problem. This was a binary classification)59.5 543.15 ms
(task with area under curve metric. We had 145,000 samples in tra)59.5 530.15 ms
(ining data)59.5 517.15 ms
(and about 2,000 anonymized features. These were useful insights)59.5 504.15 ms
(derived by us while doing EDA. And you can check out EDA done by)59.5 491.15 ms
( earlier)59.5 478.15 ms
(in our course to refresh your memory. So now let's start with fe)59.5 465.15 ms
(atures. Here we have four packs of features. First two are the b)59.5 452.15 ms
(asic dataset and)59.5 439.15 ms
(the processed dataset. To keep it simple,)59.5 426.15 ms
(we just used insights derived from EDA to clean data [INAUDIBLE])59.5 413.15 ms
( and)59.5 400.15 ms
(to generate new features. For example,)59.5 387.15 ms
(we remove duplicated features and edit some feature interaction )59.5 374.15 ms
(based)59.5 361.15 ms
(on scatter plots and correlations. Then, we mean-encoded all cat)59.5 348.15 ms
(egorical)59.5 335.15 ms
(features using growth relation loop and sign data and smoothing.)59.5 322.15 ms
( We further used the mean-encoded dataset)59.5 309.15 ms
(to calculate features based on nearest neighbors. Like, what is )59.5 296.15 ms
(the least in)59.5 283.15 ms
(closest object of the class zero? And how many objects out of te)59.5 270.15 ms
(n)59.5 257.15 ms
(nearest neighbors belong to class one? You can review how this c)59.5 244.15 ms
(ould be done in related topics introduced)59.5 231.15 ms
(by Dmitri Altihof. So finally, these four packs of)59.5 218.15 ms
(feature were level 0 of our solution. And the second level was r)59.5 205.15 ms
(epresented)59.5 192.15 ms
(by several different gradient within decision tree models,)59.5 179.15 ms
(and one neural network. The main idea here is that meta)59.5 166.15 ms
(features should be diverse. Each meta feature should bring)59.5 153.15 ms
(new information about the target. So we use both distinct parame)59.5 140.15 ms
(ters and)59.5 127.15 ms
(different sets of features for our models. For the neural networ)59.5 114.15 ms
(k, we additionally)59.5 101.15 ms
(pre-processed features with common scalars, ranks and)59.5 88.15 ms
(power transformation. The problem there was in huge outliers)59.5 75.15 ms
(which skew network training results. So ranks and power transfor)59.5 62.15 ms
(mation)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 6 6
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                      Page 6)59.5 790.15 ms
F0 sf
(helped to handle this problem. After producing meta features who)59.5 764.15 ms
( is)59.5 751.15 ms
(gradual in boosting decision to it and neural networks, we calcu)59.5 738.15 ms
(lated pay rise differences)59.5 725.15 ms
(on them to help next level models. Note that this is also an int)59.5 712.15 ms
(eresting)59.5 699.15 ms
(trick to force the model to utilize the differences in)59.5 686.15 ms
(the first level models predictions. Here we edit two datasets of)59.5 673.15 ms
(features based on nearest neighbors. One was taken directly from)59.5 660.15 ms
( level 0 and)59.5 647.15 ms
(they contain the same features. But it was calculated on the mea)59.5 634.15 ms
(n-encoded)59.5 621.15 ms
(dataset to the power of one-half. The point here was that these )59.5 608.15 ms
(features)59.5 595.15 ms
(were not completely utilized by the first level models. And inde)59.5 582.15 ms
(ed, they brought new pieces)59.5 569.15 ms
(of information to this level. Now we already have autofold)59.5 556.15 ms
(predictions from the first level and we will train with the mode)59.5 543.15 ms
(ls on them. Because we could have target leakage)59.5 530.15 ms
(here because of other folk, and also because features not very g)59.5 517.15 ms
(ood and there are almost no patterns left)59.5 504.15 ms
(in the data for models to discover. We chose simple classifiers,)59.5 491.15 ms
( keeping in)59.5 478.15 ms
(mind that predictions should be diverse. We used four different )59.5 465.15 ms
(models. Gradient boosted decision tree,)59.5 452.15 ms
(neural networks, random forest and logistic regression. So this )59.5 439.15 ms
(is all with)59.5 426.15 ms
(the second level models. And finally, we took a linear in your)59.5 413.15 ms
(combination of the second level models. Because a linear model i)59.5 400.15 ms
(s not inclined)59.5 387.15 ms
(to that we estimated coefficients directly using these four pred)59.5 374.15 ms
(ictions and)59.5 361.15 ms
(our target for throwing in data. So, this is it. We just went th)59.5 348.15 ms
(rough each level of this)59.5 335.15 ms
(stacking scheme and then the student. Why we need this kind of c)59.5 322.15 ms
(omplexity? Well, usually it's because different)59.5 309.15 ms
(models utilize different patterns in the data and we want to uni)59.5 296.15 ms
(te all)59.5 283.15 ms
(of this patterns in one mighty model. And stacking can do exactl)59.5 270.15 ms
(y that for us. This may seem too complicated. Of course, it take)59.5 257.15 ms
(s time to move up to)59.5 244.15 ms
(this kind of scheme in a competition. But be sure that after)59.5 231.15 ms
(completion our course, you already have enough)59.5 218.15 ms
(knowledge about how to do this. These schemes never appear in th)59.5 205.15 ms
(e final)59.5 192.15 ms
(shape at the beginning of the competition. Most work here usuall)59.5 179.15 ms
(y is)59.5 166.15 ms
(done on the first level. So you try to create diverse meta featu)59.5 153.15 ms
(res)59.5 140.15 ms
(and unite them in one simple model. Usually, you start to create)59.5 127.15 ms
( the high)59.5 114.15 ms
(grade second level of stacking, when you have only a few days le)59.5 101.15 ms
(ft. And after that, you mostly work on)59.5 88.15 ms
(the improvement of this scheme. That said, you already have)59.5 75.15 ms
(the required knowledge and now you just need to get)59.5 62.15 ms
(some practice out there. Be diligent, and without a doubt,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 7 7
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                      Page 7)59.5 790.15 ms
F0 sf
(you will succeed. [SOUND] [MUSIC][MUSIC] Hi, in this video, I wi)59.5 764.15 ms
(ll tell you about Microsoft)59.5 751.15 ms
(Malware Classification Challenge. We were participating in a tea)59.5 738.15 ms
(m with)59.5 725.15 ms
(other lecturers of this course, Mikhail Trofimov and Stanislav S)59.5 712.15 ms
(emenov. We got the third place)59.5 699.15 ms
(in this competition. The plan for)59.5 686.15 ms
(the presentation is the following. We will start with the data)59.5 673.15 ms
(description and a little bit of EGA. We'll then talk about featu)59.5 660.15 ms
(re extraction, then we will discuss how we did)59.5 647.15 ms
(feature processing and selection. We will see how we did the mod)59.5 634.15 ms
(eling,)59.5 621.15 ms
(and finally we will explain several tricks that allowed us)59.5 608.15 ms
(to get higher on the league of board. So let's start with proble)59.5 595.15 ms
(m description. In this competition, we will provided about half )59.5 582.15 ms
(of terabyte)59.5 569.15 ms
(of malicious software executables. Each executable was)59.5 556.15 ms
(represented in two forms. The first is, HEX dump. HEX dump is ju)59.5 543.15 ms
(st a representation of file)59.5 530.15 ms
(as a sequence of bytes, it is row format. The file on the disk i)59.5 517.15 ms
(s)59.5 504.15 ms
(stored as a sequence or bytes, that is exactly)59.5 491.15 ms
(what we see in HEX dump. The second form is a listing generated )59.5 478.15 ms
(by)59.5 465.15 ms
(interactive disassembler, or IDA in short. IDA tries to convert )59.5 452.15 ms
(low level)59.5 439.15 ms
(sequence of bytes from the HEX dump to a sequence of assembler c)59.5 426.15 ms
(olumns. Take a look at the bottom screenshot. On the left,)59.5 413.15 ms
(we see sequences of bytes from HEX dump. And on the right,)59.5 400.15 ms
(we see what IDA has converted them to. The task was to classify )59.5 387.15 ms
(malware)59.5 374.15 ms
(files into nine families. We will provide with train sets of abo)59.5 361.15 ms
(ut)59.5 348.15 ms
(10,000 examples with known labels, and a testing set of singular)59.5 335.15 ms
( size. And the evaluation metric was)59.5 322.15 ms
(multi-class logarithmic loss. Note that the final loss in this)59.5 309.15 ms
(competition was very, very low. The corresponding accuracy was m)59.5 296.15 ms
(ore than)59.5 283.15 ms
(99.7%, it's enormously high accuracy. Remember, it is sometimes)59.5 270.15 ms
(beneficial to examine metadata. In this competition, we were giv)59.5 257.15 ms
(en)59.5 244.15 ms
(seven zip archives with files. And the filenames look like hash )59.5 231.15 ms
(values. Actually, we did not find any)59.5 218.15 ms
(helpful metadata for our models, but it was interesting how trai)59.5 205.15 ms
(n test)59.5 192.15 ms
(split was done by organizers. The organizers were using the firs)59.5 179.15 ms
(t)59.5 166.15 ms
(letter of the file names for the split. On this plot, on the x a)59.5 153.15 ms
(xis, we see)59.5 140.15 ms
(the first characters of the file names. And on the y axis, we ha)59.5 127.15 ms
(ve the number of files with their)59.5 114.15 ms
(names, starting with the given character. The plots actually loo)59.5 101.15 ms
(k cool, but)59.5 88.15 ms
(the only information we get from them is the train test split is)59.5 75.15 ms
( in fact random,)59.5 62.15 ms
(and we cannot use them to improve our models. So in this competi)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 8 8
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                      Page 8)59.5 790.15 ms
F0 sf
(tion we)59.5 764.15 ms
(were given a raw data, so we needed to extract)59.5 751.15 ms
(the features ourselves. Let's see what features we extracted. We)59.5 738.15 ms
(ll, in fact, no one from our)59.5 725.15 ms
(team had domain knowledge or previous experience in)59.5 712.15 ms
(malware classification. We did not even know)59.5 699.15 ms
(what IDA disassembly is. So we started really simple. Remember i)59.5 686.15 ms
(ssue executable in the dataset)59.5 673.15 ms
(was represented as two files. So our first two features were)59.5 660.15 ms
(the sizes of those files, or equivalently their length. And surp)59.5 647.15 ms
(risingly we got 88% accuracy)59.5 634.15 ms
(just by using these two features. On the plot you see x axis cor)59.5 621.15 ms
(respond to)59.5 608.15 ms
(an index of an object in the train set. We actually sorted the o)59.5 595.15 ms
(bjects)59.5 582.15 ms
(by their class here. And the y axis shows the file sizes)59.5 569.15 ms
(of aHEX dump file for every object. And we see this feature)59.5 556.15 ms
(is quite demonstrative. The most simple features we)59.5 543.15 ms
(could derive out of sequence are account of their elements, righ)59.5 530.15 ms
(t? So this is basically what function)59.5 517.15 ms
(value comes from pandas does. So it is what we did. We counted b)59.5 504.15 ms
(ytes in HEX dump files,)59.5 491.15 ms
(and that is how we get 257 features. And 257 is because we)59.5 478.15 ms
(have 256 byte values, and there was one special symbol. We achie)59.5 465.15 ms
(ved almost 99%)59.5 452.15 ms
(accuracy using those features. At the same time that we started )59.5 439.15 ms
(to)59.5 426.15 ms
(read about this assembly format, and papers on malware classific)59.5 413.15 ms
(ation. And so, we got an idea what)59.5 400.15 ms
(feature is to generate. We looked into this assembly file. We us)59.5 387.15 ms
(e regular expressions)59.5 374.15 ms
(to extract various things. Many of the features we extracted)59.5 361.15 ms
(were thrown away immediately, some were really good. And what we)59.5 348.15 ms
( did actually first,)59.5 335.15 ms
(we counted system calls. You see that on the slide, they are als)59.5 322.15 ms
(o called API calls in)59.5 309.15 ms
(Windows as we read in the paper. And here is the error rate we g)59.5 296.15 ms
(ot)59.5 283.15 ms
(with this feature, it's pretty good. We counted assembler common)59.5 270.15 ms
(s like move,)59.5 257.15 ms
(push, goal, and assembler,)59.5 244.15 ms
(they work rather well. We also try to extract common)59.5 231.15 ms
(sequences like common end grams and extract features out of them)59.5 218.15 ms
(, but)59.5 205.15 ms
(we didn't manage to get a good result. The best features we)59.5 192.15 ms
(had were section count. Just count the number of)59.5 179.15 ms
(lines in this assembly file, which start with .text or)59.5 166.15 ms
(.data, or something like that. The classification accuracy with)59.5 153.15 ms
(that feature was more than 99% using these features. By incorpor)59.5 140.15 ms
(ating all of these, we were)59.5 127.15 ms
(able to get an error rate less than 0.5%. From our text mining e)59.5 114.15 ms
(xperience, we knew that n-grams could be)59.5 101.15 ms
(a good discriminative feature. So we computed big grams. We foun)59.5 88.15 ms
(d a paper which)59.5 75.15 ms
(proposed to use 4-grams. We computed them too, and we even)59.5 62.15 ms
(went further and extracted 10-grams. Of course, we couldn't work)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 9 9
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                      Page 9)59.5 790.15 ms
F0 sf
( with 10)59.5 764.15 ms
(grams directly, so we performed a nice feature selection, which )59.5 751.15 ms
(was one of the)59.5 738.15 ms
(most creative ideas for this competition. We will see later in t)59.5 725.15 ms
(his)59.5 712.15 ms
(video how exactly we did it. Interestingly, just by applying)59.5 699.15 ms
(the feature selection to 10-grams and fitting in XGBoost,)59.5 686.15 ms
(we could get a place in the top 30. Another feature we found)59.5 673.15 ms
(interesting is an entropy. We computed entropy of small)59.5 660.15 ms
(segments of wide sequence by moving the sliding window)59.5 647.15 ms
(over the sequence. And computing entropy inside each window. So )59.5 634.15 ms
(we've got another sequence that could contain an explicit inform)59.5 621.15 ms
(ation about)59.5 608.15 ms
(the encrypted parts of the executable. See, we expect the entrop)59.5 595.15 ms
(y to be)59.5 582.15 ms
(high if the data is encrypted and compressed, and)59.5 569.15 ms
(low if the data is structured. So the motivation is that some ma)59.5 556.15 ms
(lwares)59.5 543.15 ms
(are injected into a normal executables, and they are stored in e)59.5 530.15 ms
(ncrypted format. When the program starts, the malware)59.5 517.15 ms
(extracts itself in background first and then executes. So entrop)59.5 504.15 ms
(y features could)59.5 491.15 ms
(kind of help us to detect and encrypt the trojans in the executa)59.5 478.15 ms
(bles,)59.5 465.15 ms
(and thus, detect some classes of malware. But we got an entropy)59.5 452.15 ms
(sequence of variable length. We couldn't use those)59.5 439.15 ms
(sequences as features, right? So we generated some statistics of)59.5 426.15 ms
( entropy)59.5 413.15 ms
(distribution like mean and median. And also we computed a 20 per)59.5 400.15 ms
(centiles and)59.5 387.15 ms
(inverse percentiles, and use them as features. The same features)59.5 374.15 ms
( were extracted out)59.5 361.15 ms
(of entropy changes, that is we first apply diff function to entr)59.5 348.15 ms
(opy sequence)59.5 335.15 ms
(and then compute the statistics. It was possible to extract thre)59.5 322.15 ms
(e)59.5 309.15 ms
(things from hex dump by looking for printable sequences that)59.5 296.15 ms
(ends with 0 element. We didn't use strings themselves,)59.5 283.15 ms
(but we computed strings lengths. Distribution for each file and)59.5 270.15 ms
(extract the similar statistics, the statistics that we extracted)59.5 257.15 ms
(from entropy sequences. Okay, we finished with feature extractio)59.5 244.15 ms
(n. So let's see how we pre-process them and)59.5 231.15 ms
(perform feature selection. Those moment when we)59.5 218.15 ms
(generated a lot of features. We wanted to incorporate all of the)59.5 205.15 ms
(m)59.5 192.15 ms
(in the classifier, but we could not fit the classifier efficient)59.5 179.15 ms
(ly when)59.5 166.15 ms
(we got say 20,000 features. Most features will probably useless,)59.5 153.15 ms
( so we tried different feature selection)59.5 140.15 ms
(method and transformation techniques. Let's consider the buys co)59.5 127.15 ms
(unts features. There are 257 of them, not much, but)59.5 114.15 ms
(probably there is still a redundancy. We tried non-negative)59.5 101.15 ms
(matrix factorization, NMF. And the principal component analysis,)59.5 88.15 ms
(PCA, in order to remove this redundancy. I will remind you that )59.5 75.15 ms
(both NMF and)59.5 62.15 ms
(PCA are trying to factorize object feature matrix x into)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 10 10
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                     Page 10)59.5 790.15 ms
F0 sf
(the product of two low-rank matrices. You can think of that of a)59.5 764.15 ms
(s finding a small number of basis vectors in the)59.5 751.15 ms
(feature space, so that every object can be approximated by a lin)59.5 738.15 ms
(ear)59.5 725.15 ms
(combination of those basis vectors. And this coefficients of)59.5 712.15 ms
(approximation can be treated as new features for each object. So)59.5 699.15 ms
( the only difference between NMF and)59.5 686.15 ms
(PCA is that NMF requires all the components of floor rank)59.5 673.15 ms
(matrices to be non-negative. We set the number of basis vectors )59.5 660.15 ms
(to 15,)59.5 647.15 ms
(and here we see plots between)59.5 634.15 ms
(the first two extracted features. One for NMF and one for PCA. S)59.5 621.15 ms
(o these are the coefficient for)59.5 608.15 ms
(most important basis vectors actually. We used 3D based model an)59.5 595.15 ms
(d it is obvious that NMF features)59.5 582.15 ms
(were a lot better for trees. So NMF works good when the non-nega)59.5 569.15 ms
(tivity)59.5 556.15 ms
(of the data is essential. And in our case we worked with counts,)59.5 543.15 ms
(which are non-negative by nature. Simple trick to get another pa)59.5 530.15 ms
(ck of)59.5 517.15 ms
(features by doing almost nothing is to apply a log)59.5 504.15 ms
(transform to the data and calculate NMF on)59.5 491.15 ms
(the transformed data again. Let's think what happened here. NMF )59.5 478.15 ms
(protest originally uses MSE laws to measure the quality of)59.5 465.15 ms
(approximation it build. This trick implicitly changes the laws)59.5 452.15 ms
(NMF optimizes from MSE to RMSLE. Just recall that RMSLE)59.5 439.15 ms
(is MSE in the log space. So now the decomposition process)59.5 426.15 ms
(pays attention to different things due to different loss, and)59.5 413.15 ms
(thus produces different features. We used the small pipeline)59.5 400.15 ms
(to select 4-grams features. We removed rare features, applied)59.5 387.15 ms
(linear SVM to the L1 regularization, as such model tends to sele)59.5 374.15 ms
(ct features. And after that,)59.5 361.15 ms
(we thresholded random forest feature importances to get final fe)59.5 348.15 ms
(ature)59.5 335.15 ms
(set of only 131 feature. The pipeline was a little bit)59.5 322.15 ms
(more complicated with 10-grams. First, we used the hashing to re)59.5 309.15 ms
(use)59.5 296.15 ms
(the dimensionality of original features. We actually did it onli)59.5 283.15 ms
(ne)59.5 270.15 ms
(while computing 10-grams. We then selected about 800 features)59.5 257.15 ms
(based on L1 regularize SVM and the RandomForest importances. Thi)59.5 244.15 ms
(s is how we got about 800 features)59.5 231.15 ms
(instead of 2 to the power of 28. But we went even further. This )59.5 218.15 ms
(is actually the most)59.5 205.15 ms
(interesting part for me. The main problem with feature selection)59.5 192.15 ms
(, that I've just described,)59.5 179.15 ms
(is that we've done it for 10-grams independently of other featur)59.5 166.15 ms
(es)59.5 153.15 ms
(that we already had to that moment. After selection,)59.5 140.15 ms
(we could end up with really good features. But those features co)59.5 127.15 ms
(uld)59.5 114.15 ms
(contain the same information that we already had in other featur)59.5 101.15 ms
(es. And we actually wanted to improve)59.5 88.15 ms
(our features with 10-grams. So here is what we did instead. We g)59.5 75.15 ms
(enerated out of fold prediction for the train set using all)59.5 62.15 ms
(the features that we had. We sorted the object by their true cla)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 11 11
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                     Page 11)59.5 790.15 ms
F0 sf
(ss)59.5 764.15 ms
(predicted probability and try to find the features that would se)59.5 751.15 ms
(parate the most)59.5 738.15 ms
(error prone object from the others. So actually, we use another )59.5 725.15 ms
(model for it. We've created a new data set with)59.5 712.15 ms
(error prone objects having label 1, and others having label 0. W)59.5 699.15 ms
(e trained random forest and selected 14 10-grams, well,)59.5 686.15 ms
(actually hashes to be precise. We had a nice performance)59.5 673.15 ms
(increase on the leaderboard when incorporating those 14 features)59.5 660.15 ms
(. This method actually could)59.5 647.15 ms
(lead us to sever overfitting, but it fortunately worked out nice)59.5 634.15 ms
(ly. All right, let's get to modeling. We didn't use stacking in )59.5 621.15 ms
(this)59.5 608.15 ms
(competition as we usually do now. It became popular slightly)59.5 595.15 ms
(after this competition. We first used Random Forest everywhere,)59.5 582.15 ms
(but it turned out it needs to calibrated for)59.5 569.15 ms
(log loss. So we switched to XGBoost and)59.5 556.15 ms
(used it for all our modeling. Every person in the team)59.5 543.15 ms
(extracted his own features and trained his own models)59.5 530.15 ms
(with his own parameters. So our final solution was)59.5 517.15 ms
(a combination of diverse models. We found bagging worked)59.5 504.15 ms
(quite well in this data set. We even did the bagging)59.5 491.15 ms
(in very peculiar way. We concatenated our twin set)59.5 478.15 ms
(with a seven times larger set of object sampled from train)59.5 465.15 ms
(set with the replacement. And we used the resulting data set tha)59.5 452.15 ms
(t is eight times larger than)59.5 439.15 ms
(original train set, to train XGBoost. Of course, we averaged abo)59.5 426.15 ms
(ut 20)59.5 413.15 ms
(models to account for randomness. And finally, let's talk about )59.5 400.15 ms
(several)59.5 387.15 ms
(more tricks we used in this competition. The accuracy of the mod)59.5 374.15 ms
(els was quite high,)59.5 361.15 ms
(and we all know that the more)59.5 348.15 ms
(data we have the better. So we've decided to try to use)59.5 335.15 ms
(testing data for training our models. We just need some labels f)59.5 322.15 ms
(or)59.5 309.15 ms
(the testers, right? We either use predicted class or we sample t)59.5 296.15 ms
(he label from)59.5 283.15 ms
(the predicted class distribution. Generated test set labels)59.5 270.15 ms
(are usually called pseudo labels. So how do we perform cross)59.5 257.15 ms
(validation with pseudo labels? We split our train set into fold)59.5 244.15 ms
(as we usually do well performing cross validation. In this examp)59.5 231.15 ms
(le,)59.5 218.15 ms
(we split data in two folds. But what's different in cross valida)59.5 205.15 ms
(tion)59.5 192.15 ms
(is that now, before training the model in a particular fold, we )59.5 179.15 ms
(can concatenate)59.5 166.15 ms
(this fold with the test data. Then we switch the faults and)59.5 153.15 ms
(do the same thing. See we trained the objects to)59.5 140.15 ms
(denoted with green color and predict the once shown with red. Ok)59.5 127.15 ms
(ay, and to get predictions for)59.5 114.15 ms
(the test set, we again do kind of cross validation)59.5 101.15 ms
(thing, like on the previous slide. But now we divide the test)59.5 88.15 ms
(set in faults and concatenate train set to each)59.5 75.15 ms
(fold of the the test set. In the end, we get out of all predicti)59.5 62.15 ms
(ons)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 12 12
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                     Page 12)59.5 790.15 ms
F0 sf
(for the test set, that is our submission. One of the crucial thi)59.5 764.15 ms
(ngs to)59.5 751.15 ms
(understand about this method, that sometimes we need two)59.5 738.15 ms
(different models for it. And this is the case where one model)59.5 725.15 ms
(is very confident in its predictions. That is, the predictive pr)59.5 712.15 ms
(obabilities)59.5 699.15 ms
(are very close to 0 and 1. In this case, if we train a model,)59.5 686.15 ms
(predict task set, sample labels or)59.5 673.15 ms
(take it the most probable label and retrim same model with the s)59.5 660.15 ms
(ame)59.5 647.15 ms
(features or get no improvement at all. We just did not introduce)59.5 634.15 ms
( any information)59.5 621.15 ms
(with pseudo labeling in this way, but If I ask say, Stanislav,)59.5 608.15 ms
(to predict that with his model. And then I use his labels for a )59.5 595.15 ms
(test and create my model, that is where)59.5 582.15 ms
(I will get a nice improvement. This actually becomes just anothe)59.5 569.15 ms
(r way to incorporate knowledge)59.5 556.15 ms
(from two diverse models. And the last thing that helped)59.5 543.15 ms
(us a lot is per-class mixing. At the time of the competition,)59.5 530.15 ms
(people usually mixed models linearly. We went further and mixed )59.5 517.15 ms
(models joining)59.5 504.15 ms
(coefficients for each class separately. In fact, if you think ab)59.5 491.15 ms
(out it,)59.5 478.15 ms
(it's very similar to stacking with a linear model of a special)59.5 465.15 ms
(kind at a second level. But the second became popular in)59.5 452.15 ms
(a month after this competition, and what we did here is a very)59.5 439.15 ms
(simplest manual form of stacking. We published our source code o)59.5 426.15 ms
(nline, so if)59.5 413.15 ms
(you are interested you can check it out. All right,)59.5 400.15 ms
(this was an interesting competition. It was challenging from tec)59.5 387.15 ms
(hnical)59.5 374.15 ms
(view point as we needed to manipulate more than half of terabyte)59.5 361.15 ms
( of data,)59.5 348.15 ms
(but it was very interesting. The data was given in a raw format )59.5 335.15 ms
(and the actual challenge was to)59.5 322.15 ms
(come up with nice features. And that is where we could be creati)59.5 309.15 ms
(ve,)59.5 296.15 ms
(thank you. [MUSIC]Hi. In this video, I'm going to talk about Wal)59.5 283.15 ms
(mart trip type classification challenge which was held in Kaggle)59.5 270.15 ms
( couple of years ago. I won the first place in that competition.)59.5 257.15 ms
( And now, I will tell you about most interesting parts of the pr)59.5 244.15 ms
(oblem and about my solution. That said, this presentation consis)59.5 231.15 ms
(ts of four parts. First, we will state the problem. Second,we wi)59.5 218.15 ms
(ll understand what data format and data reprocessing. And third,)59.5 205.15 ms
( we will talk about models, their relative quality and their rel)59.5 192.15 ms
(ation to the general staking scheme. And finally, we will overvi)59.5 179.15 ms
(ew some possibilities to generate new features here. So, let's s)59.5 166.15 ms
(tart. In our data, we had purchases people made in Walmart visit)59.5 153.15 ms
(ing their shop in two weeks, and we had to classify them into 38)59.5 140.15 ms
( visiting trip types or classes. Let's take a quick look at feat)59.5 127.15 ms
(ures in the data. Trip type column represents the target, visit )59.5 114.15 ms
(number represents ID which unites purchases made by one customer)59.5 101.15 ms
( in one shopping trip. For example, a customer which made visit )59.5 88.15 ms
(number seven, purchased two items which are located in the secon)59.5 75.15 ms
(d and in the third lines of this data frame. Notice that all row)59.5 62.15 ms
(s with the same visit number have the same trip type. An importa)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 13 13
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                     Page 13)59.5 790.15 ms
F0 sf
(nt moment, is that we have to predict a trip type for visit numb)59.5 764.15 ms
(er, and not for each row in the train data. And as you can see, )59.5 751.15 ms
(in the train we have around 647,000 rows and only 95,000 visits.)59.5 738.15 ms
( Back to the features, next feature is weekday which obviously r)59.5 725.15 ms
(epresents the weekday of the visit. Next is UPC. UPC is an exact)59.5 712.15 ms
( ID of a purchased item. Then, scan count. Scan count is the exa)59.5 699.15 ms
(ct number of items purchased. Note that minus one here represent)59.5 686.15 ms
(s not the purchase but the return. Next features, department des)59.5 673.15 ms
(cription, with 68 unique values is a broad category for an item.)59.5 660.15 ms
( And finally, fineline number, with around 5000 unique values, i)59.5 647.15 ms
(s a more refined category for an item. So, after we understood w)59.5 634.15 ms
(hat this feature represents, let's recall that we have to make o)59.5 621.15 ms
(ne prediction for each visit number. Let's take a look at the da)59.5 608.15 ms
(ta for the visit number eight. We can see here that this particu)59.5 595.15 ms
(lar visit has a lot of purchases in category paint and accessori)59.5 582.15 ms
(es, which means that trip type number 26 may represent a visit w)59.5 569.15 ms
(ith most purchases in that category. Now, how to approach model )59.5 556.15 ms
(train in here. Let's take another look at the data and assess ou)59.5 543.15 ms
(r possibilities. Should we predict trip type for each item on th)59.5 530.15 ms
(e list or should we choose another way? Of course both of them a)59.5 517.15 ms
(re possible, but in the first one, we'll predict trip type for e)59.5 504.15 ms
(ach row with each data set, we'll miss important interactions be)59.5 491.15 ms
(tween items which belong to the same visit. For example, trip ty)59.5 478.15 ms
(pe may have a number of 26, if more than half of its items are f)59.5 465.15 ms
(rom paint and accessories. But, if we will not account for inter)59.5 452.15 ms
(action between these items, it can be quite hard to predict. So,)59.5 439.15 ms
( the second option of uniting all purchase in the visit and maki)59.5 426.15 ms
(ng a data set where each row represents a complete visit, seems )59.5 413.15 ms
(more reasonable. And, as can be expected, this approach leads to)59.5 400.15 ms
( more significant benefits in the competition. I'm going to show)59.5 387.15 ms
( you the easiest way to change the data format to the desired on)59.5 374.15 ms
(e. Let's choose the department description feature for the purpo)59.5 361.15 ms
(se of an example. First, let's group the data frame by visit num)59.5 348.15 ms
(ber and calculate how many times each department description is )59.5 335.15 ms
(present in a visit. Then, let's unstack last group by column so )59.5 322.15 ms
(we will get a unique column for each department description valu)59.5 309.15 ms
(e. Now, this is the format we wanted. Each row represents a visi)59.5 296.15 ms
(t and each column is a feature described in that visit. We can u)59.5 283.15 ms
(se this group by approach for other features besides department )59.5 270.15 ms
(description. Also note that items in the visit are actually very)59.5 257.15 ms
( similar to words in a text. After our confirmation, each featur)59.5 244.15 ms
(e here represents counts, so we could apply ideas which usually )59.5 231.15 ms
(works with text, for example, tf-idf transformation. As you can )59.5 218.15 ms
(guess, a lot of possibilities emerge here. Great. After this is )59.5 205.15 ms
(done and we process data in the desired format, let's move to ch)59.5 192.15 ms
(oosing a model. Based on what we already have discussed, can you)59.5 179.15 ms
( guess if we should expect the significant difference in scores )59.5 166.15 ms
(between linear models and tree-based models here? Think about th)59.5 153.15 ms
(is a bit. For example, is there a reason why linear models will )59.5 140.15 ms
(under perform in comparison to tree based-models? Yes, there is.)59.5 127.15 ms
( Again, I'm talking about interactions here. Indeed, tree-based )59.5 114.15 ms
(models in neural network have significant superiority in quality)59.5 101.15 ms
( in this competition for this very reason. But still, one can us)59.5 88.15 ms
(e linear models and TNN to produce useful method features here. )59.5 75.15 ms
(Despite the fact that they didn't imply interactions, they were )59.5 62.15 ms
(a valuable asset in my general staking scheme. I will not go int)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 14 14
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                     Page 14)59.5 790.15 ms
F0 sf
(o further details of staking here because we already covered mos)59.5 764.15 ms
(t ideas in other videos about competitions. Instead, we'll talk )59.5 751.15 ms
(a bit about feature generation. Except for interactions between )59.5 738.15 ms
(items purchased in one visit, one could try to exploit interacti)59.5 725.15 ms
(ons between features. The interesting and unexpected result here)59.5 712.15 ms
( was that one fineline number can belong to multiple department )59.5 699.15 ms
(descriptions, which means that fineline number is not a more det)59.5 686.15 ms
(ailed department description as you can think. Using this intera)59.5 673.15 ms
(ction, one can further improve his model. Another interesting fe)59.5 660.15 ms
(ature generation idea was connected to the time structure of the)59.5 647.15 ms
( data. Take a look at this plot, it represents the change in the)59.5 634.15 ms
( weekday feature relative to the row number. It looks like the d)59.5 621.15 ms
(ata is ordered by time here. And the data appears to consist of )59.5 608.15 ms
(31 days, but train test split wasn't time based. So, you could d)59.5 595.15 ms
(erive features like day number in the data set, number of a visi)59.5 582.15 ms
(t in a day, and the total amount of visits in a day. So, this is)59.5 569.15 ms
( it. We just discussed the most interesting parts of this compet)59.5 556.15 ms
(ition. Changing the data format to a more suitable, generating f)59.5 543.15 ms
(eatures while doing sold, working with models while doing stacki)59.5 530.15 ms
(ng. And finally, doing some for additional feature engineering. )59.5 517.15 ms
(The challenge itself proved useful and interesting. And I would )59.5 504.15 ms
(recommend you to check it out and try approaches we have talked )59.5 491.15 ms
(about.Hello everyone. Today, I will explain to you how, me and m)59.5 478.15 ms
(y team mate Gert, we won a very special Kaggle competition calle)59.5 465.15 ms
(d the Acquired Valued Shoppers Challenge. First let me give you )59.5 452.15 ms
(some background about the competition. It was a recommender's ch)59.5 439.15 ms
(allenge. And when I say a recommender's challenge, I mean you ha)59.5 426.15 ms
(ve a list of products and a list of customers, and you try to br)59.5 413.15 ms
(ing them together. So we target them so we recommend which custo)59.5 400.15 ms
(mer in order to increase sales loyalty or something else. There )59.5 387.15 ms
(were around 1,000 teams and for back then, at least they were qu)59.5 374.15 ms
(ite a lot. Now Kaggle has become much more popular. But back the)59.5 361.15 ms
(n, given the size of the data, which was quite big, I think this)59.5 348.15 ms
( competition attracted a lot of attention. And as I said, we att)59.5 335.15 ms
(ained first place with my teammate Gert, for what it was, I thin)59.5 322.15 ms
(k, a very clear win because we took a lead early in the competit)59.5 309.15 ms
(ion, and we maintained it. And that solution was not actually ve)59.5 296.15 ms
(ry machine learning Kebbi, but it was focused on really trying t)59.5 283.15 ms
(o understand the problem well and find ways to validate properly)59.5 270.15 ms
(. And in general, it was very, very focused on getting sensible )59.5 257.15 ms
(results. And that's why I think it's really valuable to explain )59.5 244.15 ms
(what we did. So what was the actual problem we tried to solve? I)59.5 231.15 ms
(magine you have 310,000 shoppers, almost equally split. I'm on a)59.5 218.15 ms
( train and test. You have all their transactions from a point wh)59.5 205.15 ms
(ere they were given an offer, and these were about 350 million t)59.5 192.15 ms
(ransactions. So, one year of all the transactions of all the cus)59.5 179.15 ms
(tomers involved in this challenge, and you have 37 different off)59.5 166.15 ms
(ers. When I say offer here, it is actually a coupon. So, a shopp)59.5 153.15 ms
(er is sent normally. It's a piece of paper that says, "If you bu)59.5 140.15 ms
(y this item, you can get a discount." So it recommends to certai)59.5 127.15 ms
(n item. Now, we don't know exactly what discount is. Maybe disco)59.5 114.15 ms
(unts, maybe it says, "You can buy another item for free." So, ma)59.5 101.15 ms
(ybe a different promotion, but in principle is some sort of ince)59.5 88.15 ms
(ntive for you to buy this item. I have mentioned items so far or)59.5 75.15 ms
( products, but the notion of product was not actually present in)59.5 62.15 ms
( this challenge, but we could infer it. We could say that a uniq)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 15 15
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                     Page 15)59.5 790.15 ms
F0 sf
(ue combination of a brand, category, and company that were prese)59.5 764.15 ms
(nt could form a product, at least for this competition. Let me g)59.5 751.15 ms
(ive you a bit more details about the actual problem we are tryin)59.5 738.15 ms
(g to solve. Imagine you have a timeline, starts from one year in)59.5 725.15 ms
( the past until one year after. So, in the past, a customer make)59.5 712.15 ms
(s a visit to the shop, buys certain items, leaves, comes back an)59.5 699.15 ms
(other day, makes another visit, buys more items. Then at some po)59.5 686.15 ms
(int, he or she is targeted with an offer, a coupon as I said. Al)59.5 673.15 ms
(l transactional history for this customer stops there. You don't)59.5 660.15 ms
( know anything else. Up to this point, you know everything for o)59.5 647.15 ms
(ne year back up to this. The only thing you know is, you know th)59.5 634.15 ms
(at the customer has indeed redeemed that coupon. So he did buy t)59.5 621.15 ms
(he offer product. And for that training data only, you also have)59.5 608.15 ms
( a special flag that tells you whether he or she bought that ite)59.5 595.15 ms
(m again. And you can see why this is valuable. Because normally,)59.5 582.15 ms
( when you target someone with a coupon, you give a discount, and)59.5 569.15 ms
( you don't make that much profit actually, but you aim in establ)59.5 556.15 ms
(ishing a long-term relationship with the customer. And that's wh)59.5 543.15 ms
(y they were really interested in getting a model here that could)59.5 530.15 ms
( predict which recommendation, which offer will have that effect)59.5 517.15 ms
(, will make a customer develop a habit in buying this item. And )59.5 504.15 ms
(what we were being tested on was AUC. By this point, I expect yo)59.5 491.15 ms
(u roughly know what AUC is as a metric, but in principle, you're)59.5 478.15 ms
( interested in how well your score discriminates between those t)59.5 465.15 ms
(hat bought or can buy the item again and those that will not. So)59.5 452.15 ms
(, when you have the highest score, you expect higher chance to b)59.5 439.15 ms
(uy the item, and a lower score, lower attempts to buy the item a)59.5 426.15 ms
(gain. So, higher AUC means that this discrimination is stronger )59.5 413.15 ms
(with your score. This was a challenging challenge. And it was ch)59.5 400.15 ms
(allenging because, first of all, the datasets were quite big. As)59.5 387.15 ms
( I said, 350 million transactions. Me and Gert, we didn't have c)59.5 374.15 ms
(razy resources back then. I have to admit that I have personally)59.5 361.15 ms
( improved my hardware since then, but actually back then, I was )59.5 348.15 ms
(working only with a laptop that had 32-bit Windows and only four)59.5 335.15 ms
( gigabytes of RAM. So, really small and mainly challenging that )59.5 322.15 ms
(we had to deal with these client files. And then, we didn't have)59.5 309.15 ms
( features. So, what we knew is this customer was given this offe)59.5 296.15 ms
(r, which is consistent by these three elements I mentioned befor)59.5 283.15 ms
(e, category, brand, and company, and the time that this recommen)59.5 270.15 ms
(dation was made nothing else. Then, you had to go to the transac)59.5 257.15 ms
(tional history and try to generate features. And you know, anybo)59.5 244.15 ms
(dy could create really anything they wanted. There was not a cle)59.5 231.15 ms
(ar answer about which features would work best. So this is not y)59.5 218.15 ms
(our typical challenge where you're normally given the thesis. Bu)59.5 205.15 ms
(t it is quite difficult for the type of the recommender's challe)59.5 192.15 ms
(nge. And what really makes this competition difficult, interesti)59.5 179.15 ms
(ng, and what I think at the end of the day gave us the win was t)59.5 166.15 ms
(he fact that the testing environment was very irregular. And we )59.5 153.15 ms
(can define irregular, in this context, as an environment where t)59.5 140.15 ms
(he train data and the test data had different customers. So, no )59.5 127.15 ms
(overlaps. Different customers, and one different in the other. A)59.5 114.15 ms
(lso, the training this data had in general different offers. It )59.5 101.15 ms
(was showing you a graph that shows that the distribution of its )59.5 88.15 ms
(offer and whether it appears in the train or in the test data or)59.5 75.15 ms
( both. And you can see that most offers, either appear only in t)59.5 62.15 ms
(est or they appear only in train with minimal overlap. So, that )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 16 16
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                     Page 16)59.5 790.15 ms
F0 sf
(makes it a bit difficult because you basically have to make a mo)59.5 764.15 ms
(del with soft products. They were offering the train, but in the)59.5 751.15 ms
( test data, you have completely other offers. So you don't know )59.5 738.15 ms
(how they would behave as these products have never been offered )59.5 725.15 ms
(before. And the last element is, the test data is obviously in t)59.5 712.15 ms
(he future. That is expected. But given the other elements, this )59.5 699.15 ms
(makes it more difficult, especially in some cases were well in t)59.5 686.15 ms
(he future. And some of it is not as important elements, but stil)59.5 673.15 ms
(l crucial was that this challenge was focusing on acquisition. S)59.5 660.15 ms
(o, there is not that much history between the customer and the o)59.5 647.15 ms
(ffered product. And I say this is irregular because grocery sale)59.5 634.15 ms
(s are in principle based on what the customer already like and h)59.5 621.15 ms
(as bought many times in the past. So we referred to these type o)59.5 608.15 ms
(f acquisition problem, where we don't have much history, as the )59.5 595.15 ms
(cold start problem, and it is much more challenging because you )59.5 582.15 ms
(don't have that direct link. That's, the customer really like th)59.5 569.15 ms
(is product I made an offer because we don't have a past history )59.5 556.15 ms
(that can verify this or we don't have much history. And the last)59.5 543.15 ms
( element is that if you actually see the propensity of an offer )59.5 530.15 ms
(to be bought, again in the training data, the results were quite)59.5 517.15 ms
( different. And here, I give you the offer by shortened propensi)59.5 504.15 ms
(ty, and you can see some offers had much success to be bought ag)59.5 491.15 ms
(ain. It's like offer two that somehow this had much less. For ex)59.5 478.15 ms
(ample, 20 percent, and this is just a sample. There were some ot)59.5 465.15 ms
(her offers that had around five percent. So, if you put now ever)59.5 452.15 ms
(ything into the context, you have different customer, different )59.5 439.15 ms
(offers, different buyer, different time periods. In principle, y)59.5 426.15 ms
(ou don't have that much information about the customer and the o)59.5 413.15 ms
(ffer product, and you know that the offers of the training data )59.5 400.15 ms
(are actually quite different. It's really difficult to get a sta)59.5 387.15 ms
(ndard pattern here. And you know that the offers in the test dat)59.5 374.15 ms
(a are going to be different. So, all this made it a difficult pr)59.5 361.15 ms
(oblem to solve or in irregular environment. How did we handle bi)59.5 348.15 ms
(g data? We did it with indexing. And the way I did the indexing )59.5 335.15 ms
(was, I saw that the data were already shorted by customer and ti)59.5 322.15 ms
(me. So, I passed through these big data file of transactions, an)59.5 309.15 ms
(d every time I encountered a new customer, I created a new file.)59.5 296.15 ms
( So I created a different file for each customer that contained )59.5 283.15 ms
(all his or her transactions, and that made it really easy to gen)59.5 270.15 ms
(erate features, because if I have to generate features for a spe)59.5 257.15 ms
(cific customer, I would just access this file and create all the)59.5 244.15 ms
( features I wanted at the will. This is also very scalable. So I)59.5 231.15 ms
( could create threads to do this in parallel. So, access many cu)59.5 218.15 ms
(stomers in parallel. And I did this not only for every customer,)59.5 205.15 ms
( but also for every category, brand, and company. So, every time)59.5 192.15 ms
( I wanted to access information, I would just access the right c)59.5 179.15 ms
(ategory, the right brand, or the right customer, and I will get )59.5 166.15 ms
(the information I wanted, and that made it very quick to handle )59.5 153.15 ms
(all these big chunks of data. But what I think was the most cruc)59.5 140.15 ms
(ial thing is how we handle this irregularity. I think at the end)59.5 127.15 ms
( of the day, this is what determines our victory because once we)59.5 114.15 ms
( got this right and we were able to try all sorts of things and )59.5 101.15 ms
(we had the confidence that it will work in the test data. The fi)59.5 88.15 ms
(rst thing we tried to do and this is something that I want you t)59.5 75.15 ms
(o really understand, is how we can replicate internally what we )59.5 62.15 ms
(are being tested on. That's really important. I'll give you the )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 17 17
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                     Page 17)59.5 790.15 ms
F0 sf
(room to try all these things. Try all different permutations and)59.5 764.15 ms
( combinations of data techniques, anything you have you can put )59.5 751.15 ms
(in mind, and really understand what works and what's not. So, we)59.5 738.15 ms
( tried to do that. The first of attempt didn't go very well. So )59.5 725.15 ms
(we try to randomly split the data between train and validation a)59.5 712.15 ms
(nd we're trying to make certain that each offer is represented e)59.5 699.15 ms
(qually in each one of this train and validation data set proport)59.5 686.15 ms
(ionately equally. But was that correct? I mean, if you think abo)59.5 673.15 ms
(ut it. What we were doing there, we were saying I'm building a m)59.5 660.15 ms
(odel with some offers and I'm validating in the same offers. Tha)59.5 647.15 ms
(t's good. Maybe we can do well here. But is this what we're real)59.5 634.15 ms
(ly being tested on? No. Because in the test data, we'll have com)59.5 621.15 ms
(pletely different offers. So, this didn't work very well. This w)59.5 608.15 ms
(as giving very nice internal results but not very good results i)59.5 595.15 ms
(n the test data. So, we tried something else. Can we leave one o)59.5 582.15 ms
(ffer out? And I'm showing you roughly what this look like. So, f)59.5 569.15 ms
(or every offer, can we put an offer in the validation data and u)59.5 556.15 ms
(se all the cases of every other offer to train a model? So, if w)59.5 543.15 ms
(e were to predict offer 16, we will use all customers that recei)59.5 530.15 ms
(ved offer 1 to 15 and 17 to 24 to build the model and then we'll)59.5 517.15 ms
( make predictions for all those customers that received offer 16)59.5 504.15 ms
(. And you can see that this actually is quite close to what you')59.5 491.15 ms
(re being tested on because you know you're building a model with)59.5 478.15 ms
( some offers but, you're being tested on some other offer that i)59.5 465.15 ms
(s not there. And you can take the average of all these 24 users )59.5 452.15 ms
(and I put 24 because this is how many offers you really have in )59.5 439.15 ms
(the training data. You can take that average and that average ma)59.5 426.15 ms
(y be much more close to the reality, close to what you were bein)59.5 413.15 ms
(g tested on. And this was true. This gave better results, but we)59.5 400.15 ms
( were still not there. And I'll show you why we were not there. )59.5 387.15 ms
(Consider the following problem. Here, I'll give you a small samp)59.5 374.15 ms
(le of predictions for offer two and what was the actual target? )59.5 361.15 ms
(What we see here is a perfect AUC score. Why? Because all our po)59.5 348.15 ms
(sitive cases that are labeled with one and they have the green c)59.5 335.15 ms
(olor, have higher score than the red ones, where the target is z)59.5 322.15 ms
(ero. So, the discrimination here is perfect. We have a point, a )59.5 309.15 ms
(cutoff point. We can set 0.5 here where all cases that have scor)59.5 296.15 ms
(e higher than this. We can safely say they are one and that is t)59.5 283.15 ms
(rue and everything that has a score lower than this are zero. So)59.5 270.15 ms
(, you see here one discrimination is perfect. Let's now take a s)59.5 257.15 ms
(ample from offer four. If you remember offer four, had in genera)59.5 244.15 ms
(l lower propensity. Offer two had around 0.5 and offer four had )59.5 231.15 ms
(around 0.2. So, it's mean we're center much lower and what you c)59.5 218.15 ms
(an see here is that, again, AUC is perfect for this sample becau)59.5 205.15 ms
(se again, all the higher scores that are labeled with green have)59.5 192.15 ms
( a target of one. And then the lower scores, everything that has)59.5 179.15 ms
( a score less than 0.18 has a target of 0. The discrimination is)59.5 166.15 ms
( perfect. We can find this cutoff point. We can say 0.8, where e)59.5 153.15 ms
(verything that has a score higher than this can safely be set to)59.5 140.15 ms
( one. And that is always true. And vice versa, everything that's)59.5 127.15 ms
( less than 18, then it's a 0. And that is always true. So, we ha)59.5 114.15 ms
(ve two scores. They discriminate really well between the good an)59.5 101.15 ms
(d the bad cases. However, we are not tested on the AUC of one of)59.5 88.15 ms
(fer. We are tested on the AUC of all offers together. So the tes)59.5 75.15 ms
(t data have many offers. So, you are interested in the score tha)59.5 62.15 ms
(t generalizes well against any offer. So, what happens if we try)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 18 18
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week5_win_kaggle.txt                                     Page 18)59.5 790.15 ms
F0 sf
( to merge this table? AUC is no longer perfect and why this happ)59.5 764.15 ms
(ens? Because some of the negative cases of the first offer had h)59.5 751.15 ms
(igher score than the positive cases, those that have a target eq)59.5 738.15 ms
(ual to one from the second offer. So you can see, although the d)59.5 725.15 ms
(iscrimination internally is really good, they don't blend that w)59.5 712.15 ms
(ell. You lose something from that ability of your score to discr)59.5 699.15 ms
(iminate between ones and zeros. And the moment we saw this, we k)59.5 686.15 ms
(new that just leaving one offer out was not going to be enough. )59.5 673.15 ms
(We had to make certain that when we merge all those scores toget)59.5 660.15 ms
(her, the score is still good. The ability of our model to discri)59.5 647.15 ms
(minate is still powerful or it doesn't lose. And that's why we u)59.5 634.15 ms
(se a combination of the previous average AUC of all the offers a)59.5 621.15 ms
(nd the AUC after doing this concatenation. So, the average of th)59.5 608.15 ms
(e two AUCs which really the metric we try to optimize because we)59.5 595.15 ms
( thought that this is actually very close to what we were being )59.5 582.15 ms
(tested on. And here I can show you the result of all our attempt)59.5 569.15 ms
(s and this is with a small subset of features because by that po)59.5 556.15 ms
(int, we were not interested to create the best features, we were)59.5 543.15 ms
( interested to test which approach works best. So, you can see i)59.5 530.15 ms
(f you do it standard stratified K-fold, you can get much nicer r)59.5 517.15 ms
(esults in internal cross-validation but in the test data, the re)59.5 504.15 ms
(lationship is almost opposite. So, highest score in cross-valida)59.5 491.15 ms
(tion leads to worse results in the test data. And you can see wh)59.5 478.15 ms
(y because you're not internally modeling or internally validatin)59.5 465.15 ms
(g or on what you are actually being tested on. Doing the one-off)59.5 452.15 ms
(er out keep obviously lower internal cross-validation results an)59.5 439.15 ms
(d better performance in the test data, but even better was doing)59.5 426.15 ms
( this leave-one-offer plus one concatenation in the end. And thi)59.5 413.15 ms
(s AUC was lower but actually had better test results. I believe )59.5 400.15 ms
(we could get even better results if we made certain that we are )59.5 387.15 ms
(also validating in new customers. But we didn't actually do this)59.5 374.15 ms
( because we saw that this approach had already good results. But)59.5 361.15 ms
( as a means to improve, we could have also made certains that we)59.5 348.15 ms
( validate on different customers because this is what the test w)59.5 335.15 ms
(as like.)59.5 322.15 ms
re sp
%%PageTrailer
%%Trailer
%%Pages: 18
%%EOF
