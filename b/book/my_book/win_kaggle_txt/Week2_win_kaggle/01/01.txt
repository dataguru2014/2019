[MUSIC] Hi, in this lesson we
will talk about the very first steps a good data scientist
takes when he is given a new data set. Mainly, exploratory data analysis or
EDA in short. By the end of this lesson, you will know,
what are the most important things from data understanding and exploration
prospective we need to pay attention to. This knowledge is required
to build good models and achieve high places on the leader board. We will first discuss what exploratory
data analysis is and why we need it. We will then go through important
parts of EDA process and see examples of what we
can discover during EDA. Next we will take a look at the tools
we have to perform exploration. What plots to draw and
what functions from pandas and matplotlib libraries can be useful for us. We will also briefly discuss a very
basic data set cleaning process that is convenient to perform
while exploring the data. And finally we'll go through
exploration process for the Springleaf competition
hosted on Kaggle some time ago. In this video we'll start talking
about Exploratory Data Analysis. What is Exploratory Data Analysis? It's basically a process of looking
into the data, understanding it and getting comfortable with it. Getting comfortable with a task,
probably always the first thing you do. To solve a problem,
you need to understand a problem, and to know what you
are given to solve it. In data science, complete data
understanding is required to generate powerful features and
to build accurate models. In fact while you explore the data,
you build an intuition about it. And when the data is intuitive for you, you can generate hypothesis
about possible new features and eventually find some insights in the data
which in turn can lead to a better score. We will see the example of what EDA
can give us later in this lesson. Well, one may argue that
there is another way to go. Read the data from the hard drive,
never look at it and feed the classifier immediately.They use
some pretty advanced modeling techniques, like mixing, stacking, and eventually get
a pretty good score on the leaderboard. Although this approach sometimes works, it cannot take you to the very
top positions and let you win. Top solutions always use advanced and
aggressive modeling. But usually they have
something more than that. They incorporated insights from the data,
and to find those insights,
they did a careful EDA. While we need to admit the raw
computations where all you can do is modeling and EDA will not help
you to build a better model. It is usually the case when
the data is anonymized, encrypted, pre-processed, and obfuscated. But look it will any way need to perform
EDA to realize that this is the case and you better spend more time on modeling and
make a server busy for a month. One of the main EDA
tools is Visualization. When we visualize the data,
we immediately see the patterns. And with this, ask ourselves,
what are those patterns? Why do we see them? And finally, how do we use those
patters to build a better model? It also can be another way around. Maybe we come up with a particular
hypothesis about the data. What do we do? We test it by making a visualization. In one of the next videos, we'll talk about the main visualization
tools we can use for exploration. Just as a motivation example,
I want to tell you about the competition, alexander D'yakonov, a former top one
at Kagel took part some time ago. The interesting thing about this
competition is that you do not need to do any modeling,
if you understood your data well. In that competition,
the objective was to predict whether a person will use the promo
that a company offers him. So each role correspond to a particular
promo received by a person. There are features that
describe the person, for example his age, sex,
is he married or not and so on. And there are features that describe
the promo, the target is 0 or 1, will he use the promo or not. But, among all the features there
were two especially interesting. The first one is, the number of
promos sent by the person before. And the second is the number of
promos the person had to use before. So let's take a particular user,
say with index 13, and sort the rows by number
of promos sent column. And now let's take a look
at the difference at column the number of used promos
between two consecutive rows. It is written here in diff column. And look, the values in diff column in
most cases equal the target values. And in fact, there is no magic. Just think about
the meaning of the columns. For example, for the second row we see
that the person used one promo already but he was sent only one before that time. And that is why we know that he used the
first promo and thus we have an answer for the first row. In general, if before the current
promo the person used n promos and before the next promo he used that, we know that he used n + 1 promos then we
realize that he used the current promo. And so the answer is 1. If we know that he used n
promos before the next promo, exactly as before the current promo, then obviously he did not use
the current promo and the answer is 0. Well, it's not clear what
to do with the last row for every user, or when we have missing rows,
but you see the point. We didn't even run the classifier,
and we have 80% accuracy already. This would not be possible if we didn't
do an EDA and didn't look into the data. Also as a remark, I should say
that the presented method works because of mistake made by
the organizers during data preparation. These mistakes are called leaks, and in competitions we are usually
allowed to exploit them. We'll see more of these
examples later in this course. So in this video we discussed the main
reasons for performing an EDA. That is to get comfortable with the data
and to find insights in magic features. We also saw an example where EDA and the data understanding was
important to get a better score. And finally, the point to take away. When you start a competition,
you better start with EDA, and not with hardcore modelling. We've had a lot of things to
talk about in this lesson. So letÂ´s move to the next video. [MUSIC]In this video, we'll go through and break down several important steps namely, the first, getting domain knowledge step, second, checking if data is intuitive, and finally, understanding how the data was generated. So let's start with the first step, getting the domain knowledge. If we take a look at the computations hosted in the Kaggle, well, you'll notice, they are rather diverse. Sometimes, we need to detect threats on three dimensional body scans, or predict real estate price, or classify satellite images. Computation can be on a very specific topic which we know almost nothing about, that is, we don't have a domain knowledge. Usually, we don't need to go too deep inside the field but it's preferable to understand what our aim is, what data we have, and how people usually tackle this kind of problems to build a baseline. So, our first step should probably be searching for the topic, Googling within Wikipedia, and making sure we understand the data. For example, let's say we start a new computation in which we need to predict advertisers cost. Our first step is to realize that the computation is about web advertisement. By looking and searching for the column names, using any search engine, we understand that the data was exported from Google AdWords system. And after reading several articles about Google AdWords, we get the meaning of the columns. We now know that impressions column contained a number of times a particular ad appeared before users, and clicks column is how many times the ad was clicked by the users, and of course, the number of clicks should be less or equal than the number of impression. In this video, we'll not go much further into the details about this data set, but you can open the supplementary reading material for a more detailed exploration. After we've learned some domain knowledge, it is necessary to check if the values in the data set are intuitive, and agree with our domain knowledge. For example, if there is a column with age data, we should expect the values rarely to be larger than 100. And for sure, no one ever lived more than 200 years. So, the values should be smaller than 200. But for some reason, we find this super huge value 336. Most probably, is just a typo but it should be 36 or 33, and the best we can do is manually correct it. But the other possibility is that its not a human age, but some alien's age for which it's totally normal to live more than 300 years. To check that, we should probably read the data description one more time, ask on forums. Maybe the data is totally correct, and then we just misinterpret it. Now, take a look at our Google AdWords data set. We understood that the values in the clicks variable should be less or equal than the values in impressions column. And in our case, in the first row, we see zero impressions and three clicker. That sounds like a bug, right? In fact, it probably is, but differently to the example of person's age, it could be rather a regular error made by either data exporting script or another kind of algorithm. That is, the errors were made not at random, but there is some kind of logic why there is an error in that particular place. It means that these mistakes can be used to get a better score. For example, in our case, we could create a new feature, is_incorrect, and mark all the rows that have errors. Probably, our models will find this feature helpful. It is also very important to understand how the data was generated. What was the algorithm for sampling objects from the database? Maybe, the host sample get objects at random, or they over-sample the particular class, that is, they generated more examples of that class. For example, to make the data set more class balanced. In fact, only if you know how the data was generated, you can set up a proper validation scheme for models. Coming down for the correct validation pipeline is crucial, and we will discuss it later in this course. So, what can we possibly find out about generation processes? For example, we could find out the train and test set were generated with different algorithms. And if the test set is different to the train set, we cannot use part of the train set as a validation set, because this part will not be representative of test set. And so, we cannot evaluate our models using it. So once again, to set up a correct validation, we need to know underlying data generation processes. In the ad computation, we've discussed before, that all the symptoms of different train test sampling. Improving the model on validation set didn't result into improved public leader-board score. And more, the leader-board score was unexpectedly higher than the validation score. I was also visualizing various things while trying to understand what's happening, and every time, the plots for the train set were much different to the test set plots. This also could not happen if the train and test set were similar. And finally, it was suspicious that although the train period was more than ten times larger than the test period, the train set had much fewer rows. it was not straight forward, but this triangle on the left figure was the clue for me, and the puzzle was solved. I've adjusted the train set to match test set. The validation score became reliable, and the modeling could be commenced. You can find the entire task description and investigation in the written materials. So, in this video, we've discussed several important exploratory steps. First, we need to get domain knowledge about the task as it helps to better understand the problem and the data. Next, we need to check if the data is intuitive, and agrees with our domain knowledge. And finally, it is necessary to understand how the data was generated by organizers because otherwise, we cannot establish a proper validation for our models.[SOUND] In the previous video,
we were working with the data for which we had a nice description. That is,
we knew what the features were, and the data was given us as these
without severe modifications. But, it's not always the case. The data can be anonymized,
and obfuscated. In this video, we'll first discuss
what is anonymized data, and why organizers decide to
anonymize their data. And next, we will see what we
as competitors can do about it. Sometimes we can decode the data, or if we can not we can try to guess,
what is the type of feature. So, let's get to the discussion. Sometimes the organizers really want
some information to be reviewed. So, they make an effort to
export competition data, in a way one couldn't get
while you're out of it. Yet all the features are preserved, and machinery model will be
able to do it's job. For example, if a company wants
someone to classify its document, but doesn't want to reveal
the document's content. It can replace all the word occurrences
with hash values of those words, like in the example you see here. In fact, it will not change a thing for
a model based on bags of words. I will refer to Anonymized
data as to any data which organizers intentionally changed. Although it is not completely correct,
I will use this wording for any type of changes. In computations with tabular data, companies can try to hide
information each column stores. Take a look at this data set. First, we don't have any
meaningful names for the features. The names are replaced with some dummies,
and we see some hash like values
in columns x1 and x6. Most likely, organizers decided
to hash some sensitive data. There are several things we can do
while exploring the data in this case. First, we can try to decode or de-anonymize the data,
in a legal way of course. That is, we can try to guess
true meaning of the features. Sometimes de-anonymization
is not possible, but what we almost surely can do,
is to guess the type of the features, separating them into numeric,
categorical, and so on. Then, we can try to find how
features relate to each other. That can be a specific relation
between a pair of features, or we can try to figure out if
the features are grouped in some way. In this video we will concentrate
on the first problem. In the next video we will discuss
visualization tools, that we can use both for exploring individual
features, and feature relations. Let's now get to an example
how it was possible to decode the meaning of the feature in one
local competition I took part. I want to tell you about
a competition I took part. It was a local competition, and
organizers literally didn't give competitors any information
about a dataset. They just put the link to download data on
the competition page, and nothing else. Let's read the data first, and basically what we see here is
that the data is anonymized. The column names are like x something, and the values are hashes, and
then the rest are numeric in here. But, well we don't know
what they mean at all, and basically we don't
what we are to predict. We only know that it is
a multi-class classification task, and we have four labels. So, as long as we don't
know what the data is, we can probably build a quick baseline. Let's import Random Forest Classifier. Yeah, of course we need to drop
target label from our data frame, as it is included in there. We'll fill null values with minus 999, and let's encode all the categorical features, that we can find by looking at the types. Property of our data frame. We will encode them with Label Encoder,
and it is easier to do with
function factorize from Pandas. Let's feed to
Random Forest Classifier on our data. And let's plot the feature importance's,
and what we see here is that feature
X8 looks like an interesting one. We should probably investigate
it a little bit deeper. If we take the feature X8, and print it's mean, and estimate the value. They turn out to be quite close to 0,
and 1 respectively, and it looks like this feature was
tendered skilled by the organizers. And we don't see here exactly 0,
and exactly 1, because probably training test was
concatenated when on the latest scale. If we concatenate training test,
then the mean will be exactly 0, and the std will be exactly 1. Okay, so let's also see are there any
other repeated values in these features? We can do it with a value counts function. Let's print first 15
rows of value counts out. And we can see that there
are a lot of repeated values, they repeated a thousand times. All right, so we now know that
this feature was standard scaled. Probably, we can try to scale it back. The original feature was multiplied by
a number, and was shifted by a number. All we need to do is to find the shooting
parameter, and the scaling parameter. But how do we do that,
and it is really possible? Let's take unique values of the feature,
and sort them. And let's print the difference
between two consecutive numbers, in this sorted array. And look, it looks like the values
are the same all the time. The distance between two consecutive
unique values in this feature, was the same in the original data to. It was probably not 0.043 something, it was who knows,
it could be 9 or 11 or 11.7, but it was the same between all the pairs,
so assume that it was 1 because, well,
1 looks like a natural choice. Let's divide our feature by
this number 0.043 something, and if we do it, yes, we see that
the differences become rather close to 1, they are not 1,
only because of some numeric errors. So yes, if we divide our feature by
this value, this is what you get. All right, so what else do we see here. We see that each number,
it ends with the same values. Each positive number ends
with this kind of value, and each negative with this, look. It looks like this fractional
part was a part of the shifting parameter,
let's just subtract it. And in fact if we subtract it,
the data looks like an integers, actually. Like it was integer data, but
again because of numeric errors, we see some weird numbers in here. Let's round the numbers,
and that is what we get. This is actually on the first ten rows,
not the whole feature. Okay, so what's next? What did we do so far? We found the scaling parameter,
probably we were right, because the numbers became integers,
and it's a good sign. We could be not right, because who knows,
the scaling parameter could be 10 or 2 or again 11 and
still the numbers will be integers. But, 1 looks like a good match. It couldn't be as random, I guess. But, how can we find
the shifting parameter? We found only fractional part,
can we find the other, and can we find the integer part, I mean? It's actually a hard question, because
while you have a bunch of numbers in here, and you can probably build a hypothesis. It could be something, and the regular
values for this something is like that, and we could probably scale it,
shift it by this number. But it could be only an approximation,
and not a hypothesis, and so our journey could
really end up in here. But I was really lucky, and I will show it to you,
so if you take your x8. I mean our feature, and
print value counts, what we will see, we will this number 11, 17, 18, something. And then if we scroll down
we will see this, -1968, and it definitely looks like
year a of birth, right? Immediately I have a hypothesis, that this could be a text box where
a person should enter his year of birth. And while most of the people really
enter their year of birth, but one person entered zero. Or system automatically entered 0,
when something wrong happened. And wow, that isn't the key. If we assume the value was originally 0, then the shifting parameter is
exactly 9068, let's try it. Let's add 9068 to our data,
and see the values. Again we will use value counts function,
and we will sort sorted values. This is the minimum of the values,
and in fact you see the minimum is 0, and all the values are not negative,
and it looks really plausible. Take a look, 999,
it's probably what people love to enter when they're asked to enter something,
or this, 1899. It could be a default value for
this textbook, it occurred so many times. And then we see some weird values in here. People just put them at random. And then, we see some kind of
distribution over the dates. That are plausible for
people who live now, like 1980. Well maybe 1938, I'm not sure about this, and yes of course we see some
days from the future, but for sure it looks like a year of birth, right? Well the question, how can we use
this information for the competition? Well again for linear models, you probably could make a new feature
like age group, or something like that. But In this particular competition, it was no way to use this for,
to use this knowledge. But, it was really fun to investigate. I hope you liked the example,
but usually is really hard to recognize anything sensible like
a year of birth anonymous features. The best we can do is to
recognize the type of the feature. Is it categorical, numeric,
text, or something else? Last week we saw that each data
type should be treated differently, and more treatment depends
on the model we want to use. That is why to make a stronger model, we
should know what data we are working with. Even though we cannot understand
what the features are about, we should at least detect the types
of variables in the data. Take a look at this example, we don't
have any meaningful companies, but still we can deduce what
the feature types are. So, x1 looks like text or
physical recorded, x2 and x3 are binary, x4 is numeric,
x5 is either categorical or numeric. And more, if it's numeric it could
be something like event calendars, because the values are integers. When the number of columns in data
set is small, like in our example, we can just bring the table, and
manually sort the types out. But, what if there are thousand
of features in the data set? Very useful functions to
facilitate our exploration, function d types from pandas guesses the
types for each column in the data frame. Usually it groups all the columns
into three categories, flawed, integer, and
so called object type. If dtype function assigned
flawed type to a feature, this feature is most likely to be numeric. Integer typed features can be either
binary encoded with a zero or one. Event counters, or even categorical,
encoded with the label encoder. Sometimes this function
returns a type named object. And it's the most problematic,
it can be anything, even an irregular numeric feature with
missing values filled with some text. Try it on your data, and also check out a
very similar in full function from Pandas. To deal with object types, it is useful to
print the data and literally look at it. It is useful to check unique
values with value counts function, and nulls location with
isnull function at times. In this lesson, we were discussing two
things we can do with anonymized features. We saw that sometimes, it's possible to decode features,
find out what this feature really means. It doesn't matter if we understand
the meaning of the features or not, we should guess the feature types,
in order to pre-process features accordingly to the type we have,
and selected model class. In the next video,
we'll see a lot of colorful plots, and talk about visualization, and
other tools for exploratory data analysis. [SOUND]In the previous video, we've tried to decode anonymized features and guess their types. In fact, we want to do more. We want to generate new features and to find insights in a data. And in this lesson, we will talk about various visualizations that can help us with it. We will first to see what plots we can draw to explore individual features, and then we will get to exploration of feature relations. We'll explore pairs first and then we'll try to find feature groups in a dataset. First, there is no recipe how you find interesting things in the data. You should just spend some time looking closely at the data table, printing it, and examining. If we found something interesting, we then can take a closer look. So, EDA is kind of an art, but we have a bunch of tools for it which we'll discuss right now. The first, we can build histograms. Histograms split feature edge into bins and show how many points fall into each bin. Note that histograms may be misleading in some cases, so try to overwrite its number of bins when using it. Also, know that it aggregates in the data, so we cannot see, for example, if all the values are unique or there are a lot of repeated values. Let's see in other example. The first thing that I want to illustrate here is that histograms can confuse. Looking at this histogram, we could probably think that there are a lot of zero values in this feature. But in fact, if we take logarithm of the values and build histogram again, we'll clearly see that distribution is non-degenerate and there are many more distinct values than one. So my point is never make a conclusion based on a single plot. If you have a hypothesis, try to make several different plots to prove it. The second interesting thing here is that peak. What is it? It turns out that the peak is located exactly at the mean value of this feature. Seems like organizers filled the missing values with the mean values for us. So, now we understand that values were originally missing. How can we use this information? We can replace the missing values we found with not numbers, nulls again. For example, [inaudible] has a special algorithm that can fill missing values on its own and so, maybe [inaudible] will benefit from explicit missing values. Or we can fill the missing values with something other than feature mean, for example, with -999. Or we can generate a new feature which will indicate that the value was missing. This can be particularly useful for linear models. We can also build the plot where on X axis, we have a row index, and on the Y axis, we have feature values. It is convenient not to connect points with line segments but only draw them with circles. Now, if we observe horizontal lines on this kind of plot, we understand there are a lot of repeated values in this feature. Also, note the randomness over the indices. That is, we see some horizontal patterns but no vertical ones. It means that the data is properly shuffled. We can also color code the points according to their labels. Here, we see that the feature is quite good as it presumably gives a nice class separation. And also, we clearly see that the data is not shuffled here. It is, in fact, sorted by class label. It is useful to examine statistics with Pandas' describe function. You can see examples of its output on the screenshot. It gives you information about mean, standard deviation, and several percentiles of the feature distribution. Of course, you can manually compute those statistics. In Pandas' nan type, you can find functions named by statistics they compute. Mean for mean value, var for variance, and so on, but it's really convenient to have them all in once. And finally, as we already discussed in the previous video, there is value_counts function to examine the number of occurrences of distinct feature values, and a function is null, which helps to find the missing values in the data. For example, you can visualize nulls patterns in the data as on the picture you see. So, here's the full list of functions we've discussed. Make sure you remember each of them. To this end, we've discussed visualizations for individual features. And now, let's get to the next topic of our discussion, exploration of feature relations. It turns out that sometimes, it's hard to make conclusions looking at one feature at a time. So let's look at the pairs. The best two here is a scatter plot. With it, we can draw one sequence of values versus another one. And usually, we plot one feature versus another feature. So each point on the figure correspond to an object with the feature values shown by points position. If it's a classification task, it's convenient to color code the points with their labels like on this picture. The color indicates the class of the object. For regression, the heat map light coloring can be used, too. Or alternatively, the target value can be visualized by point size. We can effectively use scatter plots to check if the data distribution in the train and test sets are the same. In this example, the red points correspond to class zero, and the blue points to class one. And on top of red and blue points, we see gray points. They correspond to test set. We don't have labels for the test set, that is why they are gray. And we clearly see that the red points are mixed with part of the gray ones, and that that is good actually. But other gray points are located in the region where we don't have any training data, and that is bad. If you see some kind of discrepancy between colored and gray points distribution, you should probably stop and think if you're doing it right. It can be just a bug in the code, or it can be completely overfitted feature, or something else that is for sure not healthy. Now, take a look at this scatter plot. Say, we plot feature X1 versus feature X2. What can we say about their relation? The right answer is X2 is less or equal than one_minus_X1. Just realize that the equation for the diagonal line is X1 + X2 = 1, and for all the points below the line, X2 is less or equal than one_minus_X1. So, suppose we found this relation between two features, how do we use this fact? Of course, it depends, but at least there are some obvious features to generate. For tree-based models, we can create a new features like the difference or ratio between X1 and X2. Now, take a look at this scatter plot. It's hard to say what is the true relation between the features, but after all, our goal is not to decode the data here but to generate new features and get a better score. And this plot gives us an idea how to generate the features out of these two features. We see several triangles on the picture, so we could probably make a feature to each triangle a given point belongs, and hope that this feature will help. When you have a small number of features, you can plot all the pairwise scatter plots at once using scatter metrics function from Pandas. It's pretty handy. It's also nice to have histogram and scatter plot before the eyes at the same time as scatter plot gives you very vague information about densities, while histograms do not show feature interactions. We can also compute some kind of distance between the columns of our feature table and store them into a matrix of size number of features by a number of features. For example, we can compute correlation between the counts. It's the most common type of matrices people build, correlation metric. But we can compute other things than correlation. For example, how many times one feature is larger than the other? I mean, how many rows are there such that the value of the first feature is larger than the value of the second one? Or another example, we can compute how many distinct combinations the features have in the dataset. With such custom functions, we should build the metrics manually, and we can use matshow function from Matplotlib to visualize it like on the slide you see. If the metrics looks like a total mess like in here, we can run some kind of clustering like K-means clustering on the rows and columns of this matrix and reorder the features. This one looks better, isn't it? We actually came to the last topic of our discussion, feature groups. And it's what we see here. There are groups of very similar features, and usually, it's a good idea to generate new features based on the groups. Again, it depends, but maybe some statistics could collated over the group will work fine as features. Another visualization that helps to find feature groups is the following: We calculate the statistics of each feature, for example, mean value, and then plot it against column index. This plot can look quite random if the columns are shuffled. So, what if we sorted the columns based on this statistic? Feature and mean, in this case. It looks like it worked out. We clearly see the groups here. So, now we can take a closer look to each group and use the imagination to generate new features. And here is a list of all the functions we've just discussed. Pause the video and check if you remember the examples we saw. So, finally in this video, we we're talking about the tools and functions that help us with data exploration. For example, to explore features one by one, we can use histograms, plots, and we can also examine statistics. To explore a relation between the features, the best tool is a scatter plot. Scatter metrics combines several scatter plots and histograms on one figure. Correlation plot is useful to understand how similar the features are. And if we reorder the columns and rows of the correlation metrics, we'll probably find feature groups. And feature groups was the last topic we discussed in this lesson. We also saw a plot of sorted feature statistics and how it can reveal as feature groups. Well, of course, we've discussed only a fraction of helpful plots there are. With practice, you will develop and find your own tools further exploration.[MUSIC] Hi, in this video we will discuss
a little bit of dataset cleaning and see how to check if dataset is shuffled. It is important to understand that
the competition data can be only apart of the data organizers have. The organizers could give us
a fraction of objects they have or a fraction of features. And that is why we can have
some issues with the data. For example, we can encounter a feature
which takes the same value for every object in both train and test set. This could be due to
the sampling procedure. For example, the future is a year, and the organizers exported
us only one year of data. So in the original data
that the organizers have, this future is not constant, but
in the competition data it is constant. And obviously, it is not useful for
the models and just occupy some memory. So we are about to remove
such constant features. In this example data set
feature of zero is constant. It can be the case that the feature
is constant on the train set but how is different values on the test set. Again, it is better to remove such
features completely since it is constant during training. In our dataset feature is f1. What is the problem, actually? For example, my new model can assign
some weight to this future, so this future will be a part of the
prediction formula, and this formula will be completely unreliable for the objects
with the new values of that feature. For example, for
the last row in our data set. J row, even if categorical feature is not
constant on the train path but there were values that present only in the test data,
we need to handle this situation properly. We need to decide,
do these new values matter much or not? For example, we can simulate this
situation with a validation set and compare the quality of the predictions
on the objects with the syn feature values and
objects with the new feature values. Maybe we will decide to remove
the feature or maybe we will decide to create a separate model for
the object with a new feature values. Sometimes there are duplicated
numerical features that these two columns are completely identical. In our example data set,
these columns f2 and f3. Obviously, we should leave only one of
those two features since the other one will not give any new information to the
model and will only slow down training. From a number of features, it's easy
to check if two columns are the same. We just can compare them element wise. We can also have duplicated
categorical features. The problem is that the features
can be identical but their levels have different names. That is it can be possible to rename
levels of one of the features and two columns will become identical. For example features f4 and f5. If we rename levels of the feature f5, C to A, A to B, and B to C. The result will look
exactly as feature f4. But how do we find such
duplicated features? Fortunately, it's quite easy, it will take us only one more
line of code to find them. We need to label and code all
the categorical features first, and then compare them as if they were numbers. The most important part
here is label encoding. We need to do it right. We need to encode the features
from top to bottom so that the first unique value we see gets
label 1, the second gets 2 and so on. For example for feature f4, we will encode A with 1,
B with 2 and C with 3. Now feature f5 will encode
it differently C will be 1, A will be 2 and B will be 3. But after such encodings columns f4 and f5 turn out to be identical and
we can remove one of them. Another important thing
to check is if there are any duplicated rows in the train and
test. Is to write a lot of duplicated rows
that also have different target, it can be a sign the competition
will be more like a roulette, and our validation will be different
to public leader board score, and private standing will be rather random. Another possibility, duplicated rows
can just be the result of a mistake. There was a competition where
one row was repeated 100,000 times in the training data set. I'm not sure if it was intentional or
not, but it was necessary to remove those duplicated rows to
have a high score on the test set. Anyway, it's better to explain it to ourselves why do we
observe such duplicated rows? This is a part of data
understanding in fact. We should also check if train and
test have common rows. Sometimes it can tell us something
about data set generation process. And again we should probably think what
could be the reason for those duplicates? Another thing we can do,
we can set labels manually for the test rows that
are present in the train set. Finally, it is very useful to check
that the data set is shuffled, because if it is not then, there is
a high chance to find data leakage. We'll have a special topic about
date leakages later, but for now we'll just discuss
that the data is shuffled. What we can do is we can plug a feature or
target vector versus row index. We can optionally smooth
the values using running average. On this slide rolling target
value from pairs competition is plotted while mean target value
is shown with dashed blue line. If the data was shuffled properly we
would expect some kind of oscillation of the target values around
the mean target value. But in this case, it looks like
the end of the train set is much different to the start,
and we have some patterns. Maybe the information from this particular
plot will not advance our model. But once again,
we should find an explanation for all extraordinary things we observe. Maybe eventually, we will find something
that will lead us to the first place. Finally, I want to encourage
you one more time to visualize every possible
thing in a dataset. Visualizations will lead
you to magic features. So this is the last slide for this lesson. Hope you've learned something new and
excited about it. Here's a whole list of
topics we've discussed. You can pause this video and try to remember what we were
talking about and where. See you later. [MUSIC]