Hi everyone. This video is dedicated to the following advanced feature engineering techniques. Calculating various statistics of one feature grouped by another and features derived from neighborhood analysis of a given point. To make it a little bit clearer, let's consider a simple example. Here we have a chunk of data for some CTR task. Let's forget about target variable and focus on human features. Namely, User_ID, unique identifier of a user, Page_ID, an identifier of a page user visited, Ad_price, item prices in the ad, and Ad_position, relative position of an ad on the web page. The most straightforward way to solve this problem is to label and call the Ad_position and feed some classifier. It would be a very good classifier that could take into account all the hidden relations between variables. But no matter how good it is, it still treats all the data points independently. And this is where we can apply feature engineering. We can imply that an ad with the lowest price on the page will catch most of the attention. The rest of the ads on the page won't be very attractive. It's pretty easy to calculate the features relevant to such an implication. We can add lowest and highest prices for every user and page per ad. Position of an ad with the lowest price could also be of use in such case. Here's one of the ways to implement statistical features with paid ads. If our data is stored in the data frame df, we call groupby method like this to get maximum and minimum price values. Then store this object in gb variable, and then join it back to the data frame df. This is it. I want to emphasize that you should not stop at this point. It's possible to add other useful features not necessarily calculated within user and page per. It could be how many pages user has visited, how many pages user has visited during the given session, and ID of the most visited page, how many users have visited that page, and many, many more features. The main idea is to introduce new information. By that means, we can drastically increase the quality of the models. But what if there is no features to use groupby on? Well, in such case, we can replace grouping operations with finding the nearest neighbors. On the one hand, it's much harder to implement and collect useful information. On the other hand, the method is more flexible. We can fine tune things like the size of relevant neighborhood or metric. The most common and natural example of neighborhood analysis arises from purposive pricing. Imagine that you need to predict rental prices. You would probably have some characteristics like floor space, number of rooms, presence of a bus stop. But you need something more than that to create a really good model. It could be the number of other houses in different neighborhoods like in 500 meters, 1,000 meters, or 1,500 meters, or average price per square meter in such neighborhoods, or the number of schools, supermarkets, and parking lots in such neighborhoods. The distances to the closest objects of interest like subway stations or gyms could also be of use. I think you've got the idea. In the example, we've used a very simple case, where neighborhoods were calculated in geographical space. But don't be afraid to apply this method to some abstract or even anonymized feature space. It still could be very useful. My team and I used this method in Spring Leaf competition. Furthermore, we did it in supervised fashion. Here is how we have done it. First of all, we applied mean encoding to all variables. By doing so, we created homogeneous feature space so we did not worry about scaling and importance of each particular feature. After that, we calculated 2,000 nearest neighbors with Bray-Curtis metric. Then we evaluated various features from those neighbors like mean target of nearest 5, 10, 15, 500, 2,000 neighbors, mean distance to 10 closest neighbors, mean distance to 10 closest neighbors with target 1, and mean distance to 10 closest neighbors with target 0, and, it worked great. In conclusion, I hope you embrace the main ideas of both groupby and nearest neighbor methods and you would be able to apply them in practice. Thank you for your attention.[MUSIC] Hi everyone, in this video I will
talk about the application of matrix factorization technique
in feature extraction. You will see a few application
of the approach for feature extraction and
we will be able to apply it. I will show you several examples
along with practical details. Here's a classic example
of recommendations. Suppose we have some information about
user, like age, region, interest and items like gender, year length. Also we know ratings that
users gave to some items. These ratings can be organized in a user
item matrix with row corresponding to users, and columns corresponding to items,
as shown in the picture. In a cell with coordinates i, j,
the user or agent can be chooser i, give the item j. Assume that our user
have some features Ui. And jth item have is
corresponding feature Mj. And scalar product of these
features produce a rating Rij. Now we can apply matrix factorization
to learning those features for item and users. Sometimes these features
can have an interpretation. Like the first feature in item can
be measured of or something similar. But generally you should consider them
as some extra features, which we can use to encode user in the same way as we did
before with labeling coder or coder. Specifically our assumption about
scale of product is the following. If we present all attributes of user and items as matrixes, the matrix product will
be very close to the matrix's ratings. In other words,
which way to find matrix's U and M, such as their product
gives the matrix R. This way, this approach is called matrix
factorization or matrix composition. In previous examples, we used both row and
column related features. But sometimes we don't let
the features correspond to rows. Let's consider another example. Suppose that we are texts, do you
remember how we usually classify text? We extract features and each document
was described by a large sparse reactor. If we do matrix factorization over
these parse features, we will get the representation for index displayed
in yellow, and terms displayed in green. Although we can somehow
use representation for jumps, we are interested only
in representations for dogs. Now every document is described
by a small, dense reactor. These are our features, and we can use
them in a way similar to previous example. This case is often called
dimension energy reduction. It's quite an efficient way to reduce
the size of feature metric, and extract real valued features
from categorical ones. In competitions we often have
different options for purchasing. For example, using text data,
you can run back of big rams and so on. Using matrix optimization technique, you are able to extract features
from all of these matrices. Since the resulting matrices will be
small, we can easily join them and use togetherness of the features
in tree-based models. Now I want to make a few comments
about matrix factorization. Not just that we are not
constrained to reduce whole matrix, you can apply factorization to a subset
of a column and leave the other as is. Besides reduction you can
use pressure boards for getting another presentation
of the same data. This is especially useful for example since it provides velocity
of its models and leads to a better. Of course matrix factorization
is a loss of transformation, in other words we will lose some
information after the search reduction. Efficiency of this approach heavily
depends on a particular task and choose a number of latent factors. The number should be considered as
a hyper parameter and needs to be tuned. It's a good practice to choose
a number of factors between 5 and 100. Now, let's switch from general idea
to particular implementations. Several matrix factorization
methods are implemented in circuit as the most famous SVD and PCA. In addition,
their use included TruncatedSVD, which can work with sparse matrices. It's very convenient for example,
in case of text datasets. Also there exists a so called non-negative
matrix factorization, or NMF. It impose an additional restrictions that
all hidden factors are non-negative, that is either zero or a positive number. It can be applied only to
non-negative matrixes. For example matrix where all represented
occurence of each word in the document. NMF has an interesting property, it transforms data in a way that makes
data more suitable for decision trees. Take a look at the picture from
Microsoft Mobile Classification Challenge. It can be seen that NMF transform data
forms lines parallel to the axis. A few more notes on matrix factorizations. Essentially they are very
similar to linear models, so we can use the same transformation
tricks as we use for linear models. So in addition to standard NMF, I advise you to apply
the factorization to transform data. Here's another plot from the competition. It's clear that these two transformations
produce different features, and we don't have to choose the best one. Instead, it's beneficial
to use both of them. I want to note that matrix factorization
is a trainable transformation, and has its own parameters. So we should be careful, and
use the same transformation for all parts of your data set. Reading and transforming each
part individually is wrong, because in that case you will get
two different transformations. This can lead to an error
which will be hard to find. The correct method is shown below,
first we need to the data information on all data and
only then apply to each individual piece. To sum up, matrix composition is a very
general approach to dimensional reduction and feature extraction. It can be used to transform
categorical feature into real ones. And tricks for linear models are also
suitable for matrix factorizations. Thank you for your attention. [MUSIC] [SOUND]Hi, everyone. The main topic of this video is Feature Interactions. You will learn how to construct them and use in problem solving. Additionally, we will discuss them for feature extraction from decision trees. Let's start with an example. Suppose that we are building a model to predict the best advertisement banner to display on a website. Among available features, there are two categorical ones that we will concentrate on. The category of the advertising banner itself and the category of the site the banner will be showing on. Certainly, we can use the features as two independent ones, but a really important feature is indeed the combination of them. We can explicitly construct the combination in order to incorporate our knowledge into a model. Let's construct new feature named ad_site that represents the combination. It will be categorical as the old ones, but set of its values will be all possible combinations of two original values. From a technical point of view, there are two ways to construct such interaction. Let's look at a simple example. Consider our first feature, f1, has values A or B. Another feature, f2, has values X or Y or Z, and our data set consist of four data points. The first approach is to concatenate the text values of f1 and f2, and use the result as a new categorical feature f_join. We can then apply the OneHot according to it. The second approach consist of two steps. Firstly, apply OneHot and connect to features f1 and f2. Secondly, construct new metrics by multiplying each column from f1 encoded metrics to each column from f2 encoded metrics. It was nothing that both methods results in practically the same new feature representations. In the above example, we can consider as interactions between categorical features, but similar ideas can be applied to real valued features. For example, having two real valued features f1 and f2, interactions between them can be obtained by multiplications of f1 and f2. In fact, we are not limited to use only multiply operation. Any function taking two arguments like sum, difference, or division is okay. The following transformations significantly enlarge feature space and makes learning easier, but keep in mind that it makes or frequent easier too. It should be emphasized that for three ways algorithms such as the random forest or gradient boost decision trees it's difficult to extract such kind of dependencies. That's why they're buffer transformation are very efficient for three based methods. Let's discuss practical details now. Where wise future generation approaches greatly increase the number of the features. If there were any original features, there will be n square. And will be even more features if several types of interaction are used. There are two ways to moderate this, either do feature selection or dimensionality reduction. I prefer doing the selection since not all but only a few interactions often achieve the same quality as all combinations of features. For each type of interaction, I construct all piecewise feature interactions. Feature random forests over them and select several most important features. Because number of resulting features for each type is relatively small. It's possible to join them together along with original features and use as input for any machine learning algorithm usually to be by use method. During the video, we have examined the method to construct second order interactions. But you can similarly produce throned order or higher. Due to the fact that number of features grow rapidly with order, it has become difficult to work with them. Therefore high order directions are often constructed semi-manually. And this is an art in some ways. Additionally, I would like to talk about methods to construct categorical features from decision trees. Take a look at the decision tree. Let's map each leaf into a binary feature. The index of the object's leaf can be used as a value for a new categorical feature. If we use not a single tree but an ensemble of them. For example, a random forest, then such operation can be applied to each of entries. This is a powerful way to extract high order interactions. This technique is quite simple to implement. Tree-based poodles from sklearn library have an apply method which takes as input feature metrics and rituals corresponding indices of leaves. In xgboost, also support to why a parameter breed leaf in predict method. I suggest we need to collaborate documentations in order to get more information about these methods and IPIs. In the end of this video, I will tackle the main points. We examined method to construct an interactions of categorical features. Also, we extend the approach to real-valued features. And we have learned how to use trees to extract high order interactions. Thank you for your attention.Hi, everyone. Today, we will discuss this new method for visualizing data integrating features. At the end of this video, you will be able to use tSNE in your products. In the previous video, we learned about metaphysician technique that is predatory very close to linear models. In this video, we will touch the subject of non-linear methods of dimensionality reduction. That says in general are called manifold learning. For example, look at the data in form of letter S on the left side. On the right, we can see results of running different manifold learning algorithm on the data. This new result is placed at the right bottom corner on the slide. This new algorithm is the main topic of the lecture, as it tells of how this really works won't be explained here. But you will come to look at additional materials for the details. Let's just say that this is a method that tries to project points from high dimensional space into small dimensional space so that the distances between points are approximately preserved. Let's look at the example of the tSNE on the MNIST dataset. Here are points from 700 dimensional space that are projected into two dimensional space. You can see that such projection forms explicit clusters. Coolest shows that these clusters are meaningful and corresponds to the target numbers well. Moreover, neighbor clusters corresponds to a visually similar numbers. For example, cluster of three is located next to the cluster of five which in chance is adjustment to the cluster of six and eight. If data has explicit structure as in case of MNIST dataset, it's likely to be reflected on tSNE plot. For the reason tSNE is widely used in exploratory data analysis. However, do not assume that tSNE is a magic want that always helps. For example, a misfortune choice of hyperparameters may lead to poor results. Consider an example, in the center is the least presented a tSNE projection of exactly the same MNIST data as in previous example, only perplexity parameter has been changed. On the left, for comparison, we have plots from previous right. On the right, so it present a tSNE projection of random data. We can see as a choice of hybrid parameters change projection of MNIST data significantly so that we cannot see clusters. Moreover, new projection become more similar to random data rather than to the original. Let's find out what depends on the perplexity hyperparameter value. On the left, we have perplexity=3, in the center=10, and on the right= 150. I want to emphasize that these projections are all made for the same data. The illustration shows that these new results strongly depends on its parameters, and the interpretation of the results is not a simple task. In particular, one cannot infer the size of original clusters using the size of projected clusters. Similar proposition is valid for a distance between clusters. Blog distill.pub contain a post about how to understand and interpret the results of tSNE. Also, it contains a great interactive demo that will help you to get into issues of how tSNE works. I strongly advise you to take a look at it. In addition to exploratory data analysis, tSNE can be considered as a method to obtain new features from data. You should just concatenate the transformers coordinates to the original feature matrix. Now if you've heard this about practical details, as it has been shown earlier, the results of tSNE algorithm, it strongly depends on hyperparameters. It is good practice to use several projections with different perplexities. In addition, because of stochastic of this methods results in different projections even with the same data and hyperparameters. This means the train and test sets should be projected together rather than separately. Also, tSNE will run for a long time if you have a lot of features. If the number of features is greater than 500, you should use one of dimensionality reduction approach and reduce number of features, for example, to 100. Implementation of tSNE can be found in the sklearn library. But personally, I prefer to use another implementation from a separate Python package called tSNE, since it provide a way more efficient implementation. In conclusion, I want to remind you the basic points of the lecture. TSNE is an excellent tool for visualizing data. If data has an explicit structure, then it likely be [inaudible] on tSNE projection. However, it requires to be cautious with interpretation of tSNE results. Sometimes you can see structure where it does not exist or vice versa, see none where structure is actually present. It's a good practice to do several tSNE projections with different perplexities. And in addition to EJ, tSNE is working very well as a feature for feeding models. Thank you for your attention.