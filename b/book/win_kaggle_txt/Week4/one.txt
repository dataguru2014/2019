[MUSIC] Hi, in this lecture, we will study
hyperparameter optimization process and talk about hyperparameters in
specific libraries and models. We will first discuss
hyperparameter tuning in general. General pipeline, ways to tuning
hyperparameters, and what it actually means to understand how a particular
hyperparameter influences the model. It is actually what we will
discuss in this video, and then we will talk about libraries and
frameworks, and see how to tune hyperparameters
of several types of models. Namely, we will first
study tree-based models, gradient boosting decision trees and
RandomForest. Then I'll review important
hyperparameters in neural nets. And finally, we will talk about
linear models, where to find them and how to tune them. Another class of interesting
models is factorization machines. We will not discuss factorization
machines in this lecture, but I suggest you to read
about them on the internet. So, let's start with a general
discussion of a model tuning process. What are the most important things to
understand when tuning hyperparameters? First, there are tons of potential
parameters to tune in every model. And so we need to realize which
parameters are affect the model most. Of course,
all the parameters are reliable, but we kind of need to select
the most important ones. Anyway we never have time to tune
all the params, that's right. So we need to come up with a nice
subset of parameters to tune. Suppose we're new to xgboost and we're trying to find out what
parameters will better to tune, and say we don't even understand how
gradient boosting decision tree works. We always can search what parameters
people usually set when using xgboost. It's quite easy to look up, right? For example, at GitHub or Kaggle Kernels. Finally, the documentation sometimes
explicitly states which parameter to tune first. From the selected set of parameters
we should then understand what would happen if we
change one of the parameters? How the training process and the training
invalidation course will change if we, for example,
increased a certain parameter? And finally, actually tune
the selected parameters, right? Most people do it manually. Just run, examine the logs,
change parameters, run again and
iterate till good parameters found. It is also possible to use
hyperparameter optimization tools like hyperopt, but it's usually
faster to do it manually to be true. So later in this video, actually discuss
the most important parameters for some models along with some intuition how
to tune those parameters of those models. But before we start, I actually want
to give you a list of libraries that you can use for
automatic hyperparameter tuning. There are lots of them actually, and
I didn't try everything from this list myself, but from what I actually tried,
I did not notice much difference in optimization speed on
real tasks between the libraries. But if you have time,
you can try every library and compare. From a user side these
libraries are very easy to use. We need first to define the function
that will run our module, in this case, it is XGBoost. That will run our module with
the given set of parameters and return a resulting validation score. And second,
we need to specify a source space. The range for the hyperparameters where
we want to look for the solution. For example, here we see that a parameter,
it is fix 0.1. And we think that optimal max depth
is somewhere between 10 and 30. And actually that is it,
we are ready to run hyperopt. It can take much time, so
the best strategy is to run it overnight. And also please note that everything
we need to know about hyperparameter's, in this case,
is an adequate range for the search. That's pretty convenient,
if you don't know the new model and you just try to run. But still,
most people tuned the models manually. So, what exactly does it
mean to understand how parameter influences the model? Broadly speaking, different values of parameters result
in three different fitting behavior. First, a model can underfit. That is, it is so constrained that
it cannot even learn the train set. Another possibility is that
the model is so powerful that it just overfits to the train set and
is not able to generalize it all. And finally, the third behavior is something
that we are actually looking for. It's somewhere between underfitting and
overfitting. So basically, what we should examine
while turning parameters is that we should try to understand if the model
is currently underfitting or overfitting. And then, we should somehow
adjust the parameters to get closer to desired behavior. We need to kind of split all the
parameters that we would like to tune into two groups. In the first group, we'll have
the parameters that constrain the model. So if we increase
the parameter from that group, the model would change its behaviour
from overfitting to underfitting. The larger the value of the parameter,
the heavier the constraint. In the following videos, we'll color such
parameters in red, and the parameters in the second group are doing an opposite
thing to our training process. The higher the value,
more powerful the main module. And so by increasing such parameters, we can change fitting behavior
from underfitting to overfitting. We will use green color for
such parameters. So, in this video we'll be discussing some general aspects of
hyperparameter organization. Most importantly,
we've defined the color coding. If you did not understand
what color stands for what, please watch a part of
the video about it again. We'll use this color coding
throughout the following videos. [MUSIC][MUSIC] In this video, we will talk about
hyperparameter optimization for some tree based models. Nowadays, XGBoost and
LightGBM became really gold standard. They are just awesome implementation
of a very versatile gradient boosted decision trees model. There is also a CatBoost library it
appeared exactly at the time when we were preparing this course, so CatBoost
didn't have time to win people's hearts. But it looks very interesting and
promising, so check it out. There is a very nice
implementation of RandomForest and ExtraTrees models sklearn. These models are powerful, and
can be used along with gradient boosting. And finally, there is a model
called regularized Greedy Forest. It showed very nice results from several
competitions, but its implementation is very slow and hard to use, but
you can try it on small data sets. Okay, what important parameters do
we have in XGBoost and LightGBM? The two libraries have similar parameters
and we'll use names from XGBoost. And on the right half of the slide
you will see somehow loosely corresponding parameter
names from LightGBM. To understand the parameters,
we better understand how XGBoost and LightGBM work at least a very high level. What these models do, these models
build decision trees one after another gradually optimizing a given objective. And first there are many parameters
that control the tree building process. Max_depth is the maximum depth of a tree. And of course, the deeper a tree can be
grown the better it can fit a dataset. So increasing this parameter will lead
to faster fitting to the train set. Depending on the task,
the optimal depth can vary a lot, sometimes it is 2, sometimes it is 27. If you increase the depth and can not get
the model to overfit, that is, the model is becoming better and better on the
validation set as you increase the depth. It can be a sign that there are a lot
of important interactions to extract from the data. So it's better to stop tuning and
try to generate some features. I would recommend to start with
a max_depth of about seven. Also remember that as
you increase the depth, the learning will take a longer time. So do not set depth to
a very higher values unless you are 100% sure you need it. In LightGBM,
it is possible to control the number of leaves in the tree rather
than the maximum depth. It is nice since a resulting
tree can be very deep, but have small number of leaves and
not over fit. Some simple parameter controls a fraction
of objects to use when feeding a tree. It's a value between 0 and 1. One might think that it's better
always use all the objects, right? But in practice,
it turns out that it's not. Actually, if only a fraction of
objects is used at every duration, then the model is less
prone to overfitting. So using a fraction of objects, the model
will fit slower on the train set, but at the same time it will probably generalize
better than this over-fitted model. So, it works kind of as a regularization. Similarly, if we can consider only
a fraction of features [INAUDIBLE] split, this is controlled by parameters
colsample_bytree and colsample_bylevel. Once again, if the model is over fitting, you can try to lower
down these parameters. There are also various regularization
parameters, min_child_weight, lambda, alpha and others. The most important one
is min_child_weight. If we increase it,
the model will become more conservative. If we set it to 0,
which is the minimum value for this parameter,
the model will be less constrained. In my experience, it's one of the most important parameters
to tune in XGBoost and LightGBM. Depending on the task,
I find optimal values to be 0, 5, 15, 300, so do not hesitate to try a wide
range of values, it depends on the data. To this end we were discussing
hyperparameters that are used to build a tree. And next, there are two very important
parameters that are tightly connected, eta and num_rounds. Eta is essentially a learning weight,
like in gradient descent. And the num_round is the how many
learning steps we want to perform or in other words how many
tree's we want to build. With each iteration
a new tree is built and added to the model with
a learning rate eta. So in general,
the higher the learning rate, the faster the model fits to the train set
and probably it can lead to over fitting. And more steps model does,
the better the model fits. But there are several caveats here. It happens that with a too high learning
rate the model will not fit at all, it will just not converge. So first, we need to find out if we
are using small enough learning rate. On the other hand,
if the learning rate is too small, the model will learn nothing
after a large number of rounds. But at the same time, small learning rate
often leads to a better generalization. So it means that learning
rate should be just right, so that the model generalize and
doesn't take forever to train. The nice thing is that we can freeze
eta to be reasonably small, say, 0.1 or 0.01, and then find how many rounds we
should train the model til it over fits. We usually use early stopping for it. We monitor the validation loss and exit
the training when loss starts to go up. Now when we found
the right number of rounds, we can do a trick that
usually improves the score. We multiply the number of
steps by a factor of alpha and at the same time,
we divide eta by the factor of alpha. For example, we double the number
of steps and divide eta by 2. In this case, the learning will
take twice longer in time, but the resulting model
usually becomes better. It may happen that the valid parameters
will need to be adjusted too, but usually it's okay to leave them as is. Finally, you may want to
use random seed argument, many people recommend to
fix seed before hand. I think it doesn't make too much
sense to fix seed in XGBoost, as anyway every changed parameter will
lead to completely different model. But I would use this
parameter to verify that different random seeds do not
change training results much. Say [INAUDIBLE] competition,
one could jump 1,000 places up or down on the leaderboard just by training
a model with different random seeds. If random seed doesn't
affect model too much, good. In other case, I suggest you to think
one more time if it's a good idea to participate in that competition as
the results can be quite random. Or at least I suggest you to adjust
validation scheme and account for the randomness. All right,
we're finished with gradient boosting. Now let's get to RandomForest and
ExtraTrees. In fact, ExtraTrees is just a more
randomized version of RandomForest and has the same parameters. So I will say RandomForest
meaning both of the models. RandomForest and ExtraBoost build trees,
one tree after another. But, RandomForest builds each
tree to be independent of others. It means that having a lot of trees
doesn't lead to overfeeding for RandomForest as opposed
to gradient boosting. In sklearn, the number of trees for random forest is controlled
by N_estimators parameter. At the start, we may want to determine what number
of trees is sufficient to have. That is, if we use more than that,
the result will not change much, but the models will fit longer. I usually first set N_estimators
to very small number, say 10, and see how long does it take
to fit 10 trees on that data. If it is not too long then I set
N_estimators to a huge value, say 300, but it actually depends. And feed the model. And then I plot how the validation
error changed depending on a number of used trees. This plot usually looks like that. We have number of trees on the x-axis and
the accuracy score on y-axis. We see here that about 50 trees
already give reasonable score and we don't need to use more
while tuning parameter. It's pretty reliable to use 50 trees. Before submitting to leaderboard, we can set N_estimators to
a higher value just to be sure. You can find code for this plot,
actually, in the reading materials. Similarly to XGBoost, there is a parameter max_depth
that controls depth of the trees. But differently to XGBoost, it can be set to none,
which corresponds to unlimited depth. It can be very useful actually when
the features in the data set have repeated values and important interactions. In other cases, the model with unconstrained
depth will over fit immediately. I recommend you to start with a depth
of about 7 for random forest. Usually an optimal depth for
random forests is higher than for gradient boosting, so do not hesitate
to try a depth 10, 20, and higher. Max_features is similar to call
sample parameter from XGBoost. The more features I use to decipher
a split, the faster the training. But on the other hand,
you don't want to use too few features. And min_samples_leaf is
a regularization parameter similar to min_child_weight from XGBoost and
the same as min_data_leaf from LightGPM. For Random Forest classifier,
we can select a criterion to eleviate a split in the tree with
a criterion parameter. It can be either Gini or Entropy. To choose one, we should just try both and
pick the best performing one. In my experience Gini is better more
often, but sometimes Entropy wins. We can also fix random seed using
random_state parameter, if we want. And finally, do not forget to set n_jobs
parameter to a number of cores you have. As by default, RandomForest from sklearn
uses only one core for some reason. So in this video, we were talking
about various hyperparameters of gradient boost and
decision trees, and random forest. In the following video, we'll discuss neural networks and linear models. [MUSIC][MUSIC] In this video we'll briefly discuss
neural network libraries and then we'll see how to tune hyperparameters
for neural networks and linear models. There are so many frameworks,
Keras, TensorFlow, MxNet, PyTorch. The choice is really personal, all frameworks implement more than enough
functionality for competition tasks. Keras is for sure the most popular in
Kaggle and has very simple interface. It takes only several dozen lines
to train a network using Keras. TensorFlow is extensively used
by companies for production. And PyTorch is very popular in
deep learning research community. I personally recommend
you to try PyTorch and Keras as they are most transparent and
easy to use frameworks. Now, how do you tune
hyperparameters in a network? We'll now talk about only
dense neural networks, that is the networks that consist
only of fully connected layers. Say we start with a three
layer neural network, what do we expect to happen if we
increase the number of neurons per layer? The network now can learn more
complex decision boundaries and so it will over fit faster. The same should happen when the number
of layers are increased, but due to optimization problems,
the learning can even stop to converge. But anyway, if you think your
network is not powerful enough, you can try to add another layer and
see what happens. My recommendation here is to
start with something very simple, say 1 or 2 layer and 64 units per layer. Debug the code, make sure the training and
[INAUDIBLE] losses go down. And then try to find a configuration that
is able to overfit the training set, just as another sanity check. After it, it is time to tune
something in the network. One of the crucial parts of neural
network is selected optimization method. Broadly speaking, we can pick either
vanilla stochastic gradient descent with momentum or
one of modern adaptive methods like Adam, Adadelta, Adagrad and so on. On this slide,
the adaptive methods are colored in green, as compared to SGD in red. I want to show here that adaptive
methods do really allow you to fit the training set faster. But in my experience,
they also lead to overfitting. Plain old stochastic gradient
descent converges slower, but the trained network usually
generalizes better. Adaptive methods are useful, but in the settings others in
classification and regression. Now here is a question for you. Just keep the size. What should we expect when
increasing a batch size with other hyperparameters fixed? In fact, it turns out that huge batch
size leads to more overfitting. Say a batch of 500 objects
is large in experience. I recommend to pick a value around 32 or
64. Then if you see the network is
still overfitting try to decrease the batch size. If it is underfitting, try to increase it. Know that is a the number
of outbox is fixed, then a network with a batch
size reduced by a factor of 2 gets updated twice more times
compared to original network. So take this into consideration. Maybe you need to reduce the number of
networks or at least somehow adjust it. The batch size also should not be too
small, the gradient will be too noisy. Same as in gradient boosting,
we need to set the proper learning rate. When the learning rate is too high,
network will not converge and with too small a learning rate,
the network will learn forever. The learning rate should be
not too high and not too low. So the optimal learning rate
depends on the other parameters. I usually start with a huge learning rate,
say 0.1, and try to lower it down till I find one with which network converges
and then I try to revise further. Interestingly, there is a connection
between the batch size and the learning rate. It is theoretically grounded for
a specific type of models, but people usually use it,
well actually some people use it as a rule of thumb with neural networks. The connection is the following. If you increase the batch
size by a factor of alpha, you can also increase the learning
rate by the same factor. But remember that the larger batch size, the more your network is
prone to overfitting. So you need a good regularization here. Sometime ago, people mostly use L2 and
L1 regularization for weights. Nowadays, most people use
dropout regularization. So whenever you see a network overfitting,
try first to a dropout layer. You can override dropout probability and a
place where you insert the dropout layer. Usually people add the dropout layer
closer to the end of the network, but it's okay to add some dropout
to every layer, it also works. Dropout helps network to find features
that really matters, and what never worked for me is to have dropout as the very
first layer, immediately after data layer. This way some information is lost
completely at the very beginning of the network and
we observe performance degradation. An interesting regularization
technique that we used in the [UNKOWN] competition is
static dropconnect, as we call it. So recall that, usually we have an input
layer densely connected to, say 128 units. We will instead use a first
hidden layer with a very huge number of units, say 4,096 units. This is a huge network for a usual
competition and it will overfeed badly. But now to regularlize it,
we'll at random drop 99% of connections between the input layer and
the first hidden layer. We call it static dropconnect
because originally in dropconnect, we need to drop random connections at
every learning iterations while we fix connectivity pattern for the network
for the whole learning process. So you see the point, we increase
the number of hidden units, but the number of parameters in the first
hidden layer remains small. Notice that anyway the weight matrix
of the second layer becomes huge, but it turns out to be
okay in the practice. This is very powerful regularizations. And more of the networks with
different connectivity patterns makes much nicer than networks
without static dropconnect. All right, last class of models
to discuss are my neuro models. Yet, a carefully tuned live GPM would
probably beat support vector machines, even on a large, sparse data set. SVM's do not require almost any tuning,
which is truly beneficial. SVM's for classification and regression
are implemented in SK learners or wrappers to algorithms from libraries
called libLinear and libSVM. The latest version of libLinear and
libSVM support multicore competitions, but unfortunately it is not possible
to use multicore version in Sklearn, so we need to compile these libraries
manually to use this option. And I've never had anyone
use kernel SVC lately, so in this video we will
talk only about linear SVM. In Sklearn we can also find logistic and
linear regression with various regularization options and also,
as your declassifier and regressor. We've already mentioned them
while discussing metrics. For the data sets that do not fit in
the memory, we can use Vowpal Wabbit. It implements learning of linear
models in online fashion. It only reads data row by row
directly from the hard drive and never loads the whole
data set in the memory. Thus, allowing to learn
on a very huge data sets. A method of online learning for linear
models called flow the regularized leader or FTRL in short was particularly
popular some time ago. It is implemented in Vowpal Wabbit but there are also lots of
implementations in pure Python. The hyperparameters we usually need
to tune linear models are L2 and L1 regularization of weights. Once again, regularization is denoted
with red color because of the higher the regularization weight is the more
model struggle to learn something. But know that, the parameter see in
SVM is inversely proportional to regularization weight, so
the dynamics is opposite. In fact, we do not need to think about the
meaning of the parameters in the case of one parameter, right? We just try several values and
find one that works best. For SVM, I usually start a very small
seed, say 10 to the power of minus 6 and then I try to increase it,
multiply each time by a factor of 10. I start from small values because
the larger the parameter C is, the longer the training takes. What type of regularization,
L1 or L2 do you choose? Actually, my answer is try both. To my mind actually they are quite similar
and one benefit that L1 can give us is weight sparsity, so the sparsity
pattern can be used for feature selection. A general advise I want to give
here do not spend too much time on tuning hyperparameters, especially
when the competition has only begun. You cannot win a competition
by tuning parameters. Appropriate features, hacks,
leaks, and insights will give you much more than carefully
tuned model built on default features. I also advice you to be patient. It was my personal mistake several times. I hated to spend more then ten minutes
on training models and was amazed how much the models could improve if I
would let it train for a longer time. And finally, average everything. When submitting, learn five models
starting from different random initializations and
average their predictions. It helps a lot actually and
some people average not only random seed, but also other parameters
around an optimal value. For example, if optimal depth for
extra boost is 5, we can average 3 digiboosts with depth 3,
4, and 5. Wow, it would be better if we could averaged 3 digiboosts with depth 4,
5, and 6. Finally, in this lecture, we discussed what is a general pipeline
for a hyperparameter optimization. And we saw, in particular,
what important hyperparameters derive for several models,
gradient boosting decision trees, random forests and extra trees,
neural networks, and linear models. I hope you found something interesting
in this lecture and see you later. [MUSIC]